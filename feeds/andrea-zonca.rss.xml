<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>Andrea Zonca's blog</title><link>http://zonca.github.io/</link><description></description><lastBuildDate>Tue, 24 May 2016 12:00:00 -0700</lastBuildDate><item><title>Jupyterhub deployment on multiple nodes with Docker Swarm</title><link>http://zonca.github.io/2016/05/jupyterhub-docker-swarm.html</link><description>&lt;p&gt;This post is part of a series on deploying Jupyterhub on OpenStack tailored at workshops, in the previous posts I showed:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://zonca.github.io/2016/04/jupyterhub-sdsc-cloud.html"&gt;How to deploy a Jupyterhub on a single server with Docker and Python/R/Julia support&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://zonca.github.io/2016/04/jupyterhub-image-sdsc-cloud.html"&gt;How to deploy the previous server from a pre-built image and customize it&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The limitation of a single server setup is that it cannot scale beyond the resources available on that server, especially memory. Therefore for a workshop that requires to load large amount of data or with lots of students it is recommended to use a multi-server setup.&lt;/p&gt;
&lt;p&gt;Fortunately Docker already provides that flexibility thanks to &lt;a href="https://docs.docker.com/swarm/overview/"&gt;Docker Swarm&lt;/a&gt;. Docker Swarm allows to have a Docker interface that behaves like a normal single server instance but instead launches containers on a pool of servers. Therefore there are mininal changes on the Jupyterhub server.&lt;/p&gt;
&lt;p&gt;Jupyterhub will interface with the Docker Swarm service running locally, Docker Swarm will take care of launching containers across the other nodes. Each container will launch a Jupyter Notebook server for a single user, then Jupyterhub will proxy the container port to the users. Users won't connect directly to the nodes in the Docker Swarm pool. &lt;/p&gt;
&lt;h2&gt;Setup the Jupyterhub server&lt;/h2&gt;
&lt;p&gt;Let's start from the public image already available, see just the first section "Create a Virtual Machine in OpenStack with the pre-built image" in &lt;a href="http://zonca.github.io/2016/04/jupyterhub-image-sdsc-cloud.html"&gt;http://zonca.github.io/2016/04/jupyterhub-image-sdsc-cloud.html&lt;/a&gt; for instructions on how to get the Jupyterhub single server running.&lt;/p&gt;
&lt;h3&gt;Setup Docker Swarm&lt;/h3&gt;
&lt;p&gt;First of all we need to have Docker accessible remotely so we need to configure it to listen on a TCP port, edit &lt;code&gt;/etc/init/docker.conf&lt;/code&gt; and replace &lt;code&gt;DOCKER_OPTS=&lt;/code&gt; in the &lt;code&gt;start&lt;/code&gt; section with:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;DOCKER_OPTS=&amp;quot;-H tcp://0.0.0.0:2375 -H unix:///var/run/docker.sock&amp;quot;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Port 2375 is not open on the OpenStack configuration, so this is not a security issue.&lt;/p&gt;
&lt;p&gt;Then we need to run 2 swarm services in Docker containers, first a distributed key-store listening on port 8500 that is needed for Swarm to store information about all the available nodes, Consul:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;docker run --restart=always  -d -p 8500:8500 --name=consul progrium/consul -server -bootstrap
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;the manager which provides the interface to Docker Swarm:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="x"&gt;HUB_LOCAL_IP=&lt;/span&gt;&lt;span class="p"&gt;$(&lt;/span&gt;&lt;span class="err"&gt;ip&lt;/span&gt; &lt;span class="err"&gt;route&lt;/span&gt; &lt;span class="err"&gt;get&lt;/span&gt; &lt;span class="m"&gt;8&lt;/span&gt;&lt;span class="err"&gt;.&lt;/span&gt;&lt;span class="m"&gt;8&lt;/span&gt;&lt;span class="err"&gt;.&lt;/span&gt;&lt;span class="m"&gt;8&lt;/span&gt;&lt;span class="err"&gt;.&lt;/span&gt;&lt;span class="m"&gt;8&lt;/span&gt; &lt;span class="err"&gt;|&lt;/span&gt; &lt;span class="err"&gt;awk&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;NR==1 {print $NF}&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="x"&gt;&lt;/span&gt;
&lt;span class="x"&gt;docker run --restart=always  -d -p 4000:4000 swarm manage -H :4000 --replication --advertise &lt;/span&gt;&lt;span class="p"&gt;$&lt;/span&gt;&lt;span class="nv"&gt;HUB_LOCAL_IP&lt;/span&gt;&lt;span class="x"&gt;:4000 consul://&lt;/span&gt;&lt;span class="p"&gt;$&lt;/span&gt;&lt;span class="nv"&gt;HUB_LOCAL_IP&lt;/span&gt;&lt;span class="x"&gt;:8500&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This sets &lt;code&gt;HUB_LOCAL_IP&lt;/code&gt; to the internal ip of the instance, then starts the Manager container.&lt;/p&gt;
&lt;p&gt;We are running both with automatic restarting, so that they are launched again in case of failure or after reboot.&lt;/p&gt;
&lt;p&gt;You can check if the containers are running with:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;docker ps -a
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;and then you can check if connection works with Docker Swarm on port 4000:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;docker -H :4000 ps -a
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Check the Docker documentation for a more robust setup with multiple Consul services and a backup Manager.&lt;/p&gt;
&lt;h3&gt;Setup Jupyterhub&lt;/h3&gt;
&lt;p&gt;Following the work by Jess Hamrick for the &lt;a href="https://github.com/compmodels/jupyterhub"&gt;compmodels Jupyterhub deployment&lt;/a&gt;, we can get the &lt;code&gt;jupyterhub_config.py&lt;/code&gt; from &lt;a href="https://gist.github.com/zonca/83d222df8d0b9eaebd02b83faa676753"&gt;https://gist.github.com/zonca/83d222df8d0b9eaebd02b83faa676753&lt;/a&gt; and copy them into the home of the ubuntu user.&lt;/p&gt;
&lt;h3&gt;Share users home via NFS&lt;/h3&gt;
&lt;p&gt;We have now a distributed system and we need a central location to store the home folders of the users, so that even if they happen to get containers on different server, they can still access their files.&lt;/p&gt;
&lt;p&gt;Install NFS with the package manager:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;sudo apt-get install nfs-kernel-server
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;edit &lt;code&gt;/etc/exports&lt;/code&gt;, add:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;/home    *(rw,sync,no_root_squash)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Ports are not open in the NFS configuration.&lt;/p&gt;
&lt;h2&gt;Setup networking&lt;/h2&gt;
&lt;p&gt;Before preparing a node, create a new security group under Compute -&amp;gt; Access &amp;amp; Security and name it &lt;code&gt;swarm_group&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;We need to be able to have open traffic between the &lt;code&gt;swarmsecgroup&lt;/code&gt; and the group of the Jupyterhub instance, &lt;code&gt;jupyterhubsecgroup&lt;/code&gt; in my previous tutorial. So in the new &lt;code&gt;swarmsecgroup&lt;/code&gt;, add this rule: &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Add Rule&lt;/li&gt;
&lt;li&gt;Rule: ALL TCP&lt;/li&gt;
&lt;li&gt;Direction: Ingress&lt;/li&gt;
&lt;li&gt;Remote: Security Group&lt;/li&gt;
&lt;li&gt;Security Group: &lt;code&gt;jupyterhubsecgroup&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Add another rule replacing Ingress with Egress.
Now open the &lt;code&gt;jupyterhubsecgroup&lt;/code&gt; group and add the same 2 rules, just make sure to choose as target "Security Group" &lt;code&gt;swarmsecgroup&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;On the &lt;code&gt;swarmsecgroup&lt;/code&gt; also add a Rule for SSH traffic from any source choosing CIDR and 0.0.0.0/0, you can disable this after having executed the configuration.&lt;/p&gt;
&lt;h2&gt;Setup the Docker Swarm nodes&lt;/h2&gt;
&lt;h3&gt;Launch a plain Ubuntu instance&lt;/h3&gt;
&lt;p&gt;Launch a new instance, all it &lt;code&gt;swarmnode&lt;/code&gt;, choose the size depending on your requirements, and then choose "Boot from image" and get Ubuntu 14.04 LTS (16.04 should work as well, but I haven't yet tested it). Remember to choose a Key Pair under Access &amp;amp; Security and assign the Security Group &lt;code&gt;swarmsecgroup&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Temporarily add a floating IP to this instance in order to SSH into it, see my first tutorial for more details.&lt;/p&gt;
&lt;h3&gt;Setup Docker Swarm&lt;/h3&gt;
&lt;p&gt;First install Docker engine:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;sudo apt update
sudo apt install apt-transport-https ca-certificates
sudo apt-key adv --keyserver hkp://p80.pool.sks-keyservers.net:80 --recv-keys 58118E89F3A912897C070ADBF76221572C52609D
echo &amp;quot;deb https://apt.dockerproject.org/repo ubuntu-trusty main&amp;quot; | sudo tee /etc/apt/sources.list.d/docker.list 
sudo apt update
sudo apt install -y docker-engine
sudo usermod -aG docker ubuntu
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Then make the same edit we did on the hub, edit &lt;code&gt;/etc/init/docker.conf&lt;/code&gt; and replace &lt;code&gt;DOCKER_OPTS=&lt;/code&gt; in the &lt;code&gt;start&lt;/code&gt; section with:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;DOCKER_OPTS=&amp;quot;-H tcp://0.0.0.0:2375 -H unix:///var/run/docker.sock&amp;quot;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Restart Docker with:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;sudo service docker restart
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Then run the container that interfaces with Swarm:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="x"&gt;HUB_LOCAL_IP=10.XX.XX.XX&lt;/span&gt;
&lt;span class="x"&gt;NODE_LOCAL_IP=&lt;/span&gt;&lt;span class="p"&gt;$(&lt;/span&gt;&lt;span class="err"&gt;ip&lt;/span&gt; &lt;span class="err"&gt;route&lt;/span&gt; &lt;span class="err"&gt;get&lt;/span&gt; &lt;span class="m"&gt;8&lt;/span&gt;&lt;span class="err"&gt;.&lt;/span&gt;&lt;span class="m"&gt;8&lt;/span&gt;&lt;span class="err"&gt;.&lt;/span&gt;&lt;span class="m"&gt;8&lt;/span&gt;&lt;span class="err"&gt;.&lt;/span&gt;&lt;span class="m"&gt;8&lt;/span&gt; &lt;span class="err"&gt;|&lt;/span&gt; &lt;span class="err"&gt;awk&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;NR==1 {print $NF}&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="x"&gt;&lt;/span&gt;
&lt;span class="x"&gt;docker run --restart=always -d swarm join --advertise=&lt;/span&gt;&lt;span class="p"&gt;$&lt;/span&gt;&lt;span class="nv"&gt;NODE_LOCAL_IP&lt;/span&gt;&lt;span class="x"&gt;:2375 consul://&lt;/span&gt;&lt;span class="p"&gt;$&lt;/span&gt;&lt;span class="nv"&gt;HUB_LOCAL_IP&lt;/span&gt;&lt;span class="x"&gt;:8500&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Copy the address of the Jupyterhub server in the &lt;code&gt;HUB_LOCAL_IP&lt;/code&gt; variable.&lt;/p&gt;
&lt;h3&gt;Setup mounting the home filesystem&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;sudo apt-get install autofs
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;add in &lt;code&gt;/etc/auto.master&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;/home         /etc/auto.home
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;create &lt;code&gt;/etc/auto.home&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="x"&gt;echo &amp;quot;* &lt;/span&gt;&lt;span class="p"&gt;$&lt;/span&gt;&lt;span class="nv"&gt;HUB_LOCAL_IP&lt;/span&gt;&lt;span class="x"&gt;:/home/&amp;amp;&amp;quot; | sudo tee /etc/auto.home&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;using the internal IP of the hub.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;sudo service autofs restart
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;verify by doing:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;ls /home/ubuntu
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;or &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;ls /home/training01
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;you should see the same files that were on the Jupyterhub server.&lt;/p&gt;
&lt;h3&gt;Create users&lt;/h3&gt;
&lt;p&gt;As we are using system users and mounting the home filesystem it is important that users have the same UID on all nodes, so we are going to run on the node the same script we ran on the Jupyterhub server:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt; bash create_users.sh
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Test Jupyterhub&lt;/h3&gt;
&lt;p&gt;Login on the Jupyterhub instance with 2 or more different users, then check on the console of the Hub that the containers were launched on the &lt;code&gt;swarmnode&lt;/code&gt; instance:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt; docker -H :4000 ps -a
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Create more nodes&lt;/h2&gt;
&lt;p&gt;Now that we created a fully functioning node we can clone it to create more to accomodate more users.&lt;/p&gt;
&lt;h3&gt;Create a snapshot of the node&lt;/h3&gt;
&lt;p&gt;First we need to delete all Docker containers, ssh into the &lt;code&gt;swarmnode&lt;/code&gt; and execute:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt; docker rm -f $(docker ps -a -q)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Docker has a unique identifying key, we need to remove that so that it will be regenerated by the clones.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;sudo service docker stop
sudo rm /etc/docker/key.json
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Then from Compute-&amp;gt;Instances choose "Create Snapshot", call it &lt;code&gt;swarmnodeimage&lt;/code&gt;.&lt;/p&gt;
&lt;h3&gt;Launch other nodes&lt;/h3&gt;
&lt;p&gt;Click on Launch instance-&amp;gt;"Boot from Snapshot"-&amp;gt;&lt;code&gt;swarmnodeimage&lt;/code&gt;, choose the &lt;code&gt;swarmnodesecgroup&lt;/code&gt; Security Group. Choose any number of instances you need.&lt;/p&gt;
&lt;p&gt;Each node will need to launch the Swarm container with its own local ip, not the same as our first node. Therefore we need to use the "Post Creation"-&amp;gt;"Direct Input" and add this script: &lt;/p&gt;
&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;1
2
3
4&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="ch"&gt;#!/bin/bash&lt;/span&gt;
&lt;span class="nv"&gt;HUB_LOCAL_IP&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;10.XX.XX.XX
&lt;span class="nv"&gt;NODE_LOCAL_IP&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;$(&lt;/span&gt;ip route get 8.8.8.8 &lt;span class="p"&gt;|&lt;/span&gt; awk &lt;span class="s1"&gt;&amp;#39;NR==1 {print $NF}&amp;#39;&lt;/span&gt;&lt;span class="k"&gt;)&lt;/span&gt;
docker run --restart&lt;span class="o"&gt;=&lt;/span&gt;always -d swarm join --advertise&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;$NODE_LOCAL_IP&lt;/span&gt;:2375 consul://&lt;span class="nv"&gt;$HUB_LOCAL_IP&lt;/span&gt;:8500
&lt;/pre&gt;&lt;/div&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;

&lt;p&gt;&lt;code&gt;HUB_LOCAL_IP&lt;/code&gt; is the internal network IP address of the Jupyterhub instance and &lt;code&gt;NODE_LOCAL_IP&lt;/code&gt; will be filled with the IP of the OpenStack image just created.&lt;/p&gt;
&lt;p&gt;See for example Jupyterhub with 3 remote Swarm nodes running containers for 4 training users:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ docker -H :4000 ps -a
CONTAINER ID        IMAGE                                     COMMAND                  CREATED              STATUS              PORTS                         NAMES
60189f208df2        zonca/jupyterhub-datascience-systemuser   &lt;span class="s2"&gt;&amp;quot;tini -- sh /srv/sing&amp;quot;&lt;/span&gt;   &lt;span class="m"&gt;11&lt;/span&gt; seconds ago       Up &lt;span class="m"&gt;7&lt;/span&gt; seconds        10.128.1.28:32769-&amp;gt;8888/tcp   swarmnodes-1/jupyter-training04
1d7b05caedb1        zonca/jupyterhub-datascience-systemuser   &lt;span class="s2"&gt;&amp;quot;tini -- sh /srv/sing&amp;quot;&lt;/span&gt;   &lt;span class="m"&gt;36&lt;/span&gt; seconds ago       Up &lt;span class="m"&gt;32&lt;/span&gt; seconds       10.128.1.27:32768-&amp;gt;8888/tcp   swarmnodes-2/jupyter-training03
733c5ff0a5ed        zonca/jupyterhub-datascience-systemuser   &lt;span class="s2"&gt;&amp;quot;tini -- sh /srv/sing&amp;quot;&lt;/span&gt;   &lt;span class="m"&gt;58&lt;/span&gt; seconds ago       Up &lt;span class="m"&gt;54&lt;/span&gt; seconds       10.128.1.29:32768-&amp;gt;8888/tcp   swarmnodes-3/jupyter-training02
282abce201dd        zonca/jupyterhub-datascience-systemuser   &lt;span class="s2"&gt;&amp;quot;tini -- sh /srv/sing&amp;quot;&lt;/span&gt;   About a minute ago   Up About a minute   10.128.1.28:32768-&amp;gt;8888/tcp   swarmnodes-1/jupyter-training01
29b2d394fab9        swarm                                     &lt;span class="s2"&gt;&amp;quot;/swarm join --advert&amp;quot;&lt;/span&gt;   &lt;span class="m"&gt;13&lt;/span&gt; minutes ago       Up &lt;span class="m"&gt;13&lt;/span&gt; minutes       2375/tcp                      swarmnodes-2/romantic_easley
8fd3d32fe849        swarm                                     &lt;span class="s2"&gt;&amp;quot;/swarm join --advert&amp;quot;&lt;/span&gt;   &lt;span class="m"&gt;13&lt;/span&gt; minutes ago       Up &lt;span class="m"&gt;13&lt;/span&gt; minutes       2375/tcp                      swarmnodes-3/clever_mestorf
1ae073f7b78b        swarm                                     &lt;span class="s2"&gt;&amp;quot;/swarm join --advert&amp;quot;&lt;/span&gt;   &lt;span class="m"&gt;13&lt;/span&gt; minutes ago       Up &lt;span class="m"&gt;13&lt;/span&gt; minutes       2375/tcp                      swarmnodes-1/jovial_goldwasser
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Where to go from here&lt;/h2&gt;
&lt;p&gt;At this level the deployment is quite complicated, so it is probably worth automating it with an &lt;code&gt;ansible&lt;/code&gt; playbook, that will be the subject of the next blog post, I think the result will be a simplified version of &lt;a href="https://github.com/compmodels/jupyterhub-deploy"&gt;Jess Hamrick's compmodels deployment&lt;/a&gt;. Still, I recommend starting with a manual setup to understand how the different pieces work.&lt;/p&gt;
&lt;h2&gt;Troubleshooting&lt;/h2&gt;
&lt;p&gt;If &lt;code&gt;docker -H :4000 ps -a&lt;/code&gt; gives the error:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;Error response from daemon: No elected primary cluster manager
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;it means the Consul container is broken, remove it and create it again.&lt;/p&gt;
&lt;h2&gt;Acknowledgments&lt;/h2&gt;
&lt;p&gt;Thanks to Jess Hamrick for sharing the setup of her &lt;a href="https://github.com/compmodels"&gt;compmodel class on Github&lt;/a&gt;, the Jupyter team for releasing such great tools and Kevin Coakley and the rest of the &lt;a href="http://www.sdsc.edu/services/it/cloud.html"&gt;SDSC Cloud&lt;/a&gt; team for OpenStack support and resources.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Andrea Zonca</dc:creator><pubDate>Tue, 24 May 2016 12:00:00 -0700</pubDate><guid isPermaLink="false">tag:zonca.github.io,2016-05-24:2016/05/jupyterhub-docker-swarm.html</guid><category>ipython</category><category>jupyterhub</category><category>sdsc</category></item><item><title>Quick Jupyterhub deployment for workshops with pre-built image</title><link>http://zonca.github.io/2016/04/jupyterhub-image-sdsc-cloud.html</link><description>&lt;p&gt;This tutorial explains how to use a OpenStack image I already built to quickly deploy a Jupyterhub Virtual Machine that can provide a good initial setup for a workshop, providing students access to Python 2/3, Julia, R, file editor and terminal with bash.&lt;/p&gt;
&lt;p&gt;For details about building the instance yourself for more customization, see the full tutorial at &lt;a href="http://zonca.github.io/2016/04/jupyterhub-sdsc-cloud.html"&gt;http://zonca.github.io/2016/04/jupyterhub-sdsc-cloud.html&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Create a Virtual Machine in OpenStack with the pre-built image&lt;/h2&gt;
&lt;p&gt;Follow the 3 steps at &lt;a href="http://zonca.github.io/2016/04/jupyterhub-sdsc-cloud.html&amp;gt;"&gt;the step by step tutorial&lt;/a&gt; under "Create a Virtual Machine in OpenStack":&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Network setup&lt;/li&gt;
&lt;li&gt;Create a new Virtual Machine: here instead of choosing the base &lt;code&gt;ubuntu&lt;/code&gt; image, choose &lt;code&gt;jupyterhub_docker&lt;/code&gt;, also you can choose any size, I recommend to start with a &lt;code&gt;c1.large&lt;/code&gt; for experimentation, you can then resize it later to a more powerful instance depending on the needs of your workshop&lt;/li&gt;
&lt;li&gt;Give public IP to the instance&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Connect to Jupyterhub&lt;/h2&gt;
&lt;p&gt;The Jupyterhub instance is ready! Just open your browser and connect to the floating IP of the instance you just created.&lt;/p&gt;
&lt;p&gt;The browser should show a security error related to the fact that the pre-installed SSL certificate is not trusted, click on "Advanced properties" and choose to connect anyway, we'll see later how to fix this.&lt;/p&gt;
&lt;p&gt;You already have 50 training users, named &lt;code&gt;training01&lt;/code&gt; to &lt;code&gt;training50&lt;/code&gt;, all with the same password &lt;code&gt;jupyterhubSDSC&lt;/code&gt; (see below how to change it). Check that you can login and create a notebook.&lt;/p&gt;
&lt;h2&gt;Administer the Jupyterhub instance&lt;/h2&gt;
&lt;p&gt;Login into the Virtual Machine with &lt;code&gt;ssh -i jupyterhub.pem ubuntu@xxx.xxx.xxx.xxx&lt;/code&gt; using the key file and the public IP setup in the previous steps.&lt;/p&gt;
&lt;p&gt;To get rid of the annoying "unable to resolve host" warning, add the hostname of the machine (check by running hostname) to &lt;code&gt;/etc/hosts&lt;/code&gt;, i.e. the first line should become something like &lt;code&gt;127.0.0.1 localhost jupyterhub&lt;/code&gt; if jupyterhub is the hostname&lt;/p&gt;
&lt;h3&gt;Change password/add more users&lt;/h3&gt;
&lt;p&gt;In the home folder of the &lt;code&gt;ubuntu&lt;/code&gt; users, there is a file named &lt;code&gt;create_users.sh&lt;/code&gt;, edit it to change the &lt;code&gt;PASSWORD&lt;/code&gt; variable and the number of users from &lt;code&gt;50&lt;/code&gt; to a larger number. Then run it with &lt;code&gt;bash create_users.sh&lt;/code&gt;. Training users &lt;strong&gt;cannot SSH&lt;/strong&gt; into the machine.&lt;/p&gt;
&lt;p&gt;Use &lt;code&gt;sudo passwd trainingXX&lt;/code&gt; to change the password of a single user.&lt;/p&gt;
&lt;h3&gt;Setup a domain (needed for SSL certificate)&lt;/h3&gt;
&lt;p&gt;If you do not know how to get a domain name, here some options:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;you can generally request a subdomain name from your institution, see for example &lt;a href="http://blink.ucsd.edu/technology/help-desk/sysadmin-resources/domain.html#Register-your-domain-name"&gt;UCSD&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;if you own a domain, go in the DNS settings, add a record of type A to a subdomain, like &lt;code&gt;jupyterhub.yourdomain.com&lt;/code&gt; that points to the floating IP of the Jupyterhub instance&lt;/li&gt;
&lt;li&gt;you can get a free dynamic dns at websites like &lt;a href="https://noip.com"&gt;noip.com&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In each case you need to have a DNS record of type A that points to the floating IP of the Jupyterhub instance.&lt;/p&gt;
&lt;h3&gt;Setup a SSL Certificate&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://letsencrypt.org/"&gt;Letsencrypt&lt;/a&gt; provides free SSL certificates by using a command line client.&lt;/p&gt;
&lt;p&gt;SSH into the server, run:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;git clone https://github.com/letsencrypt/letsencrypt
cd letsencrypt
sudo service nginx stop
./letsencrypt-auto certonly --standalone -d jupyterhubdeploy.ddns.net
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Follow instructions at the terminal to obtain a certificate&lt;/p&gt;
&lt;p&gt;Now open the nginx configuration file: &lt;code&gt;sudo vim /etc/nginx/nginx.conf&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;And modify the SSL certificate lines:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;ssl_certificate /etc/letsencrypt/live/yoursub.domain.edu/cert.pem;
ssl_certificate_key /etc/letsencrypt/live/yoursub.domain.edu/privkey.pem;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Start NGINX:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;sudo service nginx start
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Connect again to Jupyterhub and check that your browser correctly detects that the HTTPS connection is safe.&lt;/p&gt;
&lt;h2&gt;Comments? Suggestions?&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://twitter.com/andreazonca"&gt;Twitter&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Email &lt;code&gt;zonca&lt;/code&gt; on the domain &lt;code&gt;sdsc.edu&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Andrea Zonca</dc:creator><pubDate>Thu, 28 Apr 2016 12:00:00 -0700</pubDate><guid isPermaLink="false">tag:zonca.github.io,2016-04-28:2016/04/jupyterhub-image-sdsc-cloud.html</guid><category>ipython</category><category>jupyterhub</category><category>sdsc</category></item><item><title>Deploy Jupyterhub on a Virtual Machine for a Workshop</title><link>http://zonca.github.io/2016/04/jupyterhub-sdsc-cloud.html</link><description>&lt;p&gt;This tutorial describes the steps to install a Jupyterhub instance on a single machine suitable for hosting a workshop, suitable for having people login with training accounts on Jupyter Notebooks running Python 2/3, R, Julia with also Terminal access on Docker containers.
Details about the setup:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Jupyterhub installed with Anaconda directly on the host, proxied by NGINX under HTTPS with self-signed certificate&lt;/li&gt;
&lt;li&gt;Login with Linux account credentials created previously by the administrator, data in /home are persistent across sessions&lt;/li&gt;
&lt;li&gt;Each user runs in a separated Docker container with access to Python 2, Python 3, R and Julia kernels, they can also open the Notebook editor and the terminal&lt;/li&gt;
&lt;li&gt;Using a single machine you have to consider that the biggest constraint is going to be memory usage, as a rule of thumb consider 100-200 MB/user plus 5x-10x the amount of data you are loading from disk, depending on the kind of analysis. For a multi-node setup you need to look into Docker Swarm.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I am using the OpenStack deployment at the San Diego Supercomputer Center, &lt;a href="http://www.sdsc.edu/services/it/cloud.html"&gt;SDSC Cloud&lt;/a&gt;, AWS deployments should just replace the first section on Creating a VM and setting up Networking, see &lt;a href="https://github.com/jupyterhub/jupyterhub/wiki/Deploying-JupyterHub-on-AWS"&gt;the Jupyterhub wiki&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If you intend to run on SDSC Cloud, I have a pre-built image of this deployment you can setup and run quickly, see &lt;a href="http://zonca.github.io/2016/04/jupyterhub-image-sdsc-cloud.html"&gt;see my followup tutorial&lt;/a&gt;.&lt;/p&gt;
&lt;h1&gt;Create a Virtual Machine in OpenStack&lt;/h1&gt;
&lt;p&gt;First of all we need to launch a new Virtual Machine and configure the network.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Login to the SDSC Cloud OpenStack dashboard&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Network setup&lt;/h2&gt;
&lt;p&gt;Jupyterhub will be proxied to the standard HTTPS port by NGINX and we also want to redirect HTTP to HTTPS, so we open those ports, then SSH for the administrators to login and a custom TCP rule in order for the Docker containers to be able to connect to the Jupyterhub hub running on port 8081, so we are opening that port just to the subnet that is running the Docker containers.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Compute -&amp;gt; Access &amp;amp; Security -&amp;gt; Security Groups -&amp;gt; Create Security Group and name it &lt;code&gt;jupyterhubsecgroup&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Click on Manage Rules &lt;/li&gt;
&lt;li&gt;Click on add rule, choose the HTTP rule and click add&lt;/li&gt;
&lt;li&gt;Repeat the last step with HTTPS and SSH&lt;/li&gt;
&lt;li&gt;Click on add rule again, choose Custom TCP Rule, set port 8081 and set CIDR 172.17.0.0/24 (this is needed so that the containers can connect to the hub)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Create a new Virtual Machine&lt;/h2&gt;
&lt;p&gt;We choose Ubuntu here, also other distributions should work fine.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Compute -&amp;gt; Access &amp;amp; Security -&amp;gt; Key Pairs -&amp;gt; Create key pair, name it &lt;code&gt;jupyterhub&lt;/code&gt; and download it to your local machine&lt;/li&gt;
&lt;li&gt;Instances -&amp;gt; Launch Instance, Choose a name, Choose "Boot from image" in Boot Source and Ubuntu as Image name, Choose any size, depending on the number of users (TODO add link to Jupyterhub docs)&lt;/li&gt;
&lt;li&gt;Under "Access &amp;amp; Security" choose Key Pair &lt;code&gt;jupyterhub&lt;/code&gt; and Security Groups &lt;code&gt;jupyterhubsecgroup&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Click &lt;code&gt;Launch&lt;/code&gt; to create the instance&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Give public IP to the instance&lt;/h2&gt;
&lt;p&gt;By default in SDSC Cloud machines do not have a public IP.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Compute -&amp;gt; Access &amp;amp; Sewcurity -&amp;gt; Floating IPs -&amp;gt; Allocate IP To Project, "Allocate IP" to request a public IP&lt;/li&gt;
&lt;li&gt;Click on the "Associate" button of the IP just requested and under "Port to be associated"  choose the instance just created&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;Setup Jupyterhub in the Virtual Machine&lt;/h1&gt;
&lt;p&gt;In this section we will install and configure Jupyterhub and NGINX to run on the Virtual Machine.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;login into the Virtual Machine with &lt;code&gt;ssh -i jupyterhub.pem ubuntu@xxx.xxx.xxx.xxx&lt;/code&gt; using the key file and the public IP setup in the previous steps&lt;/li&gt;
&lt;li&gt;add the hostname of the machine (check by running &lt;code&gt;hostname&lt;/code&gt;) to &lt;code&gt;/etc/hosts&lt;/code&gt;, i.e. the first line should become something like &lt;code&gt;127.0.0.1 localhost jupyterhub&lt;/code&gt; if &lt;code&gt;jupyterhub&lt;/code&gt; is the hostname&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Setup Jupyterhub&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt; wget --no-check-certificate https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh
 bash Miniconda3-latest-Linux-x86_64.sh
 ```

 use all defaults, answer &amp;quot;yes&amp;quot; to modify PATH

 ```
sudo apt-get install npm nodejs-legacy
sudo npm install -g configurable-http-proxy
conda install traitlets tornado jinja2 sqlalchemy 
pip install jupyterhub
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;For authentication to work, the &lt;code&gt;ubuntu&lt;/code&gt; user needs to be able to read the &lt;code&gt;/etc/shadow&lt;/code&gt; file:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;sudo adduser ubuntu shadow
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Setup the web server&lt;/h2&gt;
&lt;p&gt;We will use the NGINX web server to proxy Jupyterhub and handle HTTPS for us, this is recommended for deployments on the public internet.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;sudo apt install nginx
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;SSL Certificate&lt;/strong&gt;: Optionally later, once we have assigned a domain to the Virtual Machine, we can install &lt;code&gt;letsencrypt&lt;/code&gt; and get a real certificate, &lt;a href="http://zonca.github.io/2016/04/jupyterhub-image-sdsc-cloud.html"&gt;see my followup tutorial&lt;/a&gt;, for simplicity here we are just using self-signed certificates that will give warnings on the first time users connect to the server, but still will keep the traffic encrypted.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;sudo mkdir /etc/nginx/ssl
sudo openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout /etc/nginx/ssl/nginx.key -out /etc/nginx/ssl/nginx.crt
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Get &lt;code&gt;/etc/nginx/nginx.conf&lt;/code&gt; from https://gist.github.com/zonca/08c413a37401bdc9d2a7f65a7af44462&lt;/p&gt;
&lt;h1&gt;Setup Docker Spawner&lt;/h1&gt;
&lt;p&gt;By default Jupyterhub runs notebooks as processes owned by each system user, for more security and isolation, we want Notebook to run in Docker containers, which are something like lightweight Virtual Machines running inside our server.&lt;/p&gt;
&lt;h2&gt;Install Docker&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Source: https://docs.docker.com/engine/installation/linux/ubuntulinux/#prerequisites&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;sudo apt update
sudo apt install apt-transport-https ca-certificates
sudo apt-key adv --keyserver hkp://p80.pool.sks-keyservers.net:80 --recv-keys 58118E89F3A912897C070ADBF76221572C52609D
echo &amp;quot;deb https://apt.dockerproject.org/repo ubuntu-trusty main&amp;quot; | sudo tee /etc/apt/sources.list.d/docker.list 
sudo apt update
sudo apt install docker-engine
sudo usermod -aG docker ubuntu
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Logout and login again for the group to take effect&lt;/p&gt;
&lt;h2&gt;Install and configure DockerSpawner&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;pip install dockerspawner
docker pull jupyter/systemuser
conda install ipython jupyter
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Create &lt;code&gt;jupyterhub_config.py&lt;/code&gt; in the home folder of the ubuntu user with this content:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;JupyterHub&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;confirm_no_ssl&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;True&lt;/span&gt;
&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;JupyterHub&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;spawner_class&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;dockerspawner.SystemUserSpawner&amp;#39;&lt;/span&gt;

&lt;span class="c1"&gt;# The docker instances need access to the Hub, so the default loopback port doesn&amp;#39;t work:&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;IPython.utils.localinterfaces&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;public_ips&lt;/span&gt;
&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;JupyterHub&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;hub_ip&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;public_ips&lt;/span&gt;&lt;span class="p"&gt;()[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h1&gt;Connect to Jupyterhub&lt;/h1&gt;
&lt;p&gt;From the home folder of the &lt;code&gt;ubuntu&lt;/code&gt; user, type &lt;code&gt;jupyterhub&lt;/code&gt; to launch the Jupyterhub process, see below how to start it automatically at boot. Use CTRL-C to stop it.&lt;/p&gt;
&lt;p&gt;Open a browser and connect to the floating IP you set for your instance, this should redirect to the https, click "Advanced" in the warning about safety due to the self signed SSL certificate and login with the training credentials.&lt;/p&gt;
&lt;p&gt;Instead of using the IP, you can use any domain that points to that same IP with a DNS record of type A or get a dymanic DNS for free on a website like http://noip.com.
Once you have a custom domain, you can configure letsencrypt to have a proper HTTPS certificate so that users do not get any warning when connecting to the instance. I will add this to the optional steps below.&lt;/p&gt;
&lt;h1&gt;Optional: Automatically start jupyterhub at boot&lt;/h1&gt;
&lt;p&gt;Save https://gist.github.com/zonca/aaeaf3c4e7339127b482d759866e5f39 as &lt;code&gt;/etc/init.d/jupyterhub&lt;/code&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;sudo chmod +x /etc/init.d/jupyterhub
sudo service jupyterhub start
sudo update-rc.d jupyterhub defaults
&lt;/pre&gt;&lt;/div&gt;


&lt;h1&gt;Optional: Create training user accounts&lt;/h1&gt;
&lt;p&gt;Add user accounts on Jupyterhub creating standard Linux users with &lt;code&gt;adduser&lt;/code&gt; interactively or with a batch script.&lt;/p&gt;
&lt;p&gt;For example the following batch script creates 10 users all with the same password:&lt;/p&gt;
&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt;1
2
3
4
5
6
7
8&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="ch"&gt;#!/bin/bash&lt;/span&gt;
&lt;span class="nv"&gt;PASSWORD&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;samepasswordforallusers
&lt;span class="nv"&gt;NUMBER_OF_USERS&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;10
&lt;span class="k"&gt;for&lt;/span&gt; n in &lt;span class="sb"&gt;`&lt;/span&gt;seq -f &lt;span class="s2"&gt;&amp;quot;%02g&amp;quot;&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt; &lt;span class="nv"&gt;$NUMBER_OF_USERS&lt;/span&gt;&lt;span class="sb"&gt;`&lt;/span&gt;
&lt;span class="k"&gt;do&lt;/span&gt;
    &lt;span class="nb"&gt;echo&lt;/span&gt; creating user training&lt;span class="nv"&gt;$n&lt;/span&gt;
    &lt;span class="nb"&gt;echo&lt;/span&gt; training&lt;span class="nv"&gt;$n&lt;/span&gt;:&lt;span class="nv"&gt;$PASSWORD&lt;/span&gt;::::/home/training&lt;span class="nv"&gt;$n&lt;/span&gt;:/bin/bash &lt;span class="p"&gt;|&lt;/span&gt; sudo newusers
&lt;span class="k"&gt;done&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;

&lt;p&gt;Also add &lt;code&gt;AllowUsers ubuntu&lt;/code&gt; to &lt;code&gt;/etc/ssh/sshd_config&lt;/code&gt; so that training users cannot SSH into the host machine.&lt;/p&gt;
&lt;h1&gt;Optional: Add the R and Julia kernels&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;SSH into the instance&lt;/li&gt;
&lt;li&gt;&lt;code&gt;git clone https://github.com/jupyter/dockerspawner&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;cd dockerspawner&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Modify the file &lt;code&gt;singleuser/Dockerfile&lt;/code&gt;, replace &lt;code&gt;FROM jupyter/scipy-notebook&lt;/code&gt; with &lt;code&gt;FROM jupyter/datascience-notebook&lt;/code&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;docker build -t datascience-singleuser singleuser
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Modify the file &lt;code&gt;systemuser/Dockerfile&lt;/code&gt;, replace &lt;code&gt;FROM jupyter/singleuser&lt;/code&gt; with &lt;code&gt;FROM datascience-singleuser&lt;/code&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;docker build -t datascience-systemuser systemuser
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Finally in &lt;code&gt;jupyterhub_config.py&lt;/code&gt;, select the new docker image:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;c.DockerSpawner.container_image = &amp;quot;datascience-systemuser&amp;quot;
&lt;/pre&gt;&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Andrea Zonca</dc:creator><pubDate>Sat, 16 Apr 2016 12:00:00 -0700</pubDate><guid isPermaLink="false">tag:zonca.github.io,2016-04-16:2016/04/jupyterhub-sdsc-cloud.html</guid><category>ipython</category><category>jupyterhub</category><category>sdsc</category></item><item><title>Use your own Python installation (kernel) in Jupyterhub</title><link>http://zonca.github.io/2015/10/use-own-python-in-jupyterhub.html</link><description>&lt;p&gt;You have access to a Jupyterhub server but the Python installation provided does not satisfy your needs,
how to use your own?&lt;/p&gt;
&lt;h2&gt;Install Anaconda&lt;/h2&gt;
&lt;p&gt;If you haven't already your own Python installation on the Jupyterhub server you have access to, you can install Anaconda in your home folder. I assume here you have a permanent home folder on the server.&lt;/p&gt;
&lt;p&gt;In order to type commands, you can either
get a Jupyterhub Terminal, or run in the IPython notebook with &lt;code&gt;!&lt;/code&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;!wget https://repo.continuum.io/archive/Anaconda3-2.3.0-Linux-x86_64.sh&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;!bash ./Anacon*&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Create a kernel file for Jupyterhub&lt;/h2&gt;
&lt;p&gt;You probably already know you can have Python 2 and Python 3 kernels on the same Jupyter notebook installation. In the same way you can create your own &lt;code&gt;KernelSpec&lt;/code&gt; that launches instead another Python installation.&lt;/p&gt;
&lt;p&gt;IPython can automatically create a &lt;code&gt;KernelSpec&lt;/code&gt; for you, from the IPython notebook, run:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;!~/anaconda3/bin/ipython kernelspec install-self --user
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;In case your path is different, just insert the full path to &lt;code&gt;ipython&lt;/code&gt; from the Python installation you would like to use.&lt;/p&gt;
&lt;p&gt;This will create a file &lt;code&gt;kernel.json&lt;/code&gt; in &lt;code&gt;~/.ipython/kernels/python3&lt;/code&gt;, you can open that file with an editor and change the &lt;code&gt;display_name&lt;/code&gt; to something better than the default &lt;code&gt;Python 3&lt;/code&gt;, like &lt;code&gt;My Anaconda&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;I also recommend to rename the &lt;code&gt;~/.ipython/kernels/python3&lt;/code&gt;, for example to  &lt;code&gt;~/.ipython/kernels/anaconda3&lt;/code&gt;, otherwise if you install another Python 3 kernel, the previous one will be overwritten.&lt;/p&gt;
&lt;p&gt;You can also add KernelSpecs for other &lt;code&gt;conda&lt;/code&gt; environments doing:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="sx"&gt;!source activate environmentname&lt;/span&gt;
&lt;span class="sx"&gt;!ipython kernelspec install-self --user&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;And then edit their &lt;code&gt;kernel.json&lt;/code&gt;.&lt;/p&gt;
&lt;h2&gt;Launch a Notebook&lt;/h2&gt;
&lt;p&gt;Go back to the Jupyterhub dashboard, reload the page, now you should have another option in the &lt;code&gt;New&lt;/code&gt; menu that says &lt;code&gt;My Anaconda&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;In order to use your new kernel with an existing notebook, click on the notebook file in the dashboard, it will launch with the default kernel, then you can change kernel from the top menu &lt;code&gt;Kernel&lt;/code&gt; &amp;gt; &lt;code&gt;Change kernel&lt;/code&gt;.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Andrea Zonca</dc:creator><pubDate>Mon, 05 Oct 2015 12:00:00 -0700</pubDate><guid isPermaLink="false">tag:zonca.github.io,2015-10-05:2015/10/use-own-python-in-jupyterhub.html</guid><category>ipython</category><category>jupyterhub</category></item><item><title>IPython/Jupyter notebook setup on NERSC Edison</title><link>http://zonca.github.io/2015/09/ipython-jupyter-notebook-nersc-edison.html</link><description>&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;This tutorial explains the setup to run an IPython Notebook on a computing node on the supercomputer Edison at NERSC and forward its port encrypted with SSH to the browser on a local laptop.
This setup is a bit more complicated than other supercomputers, i.e. see &lt;a href="http://zonca.github.io/2015/09/ipython-jupyter-notebook-sdsc-comet.html"&gt;my tutorial for Comet&lt;/a&gt; for 2 reasons:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Edison's computing nodes run a stripped down OS, with no support for SSH, unless you activate &lt;a href="https://www.nersc.gov/users/computational-systems/hopper/cluster-compatibility-mode/"&gt;Cluster Compatibility Mode&lt;/a&gt; (CCM) &lt;/li&gt;
&lt;li&gt;On edison you generally don't have direct access to a computing node, even if you request an interactive node you actually have access to an intermediary node (MOM node), from there &lt;code&gt;aprun&lt;/code&gt; sends a job for execution on the computing node.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Quick reference&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Install IPython notebook and make sure it is in the path, I recommend to install Anaconda 64bit in your home folder or on scratch.&lt;/li&gt;
&lt;li&gt;Make sure you can ssh passwordless within Edison, i.e. &lt;code&gt;ssh edison&lt;/code&gt; from Edison  login node works without password&lt;/li&gt;
&lt;li&gt;Create a folder &lt;code&gt;notebook&lt;/code&gt; in your home, get &lt;code&gt;notebook_job.pbs&lt;/code&gt; and &lt;code&gt;launch_notebook_and_tunnel_to_login.sh&lt;/code&gt; from &lt;a href="https://gist.github.com/zonca/357d36347fd5addca8f0"&gt;https://gist.github.com/zonca/357d36347fd5addca8f0&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Change the port number and customize options (duration)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;qsub notebook_job.pbs&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;From laptop, launch &lt;code&gt;bash tunnel_laptop_edisonlogin.sh ##&lt;/code&gt; from &lt;a href="https://gist.github.com/zonca/5f8b5ccb826a774d3f89"&gt;https://gist.github.com/zonca/5f8b5ccb826a774d3f89&lt;/a&gt;, where &lt;code&gt;##&lt;/code&gt; is the edison login number in 2 digits, like &lt;code&gt;03&lt;/code&gt;. First you need to modify the port number.&lt;/li&gt;
&lt;li&gt;From laptop, open browser and connect to &lt;code&gt;http://localhost:YOURPORT&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Detailed walkthrough&lt;/h2&gt;
&lt;h3&gt;One time setup on Edison&lt;/h3&gt;
&lt;p&gt;Make sure that &lt;code&gt;ipython notebook&lt;/code&gt; works on a login node, one option is to install 
Anaconda 64bit from http://continuum.io/downloads#py34. Choose Python 3.&lt;/p&gt;
&lt;p&gt;You need to be able to SSH from a node to another node on Edison with no need of a password. Create a new SSH certificate with &lt;code&gt;ssh-keygen&lt;/code&gt;, hit enter to keep all default options, DO NOT ENTER A PASSWORD. Then use &lt;code&gt;ssh-copy-id edison.nersc.gov&lt;/code&gt;, enter your password to make sure the key is copied in the authorized hosts.
Now you can check it works by executing:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;ssh edison.nersc.gov
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;from the login node and make sure you are NOT asked for your password.&lt;/p&gt;
&lt;h3&gt;Configure the script for TORQUE and submit the job&lt;/h3&gt;
&lt;p&gt;Create a &lt;code&gt;notebook&lt;/code&gt; folder on your home on Edison.&lt;/p&gt;
&lt;p&gt;Copy &lt;code&gt;notebook_job.pbs&lt;/code&gt; and &lt;code&gt;launch_notebook_and_tunnel_to_login.sh&lt;/code&gt; from &lt;a href="https://gist.github.com/zonca/357d36347fd5addca8f0"&gt;https://gist.github.com/zonca/357d36347fd5addca8f0&lt;/a&gt; to the &lt;code&gt;notebook&lt;/code&gt; folder.&lt;/p&gt;
&lt;p&gt;Change the port number in the &lt;code&gt;launch_notebook_and_tunnel_to_login.sh&lt;/code&gt; script to a port of your choosing between 7000 and 9999, referenced as YOURPORT in the rest of the tutorial. Two users on the same login node on the same port would not be allowed to forward, so try to avoid common port numbers as 8000, 9000, 8080 or 8888.&lt;/p&gt;
&lt;p&gt;Choose a duration of your job, for initial testing better keep 30 minutes so your job starts sooner.&lt;/p&gt;
&lt;p&gt;Submit the job to the scheduler:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;qsub notebook_job.pbs
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Wait for the job to start running, you should see &lt;code&gt;R&lt;/code&gt; in:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="x"&gt;qstat -u &lt;/span&gt;&lt;span class="p"&gt;$&lt;/span&gt;&lt;span class="nv"&gt;USER&lt;/span&gt;&lt;span class="x"&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The script launches an IPython notebook on a computing node and tunnels its port to the login node.&lt;/p&gt;
&lt;p&gt;You can check that everything worked by checking that no errors show up in the &lt;code&gt;notebook.log&lt;/code&gt; file, and that you can access the notebook page with &lt;code&gt;wget&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;wget localhost:YOURPORT
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;should download a &lt;code&gt;index.html&lt;/code&gt; file in the current folder, and NOT give an error like "Connection refused".&lt;/p&gt;
&lt;h3&gt;Tunnel the port to your laptop&lt;/h3&gt;
&lt;h4&gt;Linux / MAC&lt;/h4&gt;
&lt;p&gt;Download the &lt;code&gt;tunnel_laptop_edisonlogin.sh&lt;/code&gt; script from &lt;a href="https://gist.github.com/zonca/357d36347fd5addca8f0"&gt;https://gist.github.com/zonca/357d36347fd5addca8f0&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Customize the script with your port number and your username.&lt;/p&gt;
&lt;p&gt;Launch &lt;code&gt;bash tunnel_laptop_edisonlogin.sh ##&lt;/code&gt; where &lt;code&gt;##&lt;/code&gt; is the Edison login node you launched the job from in 2 digits, e.g. &lt;code&gt;03&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The script forwards the port from the login node of Edison to your laptop.&lt;/p&gt;
&lt;h4&gt;Windows&lt;/h4&gt;
&lt;p&gt;Install &lt;code&gt;putty&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Follow tutorial for local port forwarding on &lt;a href="http://howto.ccs.neu.edu/howto/windows/ssh-port-tunneling-with-putty/"&gt;http://howto.ccs.neu.edu/howto/windows/ssh-port-tunneling-with-putty/&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;set &lt;code&gt;edison##-eth5.nersc.gov&lt;/code&gt; as remote host, where &lt;code&gt;##&lt;/code&gt; is the Edison login node you launched the job from in 2 digits, e.g. &lt;code&gt;03&lt;/code&gt; and set 22 as SSH port&lt;/li&gt;
&lt;li&gt;set YOURPORT as tunnel port, replace both 8080 and 80 in the tutorial with your port number. &lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Connect to the Notebook&lt;/h3&gt;
&lt;p&gt;Open a browser and type &lt;code&gt;http://localhost:YOURPORT&lt;/code&gt; in the address bar.&lt;/p&gt;
&lt;p&gt;See in the screenshot from my local browser, the &lt;code&gt;hostname&lt;/code&gt; is one of Edison's computing node:&lt;/p&gt;
&lt;p&gt;&lt;img alt="test_edison_screenshot.png" src="/images/test_edison_screenshot.png" /&gt;&lt;/p&gt;
&lt;h2&gt;Acknowledgements&lt;/h2&gt;
&lt;p&gt;Thanks Lisa Gerhardt from NERSC user support to help me understand Edison's configuration.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Andrea Zonca</dc:creator><pubDate>Thu, 24 Sep 2015 20:00:00 -0700</pubDate><guid isPermaLink="false">tag:zonca.github.io,2015-09-24:2015/09/ipython-jupyter-notebook-nersc-edison.html</guid><category>ipython</category><category>jupyter</category><category>ipython-notebook</category></item><item><title>IPython/Jupyter notebook setup on SDSC Comet</title><link>http://zonca.github.io/2015/09/ipython-jupyter-notebook-sdsc-comet.html</link><description>&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;This tutorial explains the setup to run an IPython Notebook on a computing node on the supercomputer Comet at the San Diego Supercomputer Center and forward the port encrypted with SSH to the browser on a local laptop.&lt;/p&gt;
&lt;h2&gt;Quick reference&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Add &lt;code&gt;module load python scipy&lt;/code&gt; to &lt;code&gt;.bashrc&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Make sure you can ssh passwordless within comet, i.e. &lt;code&gt;ssh comet.sdsc.edu&lt;/code&gt; from comet  login node works without password&lt;/li&gt;
&lt;li&gt;Get &lt;code&gt;submit_slurm_comet.sh&lt;/code&gt; from &lt;a href="https://gist.github.com/zonca/5f8b5ccb826a774d3f89"&gt;https://gist.github.com/zonca/5f8b5ccb826a774d3f89&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Change the port number and customize options (duration)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;sbatch submit_slurm_comet.sh&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Remember the login node you are using&lt;/li&gt;
&lt;li&gt;From laptop, use &lt;code&gt;bash tunnel_notebook_comet.sh N&lt;/code&gt; where N is the Comet login number (e.g. 2) from &lt;a href="https://gist.github.com/zonca/5f8b5ccb826a774d3f89"&gt;https://gist.github.com/zonca/5f8b5ccb826a774d3f89&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;From laptop, open browser and connect to &lt;code&gt;http://localhost:YOURPORT&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Detailed walkthrough&lt;/h2&gt;
&lt;h3&gt;One time setup on Comet&lt;/h3&gt;
&lt;p&gt;Login into a Comet login node, edit the &lt;code&gt;.bashrc&lt;/code&gt; file in your home folder (with &lt;code&gt;nano .bashrc&lt;/code&gt; for example) and add &lt;code&gt;module load python scipy&lt;/code&gt; at the bottom. This makes sure you always have the Python environment loaded in all your jobs. Logout, log back in, make sure that &lt;code&gt;module list&lt;/code&gt; shows &lt;code&gt;python&lt;/code&gt; and &lt;code&gt;scipy&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;You need to be able to SSH from a node to another node on comet with no need of a password. Create a new SSH certificate with &lt;code&gt;ssh-keygen&lt;/code&gt;, hit enter to keep all default options, DO NOT ENTER A PASSWORD. Then use &lt;code&gt;ssh-copy-id comet.sdsc.edu&lt;/code&gt;, enter your password to make sure the key is copied in the authorized hosts.
Now you can check it works by executing:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;ssh comet.sdsc.edu
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;from the login node and make sure you are NOT asked for your password.&lt;/p&gt;
&lt;h3&gt;Configure the script for SLURM and submit the job&lt;/h3&gt;
&lt;p&gt;Copy &lt;code&gt;submit_slurm_comet.sh&lt;/code&gt; from &lt;a href="https://gist.github.com/zonca/5f8b5ccb826a774d3f89"&gt;https://gist.github.com/zonca/5f8b5ccb826a774d3f89&lt;/a&gt; on your home on Comet.&lt;/p&gt;
&lt;p&gt;Change the port number in the script to a port of your choosing between 8000 and 9999, referenced as YOURPORT in the rest of the tutorial. Two users on the same login node on the same port would not be allowed to forward, so try to avoid common port numbers as 8000, 9000, 8080 or 8888.&lt;/p&gt;
&lt;p&gt;Choose whether you prefer to use a full node to have access to all 24 cores and 128GB of RAM or if you only need 1 core and 5GB of RAM and change the top of the script accordingly.&lt;/p&gt;
&lt;p&gt;Choose a duration of your job, for initial testing better keep 30 minutes so your job starts straight away.&lt;/p&gt;
&lt;p&gt;Submit the job to the scheduler:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;sbatch submit_slurm_comet.sh
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Wait for the job to start running, you should see &lt;code&gt;R&lt;/code&gt; in:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="x"&gt;squeue -u &lt;/span&gt;&lt;span class="p"&gt;$&lt;/span&gt;&lt;span class="nv"&gt;USER&lt;/span&gt;&lt;span class="x"&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The script launches an IPython notebook on a computing node and tunnels its port to the login node.&lt;/p&gt;
&lt;p&gt;You can check that everything worked by checking that no errors show up in the &lt;code&gt;notebook.log&lt;/code&gt; file, and that you can access the notebook page with &lt;code&gt;wget&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;wget localhost:YOURPORT
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;should download a &lt;code&gt;index.html&lt;/code&gt; file in the current folder, and NOT give an error like "Connection refused".&lt;/p&gt;
&lt;p&gt;Check what login node you were using on comet, i.e. the hostname on your terminal on comet, for example &lt;code&gt;comet-ln2&lt;/code&gt;.&lt;/p&gt;
&lt;h3&gt;Tunnel the port to your laptop&lt;/h3&gt;
&lt;h4&gt;Linux / MAC&lt;/h4&gt;
&lt;p&gt;Download the &lt;code&gt;tunnel_notebook_comet.sh&lt;/code&gt; script from &lt;a href="https://gist.github.com/zonca/5f8b5ccb826a774d3f89"&gt;https://gist.github.com/zonca/5f8b5ccb826a774d3f89&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Customize the script with your port number.&lt;/p&gt;
&lt;p&gt;Lauch &lt;code&gt;bash tunnel_notebook_comet.sh N&lt;/code&gt; where N is the comet login node number. So if you were on &lt;code&gt;comet-ln2&lt;/code&gt;, use &lt;code&gt;bash tunnel_notebook_comet.sh 2&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The script forwards the port from the login node of comet to your laptop.&lt;/p&gt;
&lt;h4&gt;Windows&lt;/h4&gt;
&lt;p&gt;Install &lt;code&gt;putty&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Follow tutorial for local port forwarding on &lt;a href="http://howto.ccs.neu.edu/howto/windows/ssh-port-tunneling-with-putty/"&gt;http://howto.ccs.neu.edu/howto/windows/ssh-port-tunneling-with-putty/&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;set &lt;code&gt;comet-ln2.sdsc.edu&lt;/code&gt; as remote host, 22 as SSH port&lt;/li&gt;
&lt;li&gt;set YOURPORT as tunnel port, replace both 8080 and 80 in the tutorial with your port number. &lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Connect to the Notebook&lt;/h3&gt;
&lt;p&gt;Open a browser and type &lt;code&gt;http://localhost:YOURPORT&lt;/code&gt; in the address bar.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Andrea Zonca</dc:creator><pubDate>Thu, 17 Sep 2015 20:00:00 -0700</pubDate><guid isPermaLink="false">tag:zonca.github.io,2015-09-17:2015/09/ipython-jupyter-notebook-sdsc-comet.html</guid><category>ipython</category><category>jupyter</category><category>ipython-notebook</category></item><item><title>Run Jupyterhub on a Supercomputer</title><link>http://zonca.github.io/2015/04/jupyterhub-hpc.html</link><description>&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Summary&lt;/strong&gt;: I developed a plugin for &lt;a href="https://github.com/jupyter/jupyterhub" title="jupyterhub"&gt;Jupyterhub&lt;/a&gt;: &lt;a href="https://github.com/zonca/remotespawner"&gt;RemoteSpawner&lt;/a&gt;, it has a proof-of-concept interface with the Supercomputer Gordon at UC San Diego to spawn IPython Notebook instances as jobs throught the queue and tunnel the interface back to the Jupyterhub instance.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The IPython (recently renamed Jupyter) Notebook is a powerful tool for analyzing and visualizing data in Python and other programming languages.
A key feature is that a single document contains code, figures, text and equations.
Everything is saved in a single .ipynb file that can be shared, executed and modified. See an &lt;a href="http://nbviewer.ipython.org/github/waltherg/notebooks/blob/master/2013-12-03-Crank_Nicolson.ipynb" title="example notebook"&gt;example Notebook on integration of partial differential equations&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The Jupyter Notebook is a Python application with a web frontend, i.e. the interface runs in the user browser.
This setup makes it suitable for any kind of remote computing, in particular running the Jupyter Notebook on a computing node of a Supercomputer, and exporting the interface HTTP port to a local browser.
Setting up tunneling via SSH is tedious, in particular if the user does not have a public IP address.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/jupyter/jupyterhub" title="jupyterhub"&gt;Jupyterhub&lt;/a&gt;, developed by the Jupyter team, comes to the rescue by providing a web application that manages and proxies multiple instances of the Jupyter Notebook for any number of users.
Jupyterhub natively only spawns local processes, but supports plugins to extend its functionality.&lt;/p&gt;
&lt;p&gt;I have been developing a proof-of-concept plugin (&lt;a href="https://github.com/zonca/remotespawner"&gt;RemoteSpawner&lt;/a&gt;) designed to work on a web server and once a user is authenticated, connect to the login node of a Supercomputer and submit a Jupyter Notebook job.
As soon as the job starts execution, it sets up SSH tunneling with the Jupyterhub host so that
Jupyterhub can provide the Notebook interface to the user.
This setup allows users to simply access a Supercomputer via browser, accessing all their Python environment and data.&lt;/p&gt;
&lt;p&gt;I am looking for interested parties either as users or as collaborators to help further development. See more information about the project below.&lt;/p&gt;
&lt;h2&gt;Test it yourself&lt;/h2&gt;
&lt;p&gt;In order to have a feeling on how Jupyterhub works, you can test in your browser at:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://tmpnb.org"&gt;http://tmpnb.org&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This service by Rackspace creates temporary Jupyter Notebooks on the fly. If you click on &lt;code&gt;Welcome.ipynb&lt;/code&gt;,
you can see an example Notebook.&lt;/p&gt;
&lt;p&gt;The purpose of my project is to have a web interface to access Jupyter Notebooks that are
running on computing nodes of a Supercomputer. So that users can access the environment and
data on a Supercomputer from their browser and run data-intensive processing. &lt;/p&gt;
&lt;h2&gt;Tour of Jupyterhub on the Gordon Supercomputer&lt;/h2&gt;
&lt;p&gt;I'll show some screenshots to display how a test Jupyterhub installation on my machine is integrated with &lt;a href="http://www.sdsc.edu/us/resources/gordon/"&gt;Gordon&lt;/a&gt; thanks to the plugin.&lt;/p&gt;
&lt;p&gt;Jupyterhub is accessed publicly via browser and the user can login. Jupyterhub supports authentication for &lt;code&gt;PAM&lt;/code&gt;/&lt;code&gt;LDAP&lt;/code&gt; so it could be integrated with XSEDE credential, at the moment I am testing with local authentication.&lt;/p&gt;
&lt;p&gt;&lt;img alt="jupyterhub-hpc-login.png" src="/images/jupyterhub-hpc-login.png" /&gt;&lt;/p&gt;
&lt;p&gt;Once the user is authenticated, Jupyterhub connects via &lt;code&gt;SSH&lt;/code&gt; to a login node on Gordon and submits a batch serial job using &lt;code&gt;qsub&lt;/code&gt;. The web interface waits for the job to start running. A dedicated queue with a quick turnaround would be useful for this kind of jobs.&lt;/p&gt;
&lt;p&gt;&lt;img alt="jupyterhub-hpc-refresh.png" src="/images/jupyterhub-hpc-refresh.png" /&gt;
&lt;img alt="jupyterhub-hpc-job.png" src="/images/jupyterhub-hpc-job.png" /&gt;&lt;/p&gt;
&lt;p&gt;When the job starts running, it first sets up &lt;code&gt;SSH&lt;/code&gt; tunneling between the Jupyterhub host and the computing node, then starts the Jupyter Notebook.
As soon as the web interface detects that the job is running, proxies the tunneled HTTP port for the user. From this point the Jupyter Notebook works exactly like it would on a local machine.&lt;/p&gt;
&lt;p&gt;See an example Notebook printing the hostname of the computing node:&lt;/p&gt;
&lt;p&gt;&lt;img alt="jupyterhub-hpc-testnotebook.png" src="/images/jupyterhub-hpc-testnotebook.png" /&gt;&lt;/p&gt;
&lt;p&gt;Other two useful features of the Jupyter Notebook are a terminal:&lt;/p&gt;
&lt;p&gt;&lt;img alt="jupyterhub-hpc-terminal.png" src="/images/jupyterhub-hpc-terminal.png" /&gt;&lt;/p&gt;
&lt;p&gt;and an editor that run in the browser:&lt;/p&gt;
&lt;p&gt;&lt;img alt="jupyterhub-hpc-editor.png" src="/images/jupyterhub-hpc-editor.png" /&gt;&lt;/p&gt;
&lt;h2&gt;Launch Jupyterhub parallel to access hundreds of computing engines&lt;/h2&gt;
&lt;p&gt;The Notebook also supports using Torque to run Python computing engines and send them computationally intensive serial functions for load-balanced execution.&lt;/p&gt;
&lt;p&gt;In the Notebook interface, in the &lt;code&gt;Clusters&lt;/code&gt; tab, is it possible to choose the number of engines and click start to submit a job to the queue system:&lt;/p&gt;
&lt;p&gt;&lt;img alt="jupyterhub-hpc-clusterlaunch.png" src="/images/jupyterhub-hpc-clusterlaunch.png" /&gt;&lt;/p&gt;
&lt;p&gt;This will pack 16 jobs per node (Gordon has 16-cores CPUs) and make them available from the notebook, see an example usage where I process 1000 files with 128 engines running on a different job on Gordon:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://nbviewer.ipython.org/gist/zonca/9bd94d8782af037704ff"&gt;Example of Jupyterhub Parallel&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Andrea Zonca</dc:creator><pubDate>Thu, 02 Apr 2015 09:00:00 -0700</pubDate><guid isPermaLink="false">tag:zonca.github.io,2015-04-02:2015/04/jupyterhub-hpc.html</guid><category>python</category><category>ipython</category><category>jupyterhub</category><category>hpc</category></item><item><title>Accelerate groupby operation on pixels with Numba</title><link>http://zonca.github.io/2015/03/numba-groupby-pixels.html</link><description>&lt;p&gt;&lt;a href="/notebooks/numba_groupby_pixels.ipynb"&gt;Download the original IPython notebook&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Astrophysics background&lt;/h2&gt;
&lt;p&gt;It is very common in Astrophysics to work with sky pixels. The sky is tassellated in patches with specific properties and a sky map is then a collection of intensity values for each pixel. The most common pixelization used in Cosmology is &lt;a href="http://healpix.jpl.nasa.gov"&gt;HEALPix&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Measurements from telescopes are then represented as an array of pixels that encode the pointing of the instrument at each timestamp and the measurement output.&lt;/p&gt;
&lt;h2&gt;Sample timeline&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numba&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;For simplicity let's assume we have a sky with 50K pixels:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;NPIX = 50000
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;And we have 50 million measurement from our instrument:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;NTIME = int(50 * 1e6)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The pointing of our instrument is an array of pixels, random in our sample case:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;pixels = np.random.randint(0, NPIX-1, NTIME)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Our data are also random:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;timeline = np.random.randn(NTIME)
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Create a map of the sky with pandas&lt;/h2&gt;
&lt;p&gt;One of the most common operations is to sum all of our measurements in a sky map, so the value of each pixel in our sky map will be the sum of each individual measurement.
The easiest way is to use the &lt;code&gt;groupby&lt;/code&gt; operation in &lt;code&gt;pandas&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;timeline_pandas&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Series&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;timeline&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;pixels&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;timeline_pandas&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;head&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="mi"&gt;46889&lt;/span&gt;    &lt;span class="mf"&gt;0.407097&lt;/span&gt;
&lt;span class="mi"&gt;3638&lt;/span&gt;     &lt;span class="mf"&gt;1.300001&lt;/span&gt;
&lt;span class="mi"&gt;6345&lt;/span&gt;     &lt;span class="mf"&gt;0.174931&lt;/span&gt;
&lt;span class="mi"&gt;15742&lt;/span&gt;   &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.255958&lt;/span&gt;
&lt;span class="mi"&gt;34308&lt;/span&gt;    &lt;span class="mf"&gt;1.147338&lt;/span&gt;
&lt;span class="nl"&gt;dtype&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;float64&lt;/span&gt;

&lt;span class="nf"&gt;%time&lt;/span&gt; &lt;span class="n"&gt;m&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;timeline_pandas&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;groupby&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;level&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;).&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="n"&gt;CPU&lt;/span&gt; &lt;span class="nl"&gt;times&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;user&lt;/span&gt; &lt;span class="mf"&gt;4.09&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nl"&gt;sys&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;471&lt;/span&gt; &lt;span class="n"&gt;ms&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nl"&gt;total&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;4.56&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt;
&lt;span class="n"&gt;Wall&lt;/span&gt; &lt;span class="nl"&gt;time&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;4.55&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Create a map of the sky with numba&lt;/h2&gt;
&lt;p&gt;We would like to improve the performance of this operation using &lt;code&gt;numba&lt;/code&gt;, which allows to produce automatically C-speed compiled code from pure python functions.&lt;/p&gt;
&lt;p&gt;First we need to develop a pure python version of the code, test it, and then have &lt;code&gt;numba&lt;/code&gt; optimize it:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;def&lt;/span&gt; &lt;span class="n"&gt;groupby_python&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;output&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="n"&gt;in&lt;/span&gt; &lt;span class="n"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;output&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="n"&gt;m_python&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros_like&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;


&lt;span class="nf"&gt;%time&lt;/span&gt; &lt;span class="n"&gt;groupby_python&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pixels&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;timeline&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;m_python&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;CPU&lt;/span&gt; &lt;span class="nl"&gt;times&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;user&lt;/span&gt; &lt;span class="mf"&gt;37.5&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nl"&gt;sys&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="n"&gt;ns&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nl"&gt;total&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;37.5&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt;
&lt;span class="n"&gt;Wall&lt;/span&gt; &lt;span class="nl"&gt;time&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;37.6&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt;

&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;testing&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;assert_allclose&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;m_python&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Pure Python is slower than the &lt;code&gt;pandas&lt;/code&gt; version implemented in &lt;code&gt;cython&lt;/code&gt;.&lt;/p&gt;
&lt;h3&gt;Optimize the function with numba.jit&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;numba.jit&lt;/code&gt; gets an input function and creates an compiled version with does not depend on slow Python calls, this is enforced by &lt;code&gt;nopython=True&lt;/code&gt;, &lt;code&gt;numba&lt;/code&gt; would throw an error if it would not be possible to run in &lt;code&gt;nopython&lt;/code&gt; mode.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;groupby_numba&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;numba&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;jit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;groupby_python&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;nopython&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;m_numba&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros_like&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="nf"&gt;%time&lt;/span&gt; &lt;span class="n"&gt;groupby_numba&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pixels&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;timeline&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;m_numba&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;CPU&lt;/span&gt; &lt;span class="nl"&gt;times&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;user&lt;/span&gt; &lt;span class="mi"&gt;274&lt;/span&gt; &lt;span class="n"&gt;ms&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nl"&gt;sys&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt; &lt;span class="n"&gt;ms&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nl"&gt;total&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;279&lt;/span&gt; &lt;span class="n"&gt;ms&lt;/span&gt;
&lt;span class="n"&gt;Wall&lt;/span&gt; &lt;span class="nl"&gt;time&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;278&lt;/span&gt; &lt;span class="n"&gt;ms&lt;/span&gt;

&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;testing&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;assert_allclose&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;m_numba&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Performance improvement is about 100x compared to Python and 20x compared to Pandas, pretty good!&lt;/p&gt;
&lt;h2&gt;Use numba.jit as a decorator&lt;/h2&gt;
&lt;p&gt;The exact same result is obtained if we use &lt;code&gt;numba.jit&lt;/code&gt; as a decorator:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;@numba.jit(nopython=True)
def groupby_numba(index, value, output):
    for i in range(index.shape[0]):
        output[index[i]] += value[i]
&lt;/pre&gt;&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Andrea Zonca</dc:creator><pubDate>Tue, 24 Mar 2015 09:00:00 -0700</pubDate><guid isPermaLink="false">tag:zonca.github.io,2015-03-24:2015/03/numba-groupby-pixels.html</guid><category>python</category><category>numba</category><category>astrophysics</category></item><item><title>Software Carpentry setup for Chromebook</title><link>http://zonca.github.io/2015/02/software-carpentry-setup-chromebook.html</link><description>&lt;p&gt;In this post I'll provide instructions on how to install the main requirements of a &lt;a href="http://software-carpentry.org"&gt;Software Carpentry workshop&lt;/a&gt; on
a Chromebook.
Bash, git, IPython notebook and R.&lt;/p&gt;
&lt;h2&gt;Switch the Chromebook to Developer mode&lt;/h2&gt;
&lt;p&gt;ChromeOS is very restrictive on what users can install on the machine.
The only way to get around this is to switch to developer mode.&lt;/p&gt;
&lt;p&gt;Switching to Developer mode &lt;strong&gt;wipes&lt;/strong&gt; all the data on the local disk and 
may void warranty, do it at your own risk.&lt;/p&gt;
&lt;p&gt;Instructions are available on the &lt;a href="http://www.chromium.org/chromium-os/developer-information-for-chrome-os-devices"&gt;ChromeOS wiki&lt;/a&gt;, you need
to click on your device name and follow instructions.
For most devices you need to switch the device off, then hold down &lt;code&gt;ESC&lt;/code&gt; and &lt;code&gt;Refresh&lt;/code&gt; and poke the &lt;code&gt;Power&lt;/code&gt; button, then press &lt;code&gt;Ctrl-D&lt;/code&gt; at the
Recovery screen (there is no prompt, you have to know to do it).
This will wipe the device and activate Developer mode.&lt;/p&gt;
&lt;p&gt;Once you reboot and enter your Google credentials, the Chromebook will copy back from Google servers all of your settings.&lt;/p&gt;
&lt;p&gt;Now you are in Developer mode, the main feature is that you have a &lt;code&gt;root&lt;/code&gt; (superuser) shell you can activate using &lt;code&gt;Ctrl-Alt-T&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The worst issue of Developer mode is that at each boot the system will display a scary screen warning that OS verification is off and asks you if you would like to leave Developer mode. If you either press &lt;code&gt;Ctrl-D&lt;/code&gt; or wait 30 seconds, it will boot ChromeOS in Developer mode, if you instead hit the Space, it will wipe
everything and switch back to Normal mode.&lt;/p&gt;
&lt;h2&gt;Install Ubuntu with crouton&lt;/h2&gt;
&lt;p&gt;You can now install Ubuntu using &lt;a href="https://github.com/dnschneid/crouton"&gt;crouton&lt;/a&gt;, you can read the instructions on the page, in summary:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;First you need to install the &lt;a href="https://goo.gl/OVQOEt"&gt;Crouton Chrome extension&lt;/a&gt; on ChromeOS&lt;/li&gt;
&lt;li&gt;Download the last release from &lt;a href="https://goo.gl/fd3zc"&gt;https://goo.gl/fd3zc&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Open the ChromeOS shell using &lt;code&gt;Ctrl-Alt-t&lt;/code&gt;, digit &lt;code&gt;shell&lt;/code&gt; at the prompt and hit enter&lt;/li&gt;
&lt;li&gt;Run &lt;code&gt;sudo sh ~/Downloads/crouton -t xfce,xiwi -r trusty&lt;/code&gt;, this instlls Ubuntu Trutyty with xfce desktop and uses &lt;code&gt;kiwi&lt;/code&gt; to be able to run in a window.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Now you can have Ubuntu running in a window of the Chromebook browser by:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Press &lt;code&gt;Ctrl-Alt-T&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;digit &lt;code&gt;shell&lt;/code&gt; at the prompt and hit enter&lt;/li&gt;
&lt;li&gt;digit &lt;code&gt;sudo startxfce4&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;What is great about &lt;code&gt;crouton&lt;/code&gt; is that it is not like a Virtual Machine, Ubuntu runs at full performance on the same linux kernel of ChromeOS.&lt;/p&gt;
&lt;h2&gt;Install scientific computing stack&lt;/h2&gt;
&lt;p&gt;You can now follow the instructions for 
Linux at &lt;a href="http://software-carpentry.org/v5/setup.html"&gt;http://software-carpentry.org/v5/setup.html&lt;/a&gt;, summary of commands to run in a terminal:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;sudo apt install nano&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;sudo apt install git&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;In order to install R &lt;code&gt;sudo apt install r-base&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Download Anaconda Python 3 64bit for Linux from &lt;a href="http://continuum.io/downloads"&gt;http://continuum.io/downloads&lt;/a&gt; and execute it&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Anaconda will run under Ubuntu but when you open an IPython notebook, it will automatically open a new tab in the main browser of ChromeOS, not
inside the Ubuntu window.&lt;/p&gt;
&lt;h2&gt;Final note&lt;/h2&gt;
&lt;p&gt;I admit it looks scary, I personally followed this procedure successfully on 2 chromebooks: Samsung Chromebook 1 and Toshiba Chromebook 2.&lt;/p&gt;
&lt;p&gt;See a screenshot on my Chromebook with the Ubuntu window on the right with &lt;code&gt;git&lt;/code&gt;, &lt;code&gt;nano&lt;/code&gt; and &lt;code&gt;IPython notebook&lt;/code&gt; running, the &lt;code&gt;IPython notebook&lt;/code&gt; window opens in Chrome, see the left window (click to enlarge).&lt;/p&gt;
&lt;p&gt;&lt;a href="/images/screenshot-chromebook.png"&gt;&lt;img src="/images/screenshot-chromebook.png" alt="Screenshot Chromebook click for full resolution" style="width: 730px;"/&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;It is also possible to switch the Chromebook to Developer mode and install Anaconda and git directly there, however I think that in order to have
a complete platform for scientific computing is a lot better to have all of the packages provided by Ubuntu.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Andrea Zonca</dc:creator><pubDate>Tue, 10 Feb 2015 20:00:00 -0800</pubDate><guid isPermaLink="false">tag:zonca.github.io,2015-02-10:2015/02/software-carpentry-setup-chromebook.html</guid><category>software-carpentry</category><category>chromebook</category><category>ipython</category></item><item><title>Zero based indexing</title><link>http://zonca.github.io/2014/10/zero-based-indexing.html</link><description>&lt;h2&gt;Reads&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Dijkstra: &lt;a href="https://www.cs.utexas.edu/~EWD/transcriptions/EWD08xx/EWD831.html"&gt;https://www.cs.utexas.edu/~EWD/transcriptions/EWD08xx/EWD831.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Guido van Rossum: &lt;a href="https://plus.google.com/115212051037621986145/posts/YTUxbXYZyfi"&gt;https://plus.google.com/115212051037621986145/posts/YTUxbXYZyfi&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Comment&lt;/h2&gt;
&lt;p&gt;For Europeans zero based indexing feels reasonable if we think of floors in a house,
the lowest floor is ground floor, then 1st floor and so on.&lt;/p&gt;
&lt;p&gt;A house with 2 stories has ground and 1st floor. It is natural in this way to index
zero-based and to count 1-based.&lt;/p&gt;
&lt;p&gt;What about &lt;strong&gt;slicing&lt;/strong&gt; instead? This is a separate issue from indexing.
The main problem here is that if you include the upper bound then you cannot express
the empty slice.
Also it is elegant to print the first &lt;code&gt;n&lt;/code&gt; elements as &lt;code&gt;a[:n]&lt;/code&gt;. Slicing &lt;code&gt;a[i:j]&lt;/code&gt; excludes
the upper bound, so it probably easier to understand if we express it as &lt;code&gt;a[i:i+n]&lt;/code&gt;.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Andrea Zonca</dc:creator><pubDate>Wed, 22 Oct 2014 10:00:00 -0700</pubDate><guid isPermaLink="false">tag:zonca.github.io,2014-10-22:2014/10/zero-based-indexing.html</guid><category>python</category></item><item><title>Write unit tests as cells of IPython notebooks</title><link>http://zonca.github.io/2014/09/unit-tests-ipython-notebook.html</link><description>&lt;h2&gt;What?&lt;/h2&gt;
&lt;p&gt;Plugin for &lt;code&gt;py.test&lt;/code&gt; to write unit tests as cells in IPython notebooks:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Homepage on Github: &lt;a href="https://github.com/zonca/pytest-ipynb"&gt;https://github.com/zonca/pytest-ipynb&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;PyPi : &lt;a href="https://pypi.python.org/pypi/pytest-ipynb/"&gt;https://pypi.python.org/pypi/pytest-ipynb/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Install with &lt;code&gt;pip install pytest-ipynb&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Why?&lt;/h2&gt;
&lt;p&gt;Many unit testing fromeworks in Python, first of all the &lt;code&gt;unittest&lt;/code&gt; package in the standard library, work very well for automating unit tests, but make it very difficult to debug interactively any failed test.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://pytest.org"&gt;&lt;code&gt;py.test&lt;/code&gt;&lt;/a&gt; alleviates this problem by allowing to write just plain Python functions with &lt;code&gt;assert&lt;/code&gt; statements (no boilerplate code), discover them automatically in any file that starts with &lt;code&gt;test&lt;/code&gt; and write a useful report.&lt;/p&gt;
&lt;p&gt;I wrote a plugin for &lt;code&gt;py.test&lt;/code&gt;, &lt;a href="https://pypi.python.org/pypi/pytest-ipynb"&gt;&lt;code&gt;pytest-ipynb&lt;/code&gt;&lt;/a&gt;, that goes a step further and runs unit tests written as cells of any IPython notebook named &lt;code&gt;test*.ipynb&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The advantage is that it is easy to create and debug interactively any issue by opening the testing notebook interactively, then clean the notebook outputs and add it to the software repository.&lt;/p&gt;
&lt;p&gt;More details on Github: &lt;a href="https://github.com/zonca/pytest-ipynb"&gt;https://github.com/zonca/pytest-ipynb&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Suggestions welcome as comments or github issues.&lt;/p&gt;
&lt;p&gt;(Yes, works with Python 3)&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Andrea Zonca</dc:creator><pubDate>Tue, 30 Sep 2014 14:00:00 -0700</pubDate><guid isPermaLink="false">tag:zonca.github.io,2014-09-30:2014/09/unit-tests-ipython-notebook.html</guid><category>unit-test</category><category>ipython</category><category>ipython-notebook</category></item><item><title>How to perform code review for scientific software</title><link>http://zonca.github.io/2014/08/code-review-for-scientific-computing.html</link><description>&lt;p&gt;Code review is the formal process where a programmer inspects in detail a piece of software developed by somebody else in order to improve code quality by catching bugs, improve readibility and usability.
It is used extensively in industry, not much in academia.&lt;/p&gt;
&lt;p&gt;There has been some discussion about this lately, see:
&lt;em&gt; &lt;a href="http://ivory.idyll.org/blog/on-code-review-of-scientific-code.html"&gt;A few thoughts on code review of scientific code&lt;/a&gt; by Titus Brown
&lt;/em&gt; &lt;a href="http://mozillascience.org/code-review-for-science-what-we-learned/"&gt;Code review for science: What we learned&lt;/a&gt; by Kaitlin Thaney&lt;/p&gt;
&lt;p&gt;I participated in the &lt;a href="http://software-carpentry.org/blog/2014/01/code-review-round-2.html"&gt;second code review pilot study of Software Carpentry&lt;/a&gt; where I was paired to a research group in Genomics and I reviewed some of their analysis code.
In this blog post I'd like to write about some guidelines and best practices on how to perform code review of scientific code.&lt;/p&gt;
&lt;p&gt;Best use of code review is on libraries, prior to publication, because an improvement in code quality can help future users of the code. One-off analysis scripts benefit less from the process.&lt;/p&gt;
&lt;h2&gt;How to do a code review of a large codebase&lt;/h2&gt;
&lt;p&gt;The code review process should be performed on ~200-400 lines of code at a time.
First thing is to ask the code author if she can identify different functionalities of the code that could be packaged and distributed separately. Modularity really helps maintaining software in the long term.&lt;/p&gt;
&lt;p&gt;Then the author should follow these steps to get ready for the code review:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;For each of the packages identified previously, the code author should create a separate repository, generally on Github, possibly under an organization account (see &lt;a href="http://zonca.github.io/2014/08/github-for-research-groups.html"&gt;Github for research groups&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;Create a blank project in the programming language of choice (hopefully Python!) using a pre-defined standard template, I recommend using &lt;a href="https://github.com/audreyr/cookiecutter"&gt;CookieCutter&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Write a &lt;code&gt;README.md&lt;/code&gt; file explaining exactly the functionality of the code in general&lt;/li&gt;
&lt;li&gt;Clone the repository locally, add, commit and push the blank project with &lt;code&gt;README.md&lt;/code&gt; to the &lt;code&gt;master&lt;/code&gt; branch on Github&lt;/li&gt;
&lt;li&gt;Identify a portion of the software of about ~200-400 lines that has a defined functionality and that could be reviewed together. It doesn't necessarily need to be in a runnable state, at the beginning we can start the code review without running the code.&lt;/li&gt;
&lt;li&gt;Create a new branch locally and copy, add, commit this file or this set of files to the repository and push to Github&lt;/li&gt;
&lt;li&gt;Access the web interface of Github, it should have detected that you just pushed a new branch and asked if you want to create a pull request. Create a pull request with a few details on the code under review.&lt;/li&gt;
&lt;li&gt;Point the reviewer to the pull request&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;How to review an improvement to the software&lt;/h2&gt;
&lt;p&gt;The implementation of a feature should be performed on a separate branch, then it is straightforward to push it to Github, create a pull request and ask reviewers to look at the set of changes.&lt;/p&gt;
&lt;h2&gt;How to perform the actual code review&lt;/h2&gt;
&lt;p&gt;Coding style should not be the main focus of the review, the most important feedback for the author are high-level comments on software organization. The reviewer should focus on what makes the software more usable and more maintenable.&lt;/p&gt;
&lt;p&gt;A few examples:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;can some parts of the code be simplified?&lt;/li&gt;
&lt;li&gt;is there any functionality that could be replaced by an existing library?&lt;/li&gt;
&lt;li&gt;is it clear what each part of the software is doing?&lt;/li&gt;
&lt;li&gt;is there a more straightforward way of splitting the code into files?&lt;/li&gt;
&lt;li&gt;is documentation enough?&lt;/li&gt;
&lt;li&gt;are there some function arguments or function names that could be easily misinterpreted by a user?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The purpose is to improve the code, but also to help the code author to improve her coding skills.&lt;/p&gt;
&lt;p&gt;On the Github pull requests interface, it is possible both to write general comments, and to click on a single line of code and write an inline comment.&lt;/p&gt;
&lt;h2&gt;How to implement reviewer's recommendations&lt;/h2&gt;
&lt;p&gt;The author can improve the code locally on the same branch used in the pull request, then commit and push the changes to Github, the changes will be automatically added to the existing pull request, so the reviewer can start another iteration of the review process.&lt;/p&gt;
&lt;p&gt;Comments and suggestions are welcome.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Andrea Zonca</dc:creator><pubDate>Thu, 28 Aug 2014 17:00:00 -0700</pubDate><guid isPermaLink="false">tag:zonca.github.io,2014-08-28:2014/08/code-review-for-scientific-computing.html</guid><category>github</category><category>git</category><category>openscience</category><category>software-carpentry</category></item><item><title>Create a Github account for your research group with free private repositories</title><link>http://zonca.github.io/2014/08/github-for-research-groups.html</link><description>&lt;p&gt;&lt;a href="https://github.com/"&gt;Github&lt;/a&gt; allows a research group to create their own webpage where they can host, share and develop their software using the &lt;code&gt;git&lt;/code&gt; version control system and the powerful Github online issue-tracking interface.&lt;/p&gt;
&lt;p&gt;Since February 2014 Github also offers 20 private repositories to research groups and classrooms, plus unlimited public repositories.
Private repositories are useful for early stages of development or if it is necessary to keep software secret before publication, at publication they can easily switched to public repositories and free up their slot.&lt;/p&gt;
&lt;p&gt;Here the steps to set this up:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Create a user account on Github and choose the free plan, use your &lt;code&gt;.edu&lt;/code&gt; email address&lt;/li&gt;
&lt;li&gt;Create an organization account for your research group&lt;/li&gt;
&lt;li&gt;Go to https://education.github.com/ and click on "Request a discount"&lt;/li&gt;
&lt;li&gt;Choose what is your position, e.g. Researcher and select you want a discount for an organization&lt;/li&gt;
&lt;li&gt;Choose the organization you created earlier and confirm that it is a "Research group"&lt;/li&gt;
&lt;li&gt;Add details about your Research group&lt;/li&gt;
&lt;li&gt;Finally you need to upload a picture of your University ID card and write how you plan on using the repositories&lt;/li&gt;
&lt;li&gt;Within a week at most, but generally in less than 24 hours, you will be approved for 20 private repositories.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Once the organization is created, you can add key team members to the "Owners" group, and then create another group for students and collaborators.&lt;/p&gt;
&lt;p&gt;Consider also that is not necessary for every collaborator to have write access to your repositories. My recommendation is to ask a more experienced team member to administer the central repository, ask the students to fork the repository under their user accounts (forks of private repositories are always private, free and don't use any slot), and then &lt;a href="https://help.github.com/articles/using-pull-requests"&gt;send a pull request&lt;/a&gt; to the central repository for the administrator to review, discuss and merge.&lt;/p&gt;
&lt;p&gt;See for example the organization account of the &lt;a href="https://github.com/ged-lab"&gt;"Genomics, Evolution, and Development" at Michigan State U led by Dr. C. Titus Brown&lt;/a&gt; where they share code, documentation and papers. Open Science!!&lt;/p&gt;
&lt;p&gt;Other suggestions on the setup very welcome!&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Andrea Zonca</dc:creator><pubDate>Tue, 19 Aug 2014 15:00:00 -0700</pubDate><guid isPermaLink="false">tag:zonca.github.io,2014-08-19:2014/08/github-for-research-groups.html</guid><category>github</category><category>git</category><category>openscience</category></item><item><title>Thoughts on a career as a computational scientist</title><link>http://zonca.github.io/2014/06/career-as-a-computational-scientist.html</link><description>&lt;p&gt;Recently I've been asked what are the prospects of a wannabe computational scientist, 
both in terms of training and in terms of job opportunities.&lt;/p&gt;
&lt;p&gt;So I am writing this blog post about my personal experience.&lt;/p&gt;
&lt;h2&gt;What is a computational scientist?&lt;/h2&gt;
&lt;p&gt;In my understanding, a computational scientist is a scientist with strong skills in scientific computing who
most of the day is building software.&lt;/p&gt;
&lt;p&gt;Usually there are 2 main areas, in any field of science:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;em&gt;Data analysis&lt;/em&gt;: historically only few fields of science had to deal with large amount
    of experimental data, e.g. Astrophysics, nowadays instead every field can generate 
    extremely large amounts of data thanks to modern technology.
    The task of the computational scientist is generally to analyze the data, i.e. cleanup, check systematic effects,
    calibrate, understand and reduce to a form to be used for scientific exploitation.
    Generally a second phase of data analysis involves model fitting, i.e. check which theoretical models best fit the
    data and estimate their parameters with error bars, this requires knowledge of Statistics and Bayesian techniques,
    like Markov Chain Monte Carlo (MCMC).&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Simulations&lt;/em&gt;: production of artificial data used for their own good in the understanding of scientific models or
    by trying to reproduce experimental data in order to characterize the response of a scientific instrument. &lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Skills of a computational scientist&lt;/h2&gt;
&lt;p&gt;Starting out as a computational scientist nowadays is quite easy; with a background in any field of science, it is possible to improve computational skills thanks to several learning resources, for example:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Free online video classes on &lt;a href="https://www.coursera.org/courses?search=python"&gt;Coursera&lt;/a&gt;, &lt;a href="https://www.udacity.com/courses#!/data-science"&gt;Udacity&lt;/a&gt; and others&lt;/li&gt;
&lt;li&gt;&lt;a href="http://software-carpentry.org"&gt;Software Carpentry&lt;/a&gt; runs bootcamps for scientists to improve their computational skills&lt;/li&gt;
&lt;li&gt;Online tutorials on &lt;a href="http://scipy-lectures.github.io/"&gt;Python for scientific computing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Books, e.g. &lt;a href="http://shop.oreilly.com/product/0636920023784.do"&gt;Python for Data Analysis&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Basically it is important to have a good experience with at least one programming language, Python is the safest option because:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;it is well enstabilished in many fields of science&lt;/li&gt;
&lt;li&gt;its syntax is easier to learn than most other common programming languages&lt;/li&gt;
&lt;li&gt;it has the largest number of scientific libraries &lt;/li&gt;
&lt;li&gt;it is easy to interface with other languages, i.e. we can reuse legacy code implemented in C/C++/FORTRAN&lt;/li&gt;
&lt;li&gt;it can be used also when developing something unusual for a computational scientist, like web development (&lt;code&gt;django&lt;/code&gt;) or interfacing with hardware (&lt;code&gt;pyserial&lt;/code&gt;).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Python performance is comparable to C/C++/Java when we make use of optimized libraries like &lt;code&gt;numpy&lt;/code&gt;, &lt;code&gt;pandas&lt;/code&gt;, &lt;code&gt;scipy&lt;/code&gt;, which
have Python frontends to highly optimized C or Fortran code; therefore is necessary to avoid explicit for loops and learn
to write "vectorized" code, that allows entire arrays and matrices to be processed in one step.&lt;/p&gt;
&lt;p&gt;Some important Python tools to learn are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;IPython&lt;/code&gt; notebooks to write documents with code, documentatin and plots embedded &lt;/li&gt;
&lt;li&gt;&lt;code&gt;numpy&lt;/code&gt; and &lt;code&gt;pandas&lt;/code&gt; for data management&lt;/li&gt;
&lt;li&gt;&lt;code&gt;matplotlib&lt;/code&gt; for plotting&lt;/li&gt;
&lt;li&gt;&lt;code&gt;h5py&lt;/code&gt; or &lt;code&gt;pytables&lt;/code&gt;, HDF5 binary files manipulation&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.jeffknupp.com/blog/2013/08/16/open-sourcing-a-python-project-the-right-way/"&gt;how to publish a Python package&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;emcee&lt;/code&gt; for MCMC&lt;/li&gt;
&lt;li&gt;&lt;code&gt;scipy&lt;/code&gt; for signal processing, FFT, optimization, integration, 2d array processing&lt;/li&gt;
&lt;li&gt;&lt;code&gt;scikit-learn&lt;/code&gt; for Machine Learning&lt;/li&gt;
&lt;li&gt;&lt;code&gt;scikit-image&lt;/code&gt; for image processing &lt;/li&gt;
&lt;li&gt;Object oriented programming&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For parallel programming:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;IPython parallel&lt;/code&gt; for distributing large amount of serial and independent job on a cluster&lt;/li&gt;
&lt;li&gt;&lt;code&gt;PyTrilinos&lt;/code&gt; for distributed linear algebra (high level operations with data distributed across nodes, automatic MPI communication)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;mpi4py&lt;/code&gt; for manually create communication of data via MPI&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;On top of Python is also useful to learn a bit about shell scripting with &lt;code&gt;bash&lt;/code&gt;, which for simple automation tasks is better suited,
and it is fundamental to learn version control with git or mercurial.&lt;/p&gt;
&lt;h2&gt;My experience&lt;/h2&gt;
&lt;p&gt;I trained as Aerospace Engineer for my Master degree, and then moved to a PhD in Astrophysics, in Milano,
where I worked in the Planck collaboration and took care of simulating the inband response of the Low Frequency Instrument
detectors.
During my PhD I developed a good proficiency with Python, mainly using it for task automation and plotting. 
My previous programming experience was very low, only some Matlab during last year of my Master degree, but I found Python really easy to use,
and learned it myself with books and online tutorials.
With no formal education in Computer Science, the most complicated concept to grasp is Object Oriented programming; at the time
I was moonlighting as a web developer and I familiarized with OO using Django models.
After my PhD I got a PostDoc position at the University of California, Santa Barbara, there I had for the first time
access to supercomputers and my job involved analyzing large amount of data.
During 4 years at UCSB I had the great opportunity of choosing my own tools, implementing my own software for data processing,
so I immediately saw the value of improving my understanding of software development best practices.&lt;/p&gt;
&lt;p&gt;Unfortunately in science there is usually a push toward hacking around a quick and dirty solution to get out results and go forward,
I instead focused on learning how to build easily-maintenable libraries that I could re-use in the future. This
involved learning more advanced Python, version control, unit testing and so on. I learned these tools by reading tutorials and 
documentation on the web, answers on StackOverflow, blog posts.
It also helped that I became one of the core developers of &lt;code&gt;healpy&lt;/code&gt;, a Python package for pixelized sky maps processing.&lt;/p&gt;
&lt;p&gt;In 2013, at the 4th year of my PostDoc and with the Planck mission near to the end in 2015, I was looking for a position
as a computational scientist, mainly as a research scientist (i.e. doing research/data analysis full time, with a long term contract) 
at research labs like Berkeley Lab or Jet Propulsion Laboratory, or in a research group in Cosmology/Astrophysics or in
High Performance Computing.&lt;/p&gt;
&lt;p&gt;I got hired at the San Diego Supercomputer Center in December 2013 as a permanent staff, mainly thanks to my experience with data analysis,
Python and parallel programming, here I collaborate with research groups in any field of Science and help them deploy and optimize their software on supercomputers here at SDSC or in other XSEDE centers.&lt;/p&gt;
&lt;h2&gt;Thoughts about a career as a computational scientist&lt;/h2&gt;
&lt;p&gt;After a PhD program, a computational scientist with experience either in data analysis or simulation, especially if has experience in parallel programming, should quite easily find a position as a PostDoc, lots of research groups have huge amount of data and need software development skilled labor.&lt;/p&gt;
&lt;p&gt;I believe what is complicated is the next step, faculty jobs favour scientists with the best scientific publications, and software development generally is not recognized as a first class scientific product.
Very interesting opportunities in Academia are Research Scientist positions either at research facilities, for example Lawrence Berkeley Labs and NASA Jet Propulsion Laboratory, or supercomputer centers. These jobs are often permament positions, unless the institution runs out of funding, and allow to work 100% on research.
Another opportunity is to work as Research Scientist in a specific research group in a University, this is less common, and depends on their availability of long-term funding.&lt;/p&gt;
&lt;p&gt;Still, the total number of available positions in Academia is not very high, therefore it is very important to also keep open the opportunity of a job in Industry. Fortunately nowadays most  skills of a computational scientist are very well recognized in Industry, so I recommend to choose, whenever possible, to learn and use tools that are widely used also outside of Academia, for example Python, version control with Git, shell scripting, unit testing, databases, multi-core programming, parallel programming, GPU programming and so on.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Acknowledgement&lt;/em&gt;: thanks to Priscilla Kelly for discussion on this topic and review of the post&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Comments/feedback&lt;/em&gt;: comment on the blog using Google+ or tweet to &lt;a href="http://twitter.com/andreazonca"&gt;@andreazonca&lt;/a&gt;&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Andrea Zonca</dc:creator><pubDate>Thu, 05 Jun 2014 14:00:00 -0700</pubDate><guid isPermaLink="false">tag:zonca.github.io,2014-06-05:2014/06/career-as-a-computational-scientist.html</guid><category>career</category><category>hpc</category></item><item><title>Machine learning at scale with Python</title><link>http://zonca.github.io/2014/03/machine-learning-at-scale-with-python.html</link><description>&lt;p&gt;My talk for the San Diego Data Science meetup: &lt;a href="http://www.meetup.com/San-Diego-Data-Science-R-Users-Group/events/170967362/"&gt;http://www.meetup.com/San-Diego-Data-Science-R-Users-Group/events/170967362/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;About:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Setup &lt;a href="http://star.mit.edu/cluster/"&gt;StarCluster&lt;/a&gt; to launch EC2 instances&lt;/li&gt;
&lt;li&gt;Running IPython Notebook on Amazon EC2&lt;/li&gt;
&lt;li&gt;Running single node Machine Learning jobs using multiple cores&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Distributing jobs with IPython parallel to multiple EC2 instances&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;See HTML5 &lt;strong&gt;slides&lt;/strong&gt;: &lt;a href="http://bit.ly/ml-ec2"&gt;http://bit.ly/ml-ec2&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;See the IPython notebook sources of the slides: &lt;a href="http://bit.ly/ml-ec2-ipynb"&gt;http://bit.ly/ml-ec2-ipynb&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Finally the Github repository with additional material, under MIT license:
&lt;a href="https://github.com/zonca/machine-learning-at-scale-with-python"&gt;https://github.com/zonca/machine-learning-at-scale-with-python&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Any feedback is appreciated, google+, twitter or email.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Andrea Zonca</dc:creator><pubDate>Thu, 20 Mar 2014 20:00:00 -0700</pubDate><guid isPermaLink="false">tag:zonca.github.io,2014-03-20:2014/03/machine-learning-at-scale-with-python.html</guid><category>python</category><category>machine-learning</category></item><item><title>Python on Gordon</title><link>http://zonca.github.io/2014/03/setup-ipython-notebook-parallel-Gordon.html</link><description>&lt;p&gt;Gordon has already a &lt;code&gt;python&lt;/code&gt; environment setup which can be activated by loading the &lt;code&gt;python&lt;/code&gt; module:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;module&lt;/span&gt; &lt;span class="n"&gt;load&lt;/span&gt; &lt;span class="n"&gt;python&lt;/span&gt; &lt;span class="c c-Singleline"&gt;# add this to .bashrc to load it at every login&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Install virtualenv&lt;/h3&gt;
&lt;p&gt;Then we need to setup a sandboxed local environment to install other packages, by using &lt;code&gt;virtualenv&lt;/code&gt;, get the link to the latest version from &lt;a href="https://pypi.python.org/pypi/virtualenv"&gt;https://pypi.python.org/pypi/virtualenv&lt;/a&gt;, then download it on gordon and unpack it, e.g.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;wget --no-check-certificate https://pypi.python.org/packages/source/v/virtualenv/virtualenv-1.11.2.tar.gz
tar xzvf virtualenv*tar.gz
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Then create your own virtualenv and load it:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;mkdir ~/venv
python virtualenv-*/virtualenv.py ~/venv/py
source ~/venv/py/bin/activate # add this to .bashrc to load it at every login
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;you can restore your previous environment by deactivating the virtualenv:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;deactivate # from your bash prompt
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Install IPython&lt;/h3&gt;
&lt;p&gt;Using &lt;code&gt;pip&lt;/code&gt; you can install &lt;code&gt;IPython&lt;/code&gt; and all dependencies for the notebook and parallel tools running:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;pip install ipython pyzmq tornado jinja
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Configure the IPython notebook&lt;/h3&gt;
&lt;p&gt;For interactive data exploration, you can run the &lt;code&gt;IPython&lt;/code&gt; notebook in a computing node on Gordon and export the web interface to your local machine, which also embeds all the plots.
Configuring the tunnelling over SSH is complicated, so I created a script, takes a little time to setup but then is very easy to use, see https://github.com/pyHPC/ipynbhpc.&lt;/p&gt;
&lt;h3&gt;Configure IPython parallel&lt;/h3&gt;
&lt;p&gt;&lt;a href="http://ipython.org/ipython-doc/stable/parallel/"&gt;IPython parallel&lt;/a&gt; on Gordon allows to launch a &lt;code&gt;PBS&lt;/code&gt; job with tens (or hundreds) of Python engines and then easily submit hundreds (or thousands) of serial jobs to be executed with automatic load balancing.
First of all create the default configuration files:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;ipython profile create --parallel
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Then, in &lt;code&gt;~/.ipython/profile_default/ipcluster_config.py&lt;/code&gt;, you need to setup:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;c.IPClusterStart.controller_launcher_class = &amp;#39;LocalControllerLauncher&amp;#39; 
c.IPClusterStart.engine_launcher_class = &amp;#39;PBS&amp;#39; 
c.PBSLauncher.batch_template_file = u&amp;#39;/home/REPLACEWITHYOURUSER/.ipython/profile_default/pbs.engine.template&amp;#39; # &amp;quot;~&amp;quot; does not work
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;You also need to allow connections to the controller from other hosts, setting  in &lt;code&gt;~/.ipython/profile_default/ipcontroller_config.py&lt;/code&gt;: &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;c.HubFactory.ip = &amp;#39;*&amp;#39;
c.HubFactory.engine_ip = &amp;#39;*&amp;#39;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Finally create the PBS template &lt;code&gt;~/.ipython/profile_default/pbs.engine.template&lt;/code&gt;:&lt;/p&gt;
&lt;table class="highlighttable"&gt;&lt;tr&gt;&lt;td class="linenos"&gt;&lt;div class="linenodiv"&gt;&lt;pre&gt; 1
 2
 3
 4
 5
 6
 7
 8
 9
10&lt;/pre&gt;&lt;/div&gt;&lt;/td&gt;&lt;td class="code"&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="ch"&gt;#!/bin/bash&lt;/span&gt;
&lt;span class="c1"&gt;#PBS -q normal&lt;/span&gt;
&lt;span class="c1"&gt;#PBS -N ipcluster&lt;/span&gt;
&lt;span class="c1"&gt;#PBS -l nodes={n/16}:ppn={n}:native&lt;/span&gt;
&lt;span class="c1"&gt;#PBS -l walltime=01:00:00&lt;/span&gt;
&lt;span class="c1"&gt;#PBS -o ipcluster.out&lt;/span&gt;
&lt;span class="c1"&gt;#PBS -e ipcluster.err&lt;/span&gt;
&lt;span class="c1"&gt;#PBS -m abe&lt;/span&gt;
&lt;span class="c1"&gt;#PBS -V&lt;/span&gt;
mpirun_rsh -np &lt;span class="o"&gt;{&lt;/span&gt;n&lt;span class="o"&gt;}&lt;/span&gt; -hostfile &lt;span class="nv"&gt;$PBS_NODEFILE&lt;/span&gt; ipengine
&lt;/pre&gt;&lt;/div&gt;
&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;

&lt;p&gt;Here we chose to run 16 IPython engines per Gordon node, so each has access to 4GB of ram, if you need more just change 16 to 8 for example.&lt;/p&gt;
&lt;h3&gt;Run IPython parallel&lt;/h3&gt;
&lt;p&gt;You can submit a job to the queue running, &lt;code&gt;n&lt;/code&gt; is equal to the number of processes you want to use, so it needs to be a multiple of the &lt;code&gt;ppn&lt;/code&gt; chosen in the PBS template:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;ipcluster start --n=32 &amp;amp;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;in this case we are requesting 2 nodes, with 16 IPython engines each, check with:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="x"&gt;qstat -u &lt;/span&gt;&lt;span class="p"&gt;$&lt;/span&gt;&lt;span class="nv"&gt;USER&lt;/span&gt;&lt;span class="x"&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;basically &lt;code&gt;ipcluster&lt;/code&gt; runs an &lt;code&gt;ipcontroller&lt;/code&gt; on the login node and submits a job to PBS for running the &lt;code&gt;ipengines&lt;/code&gt; on the computing nodes.&lt;/p&gt;
&lt;p&gt;Once the PBS job is running, check that the engines are connected by opening a IPython on the login node and print the &lt;code&gt;ids&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;In&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt; &lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;IPython.parallel&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Client&lt;/span&gt;
&lt;span class="n"&gt;In&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt; &lt;span class="n"&gt;rc&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Client&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;In&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt; &lt;span class="n"&gt;rc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ids&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;You can stop the cluster (kills &lt;code&gt;ipcontroller&lt;/code&gt; and runs &lt;code&gt;qdel&lt;/code&gt; on the PBS job) either by sending CTRL-c to &lt;code&gt;ipcluster&lt;/code&gt; or running:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;ipcluster stop # from bash console
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Submit jobs to IPython parallel&lt;/h3&gt;
&lt;p&gt;As soon as &lt;code&gt;ipcluster&lt;/code&gt; is executed, &lt;code&gt;ipcontroller&lt;/code&gt; is ready to queue jobs up, which will be then consumed by the engines once they will be running.
The easiest method to submit jobs with automatic load balancing is to create a load balanced view:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;In&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt; &lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;IPython.parallel&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Client&lt;/span&gt;
&lt;span class="n"&gt;In&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt; &lt;span class="n"&gt;rc&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Client&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;In&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt; &lt;span class="n"&gt;lview&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;rc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;load_balanced_view&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="c1"&gt;# default load-balanced view&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;and then use its &lt;code&gt;map&lt;/code&gt; method:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;def exp_10(x):
    return x**10

list_of_args = range(100)
result = lview.map(exp_10, list_of_args)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;In this code &lt;code&gt;IPython&lt;/code&gt; will distribute uniformly the list of arguments to the engines and the function will be evalutated for each of them and the result copied back to the connecting client running on the login node.&lt;/p&gt;
&lt;h3&gt;Submit non-python jobs to IPython parallel&lt;/h3&gt;
&lt;p&gt;Let's assume you have a list of commands you want to run in a text file, one command per line, those could be implemented in any programming language, e.g.:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;date &amp;amp;&amp;gt; date.log
hostname &amp;amp;&amp;gt; hostname.log
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Then you create a function that executes one of those commands:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;run_command&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;command&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;subprocess&lt;/span&gt;
    &lt;span class="n"&gt;subprocess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Popen&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;command&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shell&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Then apply this function to the list of commands:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;list_of_commands = open(&amp;quot;commands.txt&amp;quot;).readlines()
lview.map(run_command, list_of_commands)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;I created a script that automates this process, see https://gist.github.com/zonca/8994544, you can run as:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;./ipcluster_run_commands.py commands.txt
&lt;/pre&gt;&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Andrea Zonca</dc:creator><pubDate>Thu, 20 Mar 2014 19:30:00 -0700</pubDate><guid isPermaLink="false">tag:zonca.github.io,2014-03-20:2014/03/setup-ipython-notebook-parallel-Gordon.html</guid><category>hpc</category><category>python</category><category>Gordon</category></item><item><title>Build Software Carpentry lessons with Pelican</title><link>http://zonca.github.io/2014/02/build-software-carpentry-with-pelican.html</link><description>&lt;p&gt;&lt;a href="http://www.software-carpentry.org"&gt;Software Carpentry&lt;/a&gt; offers bootcamps for scientist to teach basic programming skills.
All the material, mainly about bash, git, Python and R is &lt;a href="http://github.com/swcarpentry/bc"&gt;available on Github&lt;/a&gt; under Creative Commons.&lt;/p&gt;
&lt;p&gt;The content is either in Markdown or in IPython notebook format, and is currently built using Jekyll, nbconvert and Pandoc.
Basicly the requirement is to make it easy for bootcamp instructors to setup their own website, modify the content, and have the website updated.&lt;/p&gt;
&lt;p&gt;I created a fork of the Software Carpentry repository and configured Pelican for creating the website:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/swcarpentry-pelican/bootcamp-pelican"&gt;bootcamp-pelican repository&lt;/a&gt;: contains Markdown lessons in &lt;code&gt;lessons&lt;/code&gt; (version v5), &lt;code&gt;.ipynb&lt;/code&gt; in &lt;code&gt;notebooks&lt;/code&gt; and news items in &lt;code&gt;news&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/swcarpentry-pelican/swcarpentry-pelican.github.io"&gt;bootcamp-pelican Github pages&lt;/a&gt;: This repository contains the output HTML&lt;/li&gt;
&lt;li&gt;&lt;a href="http://swcarpentry-pelican.github.io/"&gt;bootcamp-pelican website&lt;/a&gt;: this is the URL where Github publishes automatically the content of the previous repository&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Pelican handles fenced code blocks, see &lt;a href="http://swcarpentry-pelican.github.io/"&gt;http://swcarpentry-pelican.github.io/&lt;/a&gt; and conversion of IPython notebooks, see &lt;a href="http://swcarpentry-pelican.github.io/lessons/numpy-notebook.html"&gt;http://swcarpentry-pelican.github.io/lessons/numpy-notebook.html&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;How to setup the repositories for a new bootcamp&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="https://github.com/organizations/new"&gt;create a new Organization on Github&lt;/a&gt; and add all the other instructors, name it: &lt;code&gt;swcarpentry-YYYY-MM-DD-INST&lt;/code&gt; where &lt;code&gt;INST&lt;/code&gt; is the institution name, e.g. &lt;code&gt;NYU&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/swcarpentry-pelican/bootcamp-pelican/fork"&gt;Fork the &lt;code&gt;bootcamp-pelican&lt;/code&gt; repository&lt;/a&gt; under the organization account&lt;/li&gt;
&lt;li&gt;Create a new repository in your organization named &lt;code&gt;swcarpentry-YYYY-MM-DD-INST.github.io&lt;/code&gt; that will host the HTML of the website, also tick &lt;strong&gt;initialize with README&lt;/strong&gt;, it will help later.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Now you can either prepare the build environment on your laptop or have the web service &lt;code&gt;travis-ci&lt;/code&gt; automatically update the website whenever you update the repository (even from the Github web interface!).&lt;/p&gt;
&lt;h2&gt;Build/Update the website from your laptop&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Clone the &lt;code&gt;bootcamp-pelican&lt;/code&gt; repository of your organization locally&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Create a &lt;code&gt;Python&lt;/code&gt; virtual environment and install requirements with:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;cd bootcamp-pelican
virtualenv swcpy
. swcpy/bin/activate
pip install -r requirements.txt
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Clone the &lt;code&gt;swcarpentry-YYYY-MM-DD-INST.github.io&lt;/code&gt; in the output folder as:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;git clone git@github.com:swcarpentry-YYYY-MM-DD-INST.github.io.git output
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Build or Update the website with Pelican running&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;fab build
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;You can display the website in your browser locally with:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;fab serve
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Finally you can publish it to Github with:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;cd output
git add .
git push origin master
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Configure Travis-ci to automatically build and publish the website&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Go to &lt;a href="http://travis-ci.org"&gt;http://travis-ci.org&lt;/a&gt; and login with Github credentials&lt;/li&gt;
&lt;li&gt;Under &lt;a href="https://travis-ci.org/profile"&gt;https://travis-ci.org/profile&lt;/a&gt; click on the organization name on the left and activate the webhook setting &lt;code&gt;ON&lt;/code&gt; on your &lt;code&gt;bootcamp-pelican&lt;/code&gt; repository&lt;/li&gt;
&lt;li&gt;Now it is necessary to setup the credentials for &lt;code&gt;travis-ci&lt;/code&gt; to write to the repository&lt;/li&gt;
&lt;li&gt;Go to https://github.com/settings/tokens/new, create a new token with default permissions&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Install the &lt;code&gt;travis&lt;/code&gt; tool (in debian/ubuntu &lt;code&gt;sudo gem install travis&lt;/code&gt;) and run from any machine (not necessary to have a clone of the repository):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;travis encrypt -r swcarpentry-YYYY-MM-DD-INST/bootcamp-pelican GH_TOKEN=TOKENGOTATTHEPREVIOUSSTEP
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;otherwise I've setup a web application that does the encryption in your browser, see: &lt;a href="http://travis-encrypt.github.io"&gt;http://travis-encrypt.github.io&lt;/a&gt;
1. Open &lt;code&gt;.travis.yml&lt;/code&gt; on the website and replace the string under &lt;code&gt;env: global: secure:&lt;/code&gt; with the string from &lt;code&gt;travis encrypt&lt;/code&gt;
1. Push the modified &lt;code&gt;.travis.yml&lt;/code&gt; to trigger the first build by Travis, and then check the log on &lt;a href="http://travis-ci.org"&gt;http://travis-ci.org&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Now any change on the source repository will be picked up automatically by Travis and used to update the website.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Andrea Zonca</dc:creator><pubDate>Wed, 26 Feb 2014 23:00:00 -0800</pubDate><guid isPermaLink="false">tag:zonca.github.io,2014-02-26:2014/02/build-software-carpentry-with-pelican.html</guid><category>python</category><category>software-carpentry</category><category>pelican</category></item><item><title>openproceedings: Github/FigShare based publishing platform for conference proceedings</title><link>http://zonca.github.io/2014/02/openproceedings-github-figshare-pelican-conference-proceedings.html</link><description>&lt;p&gt;Github provides a great interface for gathering, peer reviewing and accepting papers for conference proceedings, the second step is to publish them on a website either in HTML or PDF form or both.
The Scipy conference is at the forefront on this and did great work in peer reviewing on Github, see: &lt;a href="https://github.com/scipy-conference/scipy_proceedings/pull/61"&gt;https://github.com/scipy-conference/scipy_proceedings/pull/61&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I wanted to develop a system to make it easier to continously publish updated versions of the papers and also leverage FigShare to provide a long term repository, a sharing interface and a &lt;a href="http://en.wikipedia.org/wiki/Digital_object_identifier"&gt;DOI&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I based it on the blog engine &lt;a href="http://getpelican.com"&gt;&lt;code&gt;Pelican&lt;/code&gt;&lt;/a&gt;, developed a plugin &lt;a href="http://github.com/openproceedings/pelican_figshare_pdf"&gt;&lt;code&gt;figshare_pdf&lt;/code&gt;&lt;/a&gt; to upload a PDF of an article via API and configured &lt;a href="http://travis-ci.org"&gt;Travis-ci&lt;/a&gt; as building platform.&lt;/p&gt;
&lt;p&gt;See more details on the project page on Github:
&lt;a href="https://github.com/openproceedings/openproceedings-buildbot"&gt;https://github.com/openproceedings/openproceedings-buildbot&lt;/a&gt;&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Andrea Zonca</dc:creator><pubDate>Thu, 13 Feb 2014 23:30:00 -0800</pubDate><guid isPermaLink="false">tag:zonca.github.io,2014-02-13:2014/02/openproceedings-github-figshare-pelican-conference-proceedings.html</guid><category>python</category><category>pelican</category><category>openscience</category></item><item><title>wget file from google drive</title><link>http://zonca.github.io/2014/01/wget-file-from-google-drive.html</link><description>&lt;p&gt;Sometimes it is useful, even more if you have a chromebook, to upload a file to Google Drive and then use &lt;code&gt;wget&lt;/code&gt; to retrieve it from a server remotely.&lt;/p&gt;
&lt;p&gt;In order to do this you need to make the file available to "Anyone with the link", then click on that link from your local machine and get to the download page that displays a Download button.
Now right-click and select "Show page source" (in Chrome), and search for "downloadUrl", copy the url that starts with &lt;code&gt;https://docs.google.com&lt;/code&gt;, for example:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;https://docs.google.com/uc?id\u003d0ByPZe438mUkZVkNfTHZLejFLcnc\u0026export\u003ddownload\u0026revid\u003d0ByPZe438mUkZbUIxRkYvM2dwbVduRUxSVXNERm0zZFFiU2c0PQ
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This is unicode, so open &lt;code&gt;Python&lt;/code&gt; and do:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;download_url = &amp;quot;PASTE HERE&amp;quot;
print download_url.decode(&amp;quot;unicode_escape&amp;quot;)
u&amp;#39;https://docs.google.com/uc?id=0ByPZe438mUkZVkNfTHZLejFLcnc&amp;amp;export=download&amp;amp;revid=0ByPZe438mUkZbUIxRkYvM2dwbVduRUxSVXNERm0zZFFiU2c0PQ&amp;#39;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The last url can be pasted into a terminal and used with &lt;code&gt;wget&lt;/code&gt;.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Andrea Zonca</dc:creator><pubDate>Fri, 31 Jan 2014 18:00:00 -0800</pubDate><guid isPermaLink="false">tag:zonca.github.io,2014-01-31:2014/01/wget-file-from-google-drive.html</guid><category>bash</category></item><item><title>Run IPython Notebook on a HPC Cluster via PBS</title><link>http://zonca.github.io/2013/12/run-ipython-notebook-on-HPC-cluster-via-PBS.html</link><description>&lt;p&gt;The &lt;a href="http://ipython.org/notebook.html"&gt;IPython notebook&lt;/a&gt; is a great tool for data exploration
and visualization.
It is suitable in particular for analyzing a large amount of data remotely on a computing node
of a HPC cluster and visualize it in a browser that runs on a local machine.
In this configuration, the interface is local, it is very responsive, but the amount of memory
and CPU horsepower is provided by a HPC computing node.&lt;/p&gt;
&lt;p&gt;Also, it is possible to keep the notebook server running, disconnect and reconnect later from
another machine to the same session.&lt;/p&gt;
&lt;p&gt;I created a script which is very general and can be used on most HPC cluster and published it on Github:&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/pyHPC/ipynbhpc"&gt;https://github.com/pyHPC/ipynbhpc&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Once the script is running, it is possible to connect to &lt;code&gt;localhost:PORT&lt;/code&gt; and visualize the 
IPython notebook, see the following screenshot of Chromium running locally on my machine
connected to a IPython notebook running on a Gordon computing node:&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/run-ipython-notebook-on-HPC-cluster-via-PBS_screenshot.png" alt="IPython notebook on Gordon" style="width: 730px;"/&gt;&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Andrea Zonca</dc:creator><pubDate>Wed, 18 Dec 2013 16:30:00 -0800</pubDate><guid isPermaLink="false">tag:zonca.github.io,2013-12-18:2013/12/run-ipython-notebook-on-HPC-cluster-via-PBS.html</guid><category>ipython</category><category>ipython-notebook</category><category>HPC</category></item><item><title>Joining San Diego Supercomputer Center</title><link>http://zonca.github.io/2013/12/joining-sandiego-supercomputer-center.html</link><description>&lt;p&gt;&lt;code&gt;TL;DR&lt;/code&gt;
Left UCSB after 4 years, got staff position at San Diego Supercomputer Center within UCSD, will be helping research groups analyze their data on Gordon and more. Still 20% on Planck.&lt;/p&gt;
&lt;p&gt;I spent 4 great years at UCSB with Peter Meinhold working on analyzing Cosmic Microwave Background data from the ESA Planck space mission.
Cosmology is fascinating, also I enjoyed working with a very open minded team, that always left large freedom in choosing the techniques and the software tools for the job.&lt;/p&gt;
&lt;p&gt;My work has been mainly focused on understanding and characterizing large amount of data using &lt;code&gt;Python&lt;/code&gt; (and &lt;code&gt;C++&lt;/code&gt;) on NERSC supercomputers.
I was neither interested nor fit for a traditional academic career, and I was looking for a job that allowed me to focus on doing research/data analysis full time.&lt;/p&gt;
&lt;p&gt;The perfect opportunity showed up, as the San Diego Supercomputer Center was looking for a computational scientist with a strong scientific background in any field of science to help research teams jump into supercomputing, specifically newcomers.  This involves having the opportunity to collaborate with groups in any area of science, the first projects I am going to work on will be in Astrophysics, Quantum Chemistry and Genomics!&lt;/p&gt;
&lt;p&gt;I also have the opportunity to continue my work on calibration and mapmaking of Planck data in collaboration with UCSB for 20% of my time.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Andrea Zonca</dc:creator><pubDate>Tue, 10 Dec 2013 13:30:00 -0800</pubDate><guid isPermaLink="false">tag:zonca.github.io,2013-12-10:2013/12/joining-sandiego-supercomputer-center.html</guid><category>jobs</category></item><item><title>Published paper on Destriping Cosmic Microwave Background Polarimeter data</title><link>http://zonca.github.io/2013/11/published-paper-destriping-CMB-polarimeter.html</link><description>&lt;p&gt;TL;DR version:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Preprint on arxiv: &lt;a href="http://arxiv.org/abs/1309.5609"&gt;Destriping Cosmic Microwave Background Polarimeter data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Destriping &lt;code&gt;python&lt;/code&gt; code on github: &lt;a href="https://github.com/zonca/dst"&gt;&lt;code&gt;dst&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Output maps and sample input data on figshare: &lt;a href="http://figshare.com/articles/BMachine_40GHz_CMB_Polarimeter_sky_maps/644507"&gt;BMachine 40GHz CMB Polarimeter sky maps&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;(Paywalled published paper: &lt;a href="http://dx.doi.org/10.1016/j.ascom.2013.10.002"&gt;Destriping Cosmic Microwave Background Polarimeter data&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;My last paper was published by &lt;a href="http://www.journals.elsevier.com/astronomy-and-computing/"&gt;Astronomy and Computing&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The paper is focused on Cosmic Microwave Background data destriping, a map-making tecnique which exploits the fast
scanning of instruments in order to efficiently remove correlated low frequency noise, generally caused by thermal
fluctuations and gain instability of the amplifiers.&lt;/p&gt;
&lt;p&gt;The paper treats in particular the case of destriping data from a polarimeter, i.e. an instrument which directly measures
the polarized signal from the sky, which allows some simplification compared to the case of a simply polarization-sensitive
radiometer.&lt;/p&gt;
&lt;p&gt;I implemented a fully parallel &lt;code&gt;python&lt;/code&gt; implementation of the algorithm based on:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://trilinos.sandia.gov/packages/pytrilinos/"&gt;&lt;code&gt;PyTrilinos&lt;/code&gt;&lt;/a&gt; for Distributed Linear Algebra via MPI&lt;/li&gt;
&lt;li&gt;&lt;code&gt;HDF5&lt;/code&gt; for I/O&lt;/li&gt;
&lt;li&gt;&lt;code&gt;cython&lt;/code&gt; for improving the performance of the inner loops&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The code is available on Github under GPL.&lt;/p&gt;
&lt;p&gt;The output maps for about 30 days of the UCSB B-Machine polarimeter at 37.5 GHz are available on FigShare.&lt;/p&gt;
&lt;p&gt;The experience of publishing with ASCOM was really positive, I received 2 very helpful reviews that drove me to
work on several improvements on the paper.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Andrea Zonca</dc:creator><pubDate>Wed, 20 Nov 2013 21:30:00 -0800</pubDate><guid isPermaLink="false">tag:zonca.github.io,2013-11-20:2013/11/published-paper-destriping-CMB-polarimeter.html</guid><category>python</category><category>paper</category><category>destriping</category><category>openscience</category></item><item><title>Jiffylab multiuser IPython notebooks</title><link>http://zonca.github.io/2013/10/jiffylab-multiuser-ipython-notebooks.html</link><description>&lt;p&gt;&lt;a href="https://github.com/ptone/jiffylab"&gt;jiffylab&lt;/a&gt; is a very interesting project by &lt;a href="https://twitter.com/ptone"&gt;Preston Holmes&lt;/a&gt; to provide sandboxed IPython notebooks instances on a server using &lt;a href="http://www.docker.io/"&gt;docker&lt;/a&gt;.
There are several user cases, for example:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In a tutorial about &lt;code&gt;python&lt;/code&gt;, give users instant access to a working IPython notebook&lt;/li&gt;
&lt;li&gt;In a tutorial about some specific &lt;code&gt;python&lt;/code&gt; package, give users instant access to a python environment with that package already installed&lt;/li&gt;
&lt;li&gt;Give students in a research group access to &lt;code&gt;python&lt;/code&gt; on a server with preinstalled several packages maintained and updated by an expert user.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;How to install &lt;a href="https://github.com/ptone/jiffylab"&gt;jiffylab&lt;/a&gt; on Ubuntu 12.04&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://docs.docker.io/en/latest/installation/ubuntulinux/#ubuntu-precise"&gt;Install &lt;code&gt;docker&lt;/code&gt; on Ubuntu Precise&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Copy-paste each line of &lt;code&gt;linux-setup.sh&lt;/code&gt; to a terminal, to check what is going on step by step&lt;/li&gt;
&lt;li&gt;To start the application, change user to &lt;code&gt;jiffylabweb&lt;/code&gt;:&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;sudo su jiffylabweb
&lt;span class="nb"&gt;cd&lt;/span&gt; /usr/local/etc/jiffylab/webapp/
python app.py &lt;span class="c1"&gt;#run in debug mode&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;ul&gt;
&lt;li&gt;Point your browser to the server to check debugging messages, if any.&lt;/li&gt;
&lt;li&gt;Finally start the application in production mode:&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;python server.py &lt;span class="c1"&gt;#run in production mode&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;How &lt;code&gt;jiffylab&lt;/code&gt; works&lt;/h2&gt;
&lt;p&gt;Each users gets a sandboxed IPython notebook instance, the user can save the notebooks and reconnect to the same session later. Main things missing:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;No real authentication system / no HTTPS connection, easy workaround would be to allow access only from local network/VPN/SSH tunnel&lt;/li&gt;
&lt;li&gt;No scientific packages preinstalled, need to customize the docker image to have &lt;code&gt;numpy&lt;/code&gt;, &lt;code&gt;matplotlib&lt;/code&gt;, &lt;code&gt;pandas&lt;/code&gt;...&lt;/li&gt;
&lt;li&gt;No access to common filesystem, read-only, this I think is the most pressing feature missing, &lt;a href="https://github.com/ptone/jiffylab/issues/12"&gt;issue already on Github&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I think that just adding the common filesystem would be enough to make the project already usable to provide students a way to easily get started with python.&lt;/p&gt;
&lt;h2&gt;Few screenshots&lt;/h2&gt;
&lt;h3&gt;Login page&lt;/h3&gt;
&lt;p&gt;&lt;img src="/images/jiffylab_intro.png" alt="Jiffylab Login page" style="width: 730px;"/&gt;&lt;/p&gt;
&lt;h3&gt;IPython notebook dashboard&lt;/h3&gt;
&lt;p&gt;&lt;img src="/images/jiffylab_dashboard.png" alt="Jiffylab IPython notebook dashboard" style="width: 730px;"/&gt;&lt;/p&gt;
&lt;h3&gt;IPython notebook&lt;/h3&gt;
&lt;p&gt;&lt;img src="/images/jiffylab_notebook.png" alt="Jiffylab IPython notebook" style="width: 730px;"/&gt;&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Andrea Zonca</dc:creator><pubDate>Mon, 14 Oct 2013 10:30:00 -0700</pubDate><guid isPermaLink="false">tag:zonca.github.io,2013-10-14:2013/10/jiffylab-multiuser-ipython-notebooks.html</guid><category>python</category><category>ipython-notebook</category></item><item><title>How to log exceptions in Python</title><link>http://zonca.github.io/2013/10/how-to-log-exceptions-in-python.html</link><description>&lt;p&gt;Sometimes it is useful to just catch any exception, write details to a log file and continue execution.&lt;/p&gt;
&lt;p&gt;In the &lt;code&gt;Python&lt;/code&gt; standard library, it is possible to use the &lt;code&gt;logging&lt;/code&gt; and &lt;code&gt;exceptions&lt;/code&gt; modules to achieve this.
First of all, we want to catch any exception, but also being able to access all information about it:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;try&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;my_function_1&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="k"&gt;except&lt;/span&gt; &lt;span class="n"&gt;exception&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Exception&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;__class__&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;__doc__&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;message&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Then we want to write those to a logging file, so we need to setup the logging module:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;logging&lt;/span&gt;
&lt;span class="n"&gt;logging&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;basicConfig&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="n"&gt;filename&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;main.log&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                     &lt;span class="n"&gt;filemode&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;w&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                     &lt;span class="n"&gt;level&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;logging&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DEBUG&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                     &lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="si"&gt;%(asctime)s&lt;/span&gt;&lt;span class="s1"&gt; - &lt;/span&gt;&lt;span class="si"&gt;%(levelname)s&lt;/span&gt;&lt;span class="s1"&gt; - &lt;/span&gt;&lt;span class="si"&gt;%(message)s&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                   &lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;a href="https://gist.github.com/zonca/6782980"&gt;In the following gist&lt;/a&gt; everything together, with also &lt;a href="http://stackoverflow.com/questions/2380073/how-to-identify-what-function-call-raise-an-exception-in-python"&gt;function name detection from Alex Martelli&lt;/a&gt;:&lt;/p&gt;
&lt;script src="https://gist.github.com/zonca/6782980.js"&gt;&lt;/script&gt;

&lt;p&gt;Here the output log:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;2013-10-01 11:32:56,466 - ERROR - Function my_function_1() raised &amp;lt;type &amp;#39;exceptions.IndexError&amp;#39;&amp;gt; (Sequence index out of range.): Some indexing error
2013-10-01 11:32:56,466 - ERROR - Function my_function_2() raised &amp;lt;class &amp;#39;my_module.MyException&amp;#39;&amp;gt; (This is my own Exception): Something went quite wrong
2013-10-01 11:32:56,466 - ERROR - Function my_function_1_wrapper() raised &amp;lt;type &amp;#39;exceptions.IndexError&amp;#39;&amp;gt; (Sequence index out of range.): Some indexing error
&lt;/pre&gt;&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Andrea Zonca</dc:creator><pubDate>Tue, 01 Oct 2013 10:30:00 -0700</pubDate><guid isPermaLink="false">tag:zonca.github.io,2013-10-01:2013/10/how-to-log-exceptions-in-python.html</guid><category>python</category><category>exceptions</category></item><item><title>Google Plus comments plugin for Pelican</title><link>http://zonca.github.io/2013/09/google-plus-comments-plugin-for-pelican.html</link><description>&lt;p&gt;There has been recently several discussions about 
&lt;a href="http://www.popsci.com/science/article/2013-09/why-were-shutting-our-comments"&gt;whether comments are any useful on blogs&lt;/a&gt;
I think it is important to find better ways to connect blogs to social networks.
In my opinion the most suitable social network for this is Google+, because there is space for larger discussion, without Twitter's character limit.&lt;/p&gt;
&lt;p&gt;So, for my small blog I've decided to implement the Google+ commenting system, which Google originally implemented just for Blogger but that &lt;a href="http://browsingthenet.blogspot.com/2013/04/google-plus-comments-on-any-website.html"&gt;works on any website&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;See it in action below.&lt;/p&gt;
&lt;p&gt;The plugin is available in the &lt;code&gt;googleplus_comments&lt;/code&gt; branch in:&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/zonca/pelican-plugins/tree/googleplus_comments/googleplus_comments"&gt;https://github.com/zonca/pelican-plugins/tree/googleplus_comments/googleplus_comments&lt;/a&gt;&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Andrea Zonca</dc:creator><pubDate>Fri, 27 Sep 2013 17:45:00 -0700</pubDate><guid isPermaLink="false">tag:zonca.github.io,2013-09-27:2013/09/google-plus-comments-plugin-for-pelican.html</guid><category>python</category><category>pelican</category><category>github</category></item><item><title>How to automatically build your Pelican blog and publish it to Github Pages</title><link>http://zonca.github.io/2013/09/automatically-build-pelican-and-publish-to-github-pages.html</link><description>&lt;p&gt;Something I like a lot about Jekyll, the Github static blog generator, is that you just push commits to your repository and Github takes care of re-building and publishing your website.
Thanks to this, it is possible to create a quick blog post from the Github web interface, without the need to use a machine with Python environment.&lt;/p&gt;
&lt;p&gt;The Pelican developers have a &lt;a href="http://blog.getpelican.com/using-pelican-with-heroku.html"&gt;method for building and deploying Pelican on Heroku&lt;/a&gt;, which is really useful, but I would like instead to use Github Pages.&lt;/p&gt;
&lt;p&gt;I realized that the best way to do this is to rely on &lt;a href="https://travis-ci.org/"&gt;Travis-CI&lt;/a&gt;, as the build/deploy workflow is pretty similar to install/unit-testing Travis is designed for.&lt;/p&gt;
&lt;h2&gt;How to setup Pelican to build on Travis&lt;/h2&gt;
&lt;p&gt;I suggest to use 2 separate git repositories on Github for the source and the built website, let's first only create the repository for the source:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;create the &lt;code&gt;yourusername.github.io-source&lt;/code&gt; repository for Pelican and add it as &lt;code&gt;origin&lt;/code&gt; in your Pelican folder repository&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;add a &lt;code&gt;requirements.txt&lt;/code&gt; file in your Pelican folder:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;github&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="n"&gt;zonca&lt;/span&gt;&lt;span class="sr"&gt;/zonca.github.io-source/&lt;/span&gt;&lt;span class="n"&gt;requirements&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;txt&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;add a &lt;code&gt;.travis.yml&lt;/code&gt; file to your repository:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;github&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="n"&gt;zonca&lt;/span&gt;&lt;span class="sr"&gt;/zonca.github.io-source/&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;travis&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;yml&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;In order to create the encrypted token under env, you can login to the Github web interface to get an &lt;a href="https://help.github.com/articles/creating-an-access-token-for-command-line-use"&gt;Authentication Token&lt;/a&gt;, and then install the &lt;code&gt;travis&lt;/code&gt; command line tool with:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;# on Ubuntu you need ruby dev
sudo apt-get install ruby1.9.1-dev
sudo gem install travis
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;and run from inside the repository:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;travis encrypt GH_TOKEN=LONGTOKENFROMGITHUB --add env.global
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Then add also the &lt;code&gt;deploy.sh&lt;/code&gt; script and update the global variable with yours:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;github&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="n"&gt;zonca&lt;/span&gt;&lt;span class="sr"&gt;/zonca.github.io-source/&lt;/span&gt;&lt;span class="n"&gt;deploy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="na"&gt;sh&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Then we can create the repository that will host the actual blog:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;create the &lt;code&gt;yourusername.github.io&lt;/code&gt; repository for the website (with initial readme, so you can clone it)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Finally we can connect to &lt;a href="https://travis-ci.org/"&gt;Travis-CI&lt;/a&gt;, connect our Github profile and activate Continous Integration on our &lt;code&gt;yourusername.github.io-source&lt;/code&gt; repository.&lt;/p&gt;
&lt;p&gt;Now, you can push a new commit to your source repository and check on Travis if the build and deploy is successful, hopefully it is (joking, no way it is going to work on the first try!).&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Andrea Zonca</dc:creator><pubDate>Thu, 26 Sep 2013 13:45:00 -0700</pubDate><guid isPermaLink="false">tag:zonca.github.io,2013-09-26:2013/09/automatically-build-pelican-and-publish-to-github-pages.html</guid><category>python</category><category>travis-ci</category><category>github</category></item><item><title>clviewer, interactive plot of CMB spectra</title><link>http://zonca.github.io/2013/09/clviewer-interactive-plot-of-CMB-spectra.html</link><description>&lt;p&gt;Today it was HackDay at &lt;a href="http://dotastronomy.com"&gt;.Astronomy&lt;/a&gt;, so I felt compelled to hack something around myself,
creating something I have been thinking for a while after my previous work on &lt;a href="http://zonca.github.io/2013/08/interactive-figures-planck-power-spectra.html"&gt;Interactive CMB power spectra in the browser&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The idea is to get text files from a user and load it in a browser-based interactive display built on top of the &lt;a href="http://d3js.org"&gt;d3.js&lt;/a&gt; and &lt;a href="http://code.shutterstock.com/rickshaw/"&gt;rickshaw&lt;/a&gt; libraries.&lt;/p&gt;
&lt;p&gt;Similar to &lt;a href="http://nbviewer.ipython.org/"&gt;nbviewer&lt;/a&gt;, I think it is very handy to load data from &lt;a href="https://gist.github.com/"&gt;Github gists&lt;/a&gt;, because then there is no need of uploading files and it is easier to circulate links.&lt;/p&gt;
&lt;p&gt;So I created a small web app, in &lt;code&gt;Python&lt;/code&gt; of course, using &lt;a href="http://flask.pocoo.org/"&gt;Flask&lt;/a&gt; and deployed on &lt;a href="http://heroku.com"&gt;Heroku&lt;/a&gt;.
It just gets a gist number, calls the Github APIs to load the files, and displays them in the browser:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Application website: &lt;a href="http://clviewer.herokuapp.com"&gt;http://clviewer.herokuapp.com&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Example input data: &lt;a href="https://gist.github.com/zonca/6599016"&gt;https://gist.github.com/zonca/6599016&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Example interactive plot: &lt;a href="http://clviewer.herokuapp.com/6599016"&gt;http://clviewer.herokuapp.com/6599016&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Source: &lt;a href="https://github.com/zonca/clviewer"&gt;https://github.com/zonca/clviewer&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Andrea Zonca</dc:creator><pubDate>Tue, 17 Sep 2013 18:30:00 -0700</pubDate><guid isPermaLink="false">tag:zonca.github.io,2013-09-17:2013/09/clviewer-interactive-plot-of-CMB-spectra.html</guid><category>cosmology</category><category>python</category><category>astrophysics</category><category>Planck</category><category>dotastronomy</category></item><item><title>Planck CMB map at high resolution</title><link>http://zonca.github.io/2013/09/Planck-CMB-map-at-high-resolution.html</link><description>&lt;p&gt;Prompted by a colleague, I created a high-resolution version of the Cosmic Microwave Background map in MollWeide projection released by the Planck collaboration, available on the &lt;a href="http://irsa.ipac.caltech.edu/data/Planck/release_1/all-sky-maps/previews/COM_CompMap_CMB-smica_2048_R1.20/index.html"&gt;Planck Data Release Website&lt;/a&gt; in FITS format.&lt;/p&gt;
&lt;p&gt;The map is a PNG at a resolution of 17469x8796 pixels, which is suitable for printing at 300dpi up to 60x40 inch, or 150x100 cm, file size is about 150MB.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Update&lt;/em&gt;: now with Planck color scale&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Update&lt;/em&gt;: previous version had grayed out pixels in the galactic plane represents the fraction of the sky that is not possible to reconstruct due to bright galactic sources. The last version uses inpainting to create a constrained CMB realization with the same statistics as the observed CMB to fill the unobserved pixels, more details in the &lt;a href="http://www.sciops.esa.int/wikiSI/planckpla/index.php?title=CMB_and_astrophysical_component_maps&amp;amp;instance=Planck_Public_PLA"&gt;Planck Explanatory Supplement&lt;/a&gt;. &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://dx.doi.org/10.6084/m9.figshare.795296"&gt;High Resolution image on FigShare&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Small size preview:&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="Preview of Planck CMB map" src="/images/Planck-CMB-map-at-high-resolution_planck_cmb_map.jpg" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Python code:&lt;/li&gt;
&lt;/ul&gt;
&lt;script src="https://gist.github.com/zonca/6515744.js"&gt;&lt;/script&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Andrea Zonca</dc:creator><pubDate>Tue, 10 Sep 2013 14:00:00 -0700</pubDate><guid isPermaLink="false">tag:zonca.github.io,2013-09-10:2013/09/Planck-CMB-map-at-high-resolution.html</guid><category>cosmology</category><category>python</category><category>astrophysics</category><category>Planck</category></item><item><title>Run Hadoop Python jobs on Amazon with MrJob</title><link>http://zonca.github.io/2013/09/run-hadoop-python-jobs-on-amazon-with-mrjob.html</link><description>&lt;p&gt;&lt;br/&gt;
First we need to install mrjob with:
&lt;br/&gt;
&lt;blockquote class="tr_bq"&gt;
 pip install mrjob
&lt;/blockquote&gt;
I am starting with a simple example of word counting. Previously I implemented this directly using the hadoop streaming interface, therefore mapper and reducer were scripts that read from standard input and print to standard output, see mapper.py and reducer.py in:
&lt;br/&gt;
&lt;br/&gt;
&lt;a href="https://github.com/zonca/python-wordcount-hadoop"&gt;
 https://github.com/zonca/python-wordcount-hadoop
&lt;/a&gt;
&lt;br/&gt;
&lt;br/&gt;
With MrJob instead the interface is a little different, we implement the mapper  method of our subclass of MrJob that already gets a "line" argument and yields the output as a tuple like ("word", 1).
&lt;br/&gt;
&lt;div&gt;
 MrJob makes the implementation of the reducer particularly simple. Using hadoop-streaming directly, we needed also to first parse back the output of the mapper into python objects, while MrJob does it for you and gives directly the key and the list of count, that we just need to sum.
&lt;/div&gt;
&lt;div&gt;
 &lt;br/&gt;
 &lt;a name="more"&gt;
 &lt;/a&gt;
&lt;/div&gt;
&lt;div&gt;
 The code is pretty simple:
 &lt;br/&gt;
 &lt;br/&gt;
 &lt;script src="http://gist-it.appspot.com/github/zonca/python-wordcount-hadoop/blob/master/mrjob/word_count_mrjob.py"&gt;
 &lt;/script&gt;
 &lt;div&gt;
  &lt;br/&gt;
 &lt;/div&gt;
 First we can test locally with 2 different methods, either:
 &lt;br/&gt;
 &lt;br/&gt;
 &lt;blockquote class="tr_bq"&gt;
  python word_count_mrjob.py gutemberg/20417.txt.utf-8
 &lt;/blockquote&gt;
 &lt;br/&gt;
 or:
 &lt;br/&gt;
 &lt;br/&gt;
 &lt;blockquote class="tr_bq"&gt;
  python word_count_mrjob.py --runner=local gutemberg/20417.txt.utf-8
 &lt;/blockquote&gt;
 &lt;br/&gt;
 The first is a simple local test, the seconds sets some hadoop variables and uses multiprocessing to run the mapper in parallel.
 &lt;br/&gt;
 &lt;div&gt;
  &lt;br/&gt;
 &lt;/div&gt;
 &lt;span style="font-size: large;"&gt;
  Run on Amazon Elastic Map Reduce
 &lt;/span&gt;
 &lt;br/&gt;
 &lt;br/&gt;
&lt;/div&gt;
&lt;div&gt;
 Next step is submitting the job to EMR.
 &lt;br/&gt;
 First get an account on Amazon Web Services from
 &lt;a href="http://aws.amazon.com/"&gt;
  aws.amazon.com
 &lt;/a&gt;
 .
 &lt;br/&gt;
 &lt;br/&gt;
 Setup MrJob with Amazon:
 &lt;br/&gt;
 &lt;br/&gt;
 &lt;a href="http://pythonhosted.org/mrjob/guides/emr-quickstart.html#amazon-setup"&gt;
  http://pythonhosted.org/mrjob/guides/emr-quickstart.html#amazon-setup
 &lt;/a&gt;
 &lt;br/&gt;
 &lt;br/&gt;
 &lt;div&gt;
  Then we just need to choose the "emr" runner for MrJob to take care of:
 &lt;/div&gt;
 &lt;div&gt;
  &lt;ul&gt;
   &lt;li&gt;
    Copy the python module to Amazon S3, with requirements
   &lt;/li&gt;
   &lt;li&gt;
    Copy the input data to S3
   &lt;/li&gt;
   &lt;li&gt;
    Create a small EC2 instance (of course we could set it up to run 1000 instead)
   &lt;/li&gt;
   &lt;li&gt;
    Run Hadoop to process the jobs
   &lt;/li&gt;
   &lt;li&gt;
    Create a local web service that allows easy monitoring of the cluster
   &lt;/li&gt;
   &lt;li&gt;
    When completed, copy the results back (this can be disabled to just leave the results on S3.
   &lt;/li&gt;
  &lt;/ul&gt;
 &lt;/div&gt;
 &lt;div&gt;
  e.g.:
 &lt;/div&gt;
 &lt;blockquote class="tr_bq"&gt;
  python word_count_mrjob.py --runner=emr --aws-region=us-west-2 gutemberg/20417.txt.utf-8
 &lt;/blockquote&gt;
 &lt;div&gt;
  It is important to make sure that the aws-region used by MrJob is the same we used for creating the SSH key on the EC2 console in the MrJob configuration step, i.e. SSH keys are region-specific.
  &lt;br/&gt;
  &lt;br/&gt;
  &lt;span style="font-size: large;"&gt;
   Logs and output of the run
  &lt;/span&gt;
  &lt;br/&gt;
  &lt;br/&gt;
  MrJob copies the needed files to S3:
  &lt;br/&gt;
  &lt;blockquote class="tr_bq"&gt;
   . runemr.sh
   &lt;br/&gt;
   using configs in /home/zonca/.mrjob.conf
   &lt;br/&gt;
   using existing scratch bucket mrjob-ecd1d07aeee083dd
   &lt;br/&gt;
   using s3://mrjob-ecd1d07aeee083dd/tmp/ as our scratch dir on S3
   &lt;br/&gt;
   creating tmp directory /tmp/mrjobjob.zonca.20130901.192250.785550
   &lt;br/&gt;
   Copying non-input files into s3://mrjob-ecd1d07aeee083dd/tmp/mrjobjob.zonca.20130901.192250.785550/files/
   &lt;br/&gt;
   Waiting 5.0s for S3 eventual consistency
   &lt;br/&gt;
   Creating Elastic MapReduce job flow
   &lt;br/&gt;
   Job flow created with ID: j-2E83MO9QZQILB
   &lt;br/&gt;
   Created new job flow j-2E83MO9QZQILB
  &lt;/blockquote&gt;
  Creates the instances:
  &lt;br/&gt;
  &lt;blockquote class="tr_bq"&gt;
   Job launched 30.9s ago, status STARTING: Starting instances
   &lt;br/&gt;
   Job launched 123.9s ago, status BOOTSTRAPPING: Running bootstrap actions
   &lt;br/&gt;
   Job launched 250.5s ago, status RUNNING: Running step (mrjobjob.zonca.20130901.192250.785550: Step 1 of 1)
  &lt;/blockquote&gt;
  Creates an SSH tunnel to the tracker:
  &lt;br/&gt;
  &lt;blockquote class="tr_bq"&gt;
   Opening ssh tunnel to Hadoop job tracker
   &lt;br/&gt;
   Connect to job tracker at: http://localhost:40630/jobtracker.jsp
  &lt;/blockquote&gt;
 &lt;/div&gt;
 Therefore we can connect to that address to check realtime information about the cluster running on EC2, for example:
 &lt;br/&gt;
 &lt;br/&gt;
 &lt;div class="separator" style="clear: both; text-align: center;"&gt;
  &lt;a href="http://zonca.github.io/images/run-hadoop-python-jobs-on-amazon-with-mrjob_s1600_awsjobdetails.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"&gt;
   &lt;img border="0" height="588" src="http://zonca.github.io/images/run-hadoop-python-jobs-on-amazon-with-mrjob_s640_awsjobdetails.png" width="640"/&gt;
  &lt;/a&gt;
 &lt;/div&gt;
 &lt;br/&gt;
 Once the job completes, MrJob copies the output back to the local machine, here are few lines from the file:
 &lt;br/&gt;
 &lt;blockquote class="tr_bq"&gt;
  "maladies"
  &lt;span class="Apple-tab-span" style="white-space: pre;"&gt;
  &lt;/span&gt;
  1
  &lt;br/&gt;
  "malaria"
  &lt;span class="Apple-tab-span" style="white-space: pre;"&gt;
  &lt;/span&gt;
  5
  &lt;br/&gt;
  "male"
  &lt;span class="Apple-tab-span" style="white-space: pre;"&gt;
  &lt;/span&gt;
  18
  &lt;br/&gt;
  "maleproducing"
  &lt;span class="Apple-tab-span" style="white-space: pre;"&gt;
  &lt;/span&gt;
  1
  &lt;br/&gt;
  "males"
  &lt;span class="Apple-tab-span" style="white-space: pre;"&gt;
  &lt;/span&gt;
  5
  &lt;br/&gt;
  "mammal"
  &lt;span class="Apple-tab-span" style="white-space: pre;"&gt;
  &lt;/span&gt;
  10
  &lt;br/&gt;
  "mammalInstinctive"
  &lt;span class="Apple-tab-span" style="white-space: pre;"&gt;
  &lt;/span&gt;
  1
  &lt;br/&gt;
  "mammalian"
  &lt;span class="Apple-tab-span" style="white-space: pre;"&gt;
  &lt;/span&gt;
  4
  &lt;br/&gt;
  "mammallike"
  &lt;span class="Apple-tab-span" style="white-space: pre;"&gt;
  &lt;/span&gt;
  1
  &lt;br/&gt;
  "mammals"
  &lt;span class="Apple-tab-span" style="white-space: pre;"&gt;
  &lt;/span&gt;
  87
  &lt;br/&gt;
  "mammoth"
  &lt;span class="Apple-tab-span" style="white-space: pre;"&gt;
  &lt;/span&gt;
  5
  &lt;br/&gt;
  "mammoths"
  &lt;span class="Apple-tab-span" style="white-space: pre;"&gt;
  &lt;/span&gt;
  1
  &lt;br/&gt;
  "man"
  &lt;span class="Apple-tab-span" style="white-space: pre;"&gt;
  &lt;/span&gt;
  152
 &lt;/blockquote&gt;
 I've been positively impressed that it is so easy to implement and run a MapReduce job with MrJob without need of managing directly EC2 instances or the Hadoop installation.
 &lt;br/&gt;
 This same setup could be used on GB of data with hundreds of instances.
&lt;/div&gt;&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Andrea Zonca</dc:creator><pubDate>Mon, 02 Sep 2013 02:36:00 -0700</pubDate><guid isPermaLink="false">tag:zonca.github.io,2013-09-02:2013/09/run-hadoop-python-jobs-on-amazon-with-mrjob.html</guid><category>bigdata</category><category>github</category><category>python</category><category>aws</category><category>hadoop</category></item><item><title>Interactive figures in the browser: CMB Power Spectra</title><link>http://zonca.github.io/2013/08/interactive-figures-planck-power-spectra.html</link><description>&lt;p&gt;
 For a long time I've been curious about trying out
 &lt;span style="font-family: Courier New, Courier, monospace;"&gt;
  d3.js
 &lt;/span&gt;
 , the javascript plotting library which is becoming the standard for interactive plotting in the browser.
 &lt;br/&gt;
&lt;/p&gt;

&lt;div&gt;
 &lt;br/&gt;
&lt;/div&gt;

&lt;div&gt;
 What is really appealing is the capability of sharing with other people powerful interactive visualization simply via the link to a web page. This will hopefully be the future of scientific publications, as envisioned, for example, by
 &lt;a href="https://www.authorea.com/"&gt;
  Authorea
 &lt;/a&gt;
 .
&lt;/div&gt;

&lt;div&gt;
 &lt;a name="more"&gt;
 &lt;/a&gt;
 An interesting example related to my work on Planck is a plot of the high number of Angular Power Spectra of the anisotropies of the Cosmic Microwave Background Temperature.
&lt;/div&gt;

&lt;div&gt;
 The CMB Power spectra describe how the temperature fluctuations were distributed in the sky as a function of the angular scale, for example the largest peak at about 1 degree means that the brightest cold/warm spots of the CMB have that angular size, see
 &lt;a href="http://www.strudel.org.uk/blog/astro/001030.shtml"&gt;
  The Universe Simulator in the browser
 &lt;/a&gt;
 .
&lt;/div&gt;

&lt;div&gt;
 The
 &lt;a href="http://irsa.ipac.caltech.edu/data/Planck/release_1/ancillary-data/"&gt;
  Planck Collaboration released
 &lt;/a&gt;
 a combined spectrum, which aggregates several channels to give the best result, spectra frequency by frequency (for some frequencies split in detector-sets) and a best-fit spectrum given a Universe Model.
&lt;/div&gt;

&lt;div&gt;
 It is also interesting to compare to the latest release spectrum by WMAP with 9 years of data.
&lt;/div&gt;

&lt;div&gt;
 &lt;br/&gt;
&lt;/div&gt;

&lt;div&gt;
 The plan is to create a visualization where it is easier to zoom to different angular scales on the horizontal axis and quickly show/hide each curve.
&lt;/div&gt;

&lt;div&gt;
 For this I used
 &lt;a href="http://code.shutterstock.com/rickshaw/"&gt;
  rickshaw
 &lt;/a&gt;
 , a library based on
 &lt;span style="font-family: Courier New, Courier, monospace;"&gt;
  d3.js
 &lt;/span&gt;
 &lt;span style="font-family: inherit;"&gt;
  which makes it easier to create time-series plots.
 &lt;/span&gt;
&lt;/div&gt;

&lt;div&gt;
 &lt;span style="font-family: inherit;"&gt;
  In fact most of the features are already implemented, it is just a matter of configuring them, see the code on github:
 &lt;/span&gt;
 &lt;a href="https://github.com/zonca/visualize-planck-cl"&gt;
  https://github.com/zonca/visualize-planck-cl
 &lt;/a&gt;
&lt;/div&gt;

&lt;div&gt;
 The most complex task is actually to load all the data, previously converted to JSON, in the background from the server and push them in a data structure which is understood by rickshaw.
&lt;/div&gt;

&lt;div&gt;
 &lt;br/&gt;
&lt;/div&gt;

&lt;div&gt;
 Check out the result:
&lt;/div&gt;

&lt;div style="text-align: center;"&gt;
 &lt;b&gt;
  &lt;a href="http://bit.ly/planck-spectra"&gt;
   http://bit.ly/planck-spectra
  &lt;/a&gt;
 &lt;/b&gt;
&lt;/div&gt;

&lt;div&gt;
 &lt;br/&gt;
&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Andrea Zonca</dc:creator><pubDate>Fri, 30 Aug 2013 08:52:00 -0700</pubDate><guid isPermaLink="false">tag:zonca.github.io,2013-08-30:2013/08/interactive-figures-planck-power-spectra.html</guid><category>javascript</category><category>d3</category><category>power spectra</category><category>astrophysics</category><category>Planck</category></item><item><title>Planck CTP angular power spectrum ell binning</title><link>http://zonca.github.io/2013/08/planck-ctp-angular-power-spectrum-ell.html</link><description>&lt;p&gt;
 Planck released a binning of the angular power spectrum in the Explanatory supplement,
 &lt;br/&gt;
 unfortunately the file is in PDF format, non easily machine-readable:
 &lt;br/&gt;
 &lt;br/&gt;
 &lt;a href="http://www.sciops.esa.int/wikiSI/planckpla/index.php?title=Frequency_maps_angular_power_spectra&amp;amp;instance=Planck_Public_PLA"&gt;
  http://www.sciops.esa.int/wikiSI/planckpla/index.php?title=Frequency_maps_angular_power_spectra&amp;amp;instance=Planck_Public_PLA
 &lt;/a&gt;
 &lt;br/&gt;
 &lt;br/&gt;
 So here is a csv version:
 &lt;br/&gt;
 &lt;a href="https://gist.github.com/zonca/6288439"&gt;
  https://gist.github.com/zonca/6288439
 &lt;/a&gt;
 &lt;br/&gt;
 &lt;br/&gt;
 Follows embedded gist.
 &lt;br/&gt;
 &lt;br/&gt;
 &lt;a name="more"&gt;
 &lt;/a&gt;
 &lt;br/&gt;
 &lt;br/&gt;
 &lt;br/&gt;
 &lt;script src="https://gist.github.com/zonca/6288439.js"&gt;
 &lt;/script&gt;
&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Andrea Zonca</dc:creator><pubDate>Tue, 20 Aug 2013 23:03:00 -0700</pubDate><guid isPermaLink="false">tag:zonca.github.io,2013-08-20:2013/08/planck-ctp-angular-power-spectrum-ell.html</guid><category>power spectra</category><category>Planck</category></item><item><title>HEALPix map of the Earth using healpy</title><link>http://zonca.github.io/2013/08/healpix-map-of-earth-using-healpy.html</link><description>&lt;p&gt;
 HEALPix maps can also be used to create equal-area pixelized maps of the Earth, RGB colors are not supported in healpy, so we need to convert the image to colorscale.
 &lt;br/&gt;
 The best user case is for using spherical harmonic transforms, e.g. apply a smoothing filter, in this case HEALPix/healpy tools are really efficient.
 &lt;br/&gt;
 However, other tools for transforming between angles (coordinates), 3d vectors and pixels might be useful.
 &lt;br/&gt;
 &lt;br/&gt;
 &lt;a name="more"&gt;
 &lt;/a&gt;
 &lt;br/&gt;
 I've created an IPython notebook that provides a simple example:
 &lt;br/&gt;
 &lt;br/&gt;
 &lt;a href="http://nbviewer.ipython.org/6187504"&gt;
  http://nbviewer.ipython.org/6187504
 &lt;/a&gt;
 &lt;br/&gt;
 &lt;br/&gt;
 Here is the output Mollweide projection provided by healpy:
 &lt;br/&gt;
 &lt;br/&gt;
&lt;/p&gt;

&lt;div class="separator" style="clear: both; text-align: center;"&gt;
 &lt;a href="http://zonca.github.io/images/healpix-map-of-earth-using-healpy_s1600_download.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"&gt;
  &lt;img border="0" height="230" src="http://zonca.github.io/images/healpix-map-of-earth-using-healpy_s400_download.png" width="400"/&gt;
 &lt;/a&gt;
&lt;/div&gt;

&lt;p&gt;&lt;br/&gt;
Few notes:
&lt;br/&gt;
&lt;br/&gt;
&lt;div&gt;
&lt;/div&gt;
&lt;br/&gt;
&lt;ul style="-webkit-text-stroke-width: 0px; color: black; font-family: 'Times New Roman'; font-size: medium; font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px;"&gt;
 &lt;li&gt;
  always use
  &lt;span style="font-family: Courier New, Courier, monospace;"&gt;
   flip="geo"
  &lt;/span&gt;
  for plotting, otherwise maps are flipped East-West
 &lt;/li&gt;
 &lt;li&gt;
  increase the resolution of the plots (which is different from the resolution of the map array) by providing at least xsize=2000 to mollview and a reso lower than 1 to gnomview
 &lt;/li&gt;
&lt;/ul&gt;&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Andrea Zonca</dc:creator><pubDate>Thu, 08 Aug 2013 19:07:00 -0700</pubDate><guid isPermaLink="false">tag:zonca.github.io,2013-08-08:2013/08/healpix-map-of-earth-using-healpy.html</guid></item><item><title>Export google analytics data via API with Python</title><link>http://zonca.github.io/2013/08/export-google-analytics-data-via-api.html</link><description>&lt;p&gt;
 Fun weekend hacking project: export google analytics data using the google APIs.
 &lt;br/&gt;
 &lt;br/&gt;
 Clone the latest version of the API client from:
 &lt;br/&gt;
 &lt;br/&gt;
 &lt;a href="https://code.google.com/p/google-api-python-client"&gt;
  https://code.google.com/p/google-api-python-client
 &lt;/a&gt;
 &lt;br/&gt;
 &lt;br/&gt;
 there is an example for accessing analytics APIs in the samples/analytics folder,
 &lt;br/&gt;
 but you need to fill in client_secrets.json.
 &lt;br/&gt;
 &lt;br/&gt;
 You can get the credentials from the APIs console:
 &lt;br/&gt;
 &lt;br/&gt;
 &lt;a href="https://code.google.com/apis/console"&gt;
  https://code.google.com/apis/console
 &lt;/a&gt;
 &lt;br/&gt;
 &lt;br/&gt;
 In SERVICES: activate google analytics
 &lt;br/&gt;
 In API Access: Create a "Client ID for installed applications" choosing "Other" as a platform
 &lt;br/&gt;
 &lt;br/&gt;
 Copy the client id and the client secret to client_secrets.json.
 &lt;br/&gt;
 &lt;br/&gt;
 &lt;a name="more"&gt;
 &lt;/a&gt;
 &lt;br/&gt;
 Now you only need the profile ID of the google analytics account, it is in the google analytics web interface, just choose the website, then click on Admin, then on the profile name in the profile tab, and then on profile settings.
 &lt;br/&gt;
 &lt;br/&gt;
 You can then run:
 &lt;br/&gt;
 &lt;br/&gt;
&lt;/p&gt;

&lt;blockquote class="tr_bq"&gt;
 python core_reporting_v3_reference.py ga:PROFILEID
&lt;/blockquote&gt;

&lt;p&gt;The first time you run it, it will open a browser for authentication, but then the auth token is saved and used for future requests.
&lt;br/&gt;
&lt;br/&gt;
This retrieves from the APIs the visits to the website from search, with keywords and the number of visits, for example for my blog:
&lt;br/&gt;
&lt;br/&gt;
&lt;blockquote class="tr_bq"&gt;
 Total Metrics For All Results:
 &lt;br/&gt;
 This query returned 25 rows.
 &lt;br/&gt;
 But the query matched 30 total results.
 &lt;br/&gt;
 Here are the metric totals for the matched total results.
 &lt;br/&gt;
 Metric Name  = ga:visits
 &lt;br/&gt;
 Metric Total = 174
 &lt;br/&gt;
 Rows:
 &lt;br/&gt;
 google
 &lt;span class="Apple-tab-span" style="white-space: pre;"&gt;
 &lt;/span&gt;
 (not provided)
 &lt;span class="Apple-tab-span" style="white-space: pre;"&gt;
 &lt;/span&gt;
 121
 &lt;br/&gt;
 google
 &lt;span class="Apple-tab-span" style="white-space: pre;"&gt;
 &lt;/span&gt;
 andrea zonca
 &lt;span class="Apple-tab-span" style="white-space: pre;"&gt;
 &lt;/span&gt;
 17
 &lt;br/&gt;
 google
 &lt;span class="Apple-tab-span" style="white-space: pre;"&gt;
 &lt;/span&gt;
 butterworth filter python
 &lt;span class="Apple-tab-span" style="white-space: pre;"&gt;
 &lt;/span&gt;
 4
 &lt;br/&gt;
 google
 &lt;span class="Apple-tab-span" style="white-space: pre;"&gt;
 &lt;/span&gt;
 andrea zonca blog
 &lt;span class="Apple-tab-span" style="white-space: pre;"&gt;
 &lt;/span&gt;
 2
 &lt;br/&gt;
 google
 &lt;span class="Apple-tab-span" style="white-space: pre;"&gt;
 &lt;/span&gt;
 healpix for ubuntu
 &lt;span class="Apple-tab-span" style="white-space: pre;"&gt;
 &lt;/span&gt;
 2
 &lt;br/&gt;
 google
 &lt;span class="Apple-tab-span" style="white-space: pre;"&gt;
 &lt;/span&gt;
 healpy install ubuntu
 &lt;span class="Apple-tab-span" style="white-space: pre;"&gt;
 &lt;/span&gt;
 2
 &lt;br/&gt;
 google
 &lt;span class="Apple-tab-span" style="white-space: pre;"&gt;
 &lt;/span&gt;
 python butterworth filter
 &lt;span class="Apple-tab-span" style="white-space: pre;"&gt;
 &lt;/span&gt;
 2
 &lt;br/&gt;
 google
 &lt;span class="Apple-tab-span" style="white-space: pre;"&gt;
 &lt;/span&gt;
 zonca andrea
 &lt;span class="Apple-tab-span" style="white-space: pre;"&gt;
 &lt;/span&gt;
 2
 &lt;br/&gt;
 google
 &lt;span class="Apple-tab-span" style="white-space: pre;"&gt;
 &lt;/span&gt;
 andrea zonca buchrain luzern
 &lt;span class="Apple-tab-span" style="white-space: pre;"&gt;
 &lt;/span&gt;
 1
 &lt;br/&gt;
 google
 &lt;span class="Apple-tab-span" style="white-space: pre;"&gt;
 &lt;/span&gt;
 andrea zonca it
 &lt;span class="Apple-tab-span" style="white-space: pre;"&gt;
 &lt;/span&gt;
 1
 &lt;br/&gt;
 google
 &lt;span class="Apple-tab-span" style="white-space: pre;"&gt;
 &lt;/span&gt;
 astrofisica in pillole
 &lt;span class="Apple-tab-span" style="white-space: pre;"&gt;
 &lt;/span&gt;
 1
 &lt;br/&gt;
 google
 &lt;span class="Apple-tab-span" style="white-space: pre;"&gt;
 &lt;/span&gt;
 bin data healpy
 &lt;span class="Apple-tab-span" style="white-space: pre;"&gt;
 &lt;/span&gt;
 1
 &lt;br/&gt;
 google
 &lt;span class="Apple-tab-span" style="white-space: pre;"&gt;
 &lt;/span&gt;
 ellipticity fwhm
 &lt;span class="Apple-tab-span" style="white-space: pre;"&gt;
 &lt;/span&gt;
 1
 &lt;br/&gt;
 google
 &lt;span class="Apple-tab-span" style="white-space: pre;"&gt;
 &lt;/span&gt;
 enthought and healpy
 &lt;span class="Apple-tab-span" style="white-space: pre;"&gt;
 &lt;/span&gt;
 1
 &lt;br/&gt;
 google
 &lt;span class="Apple-tab-span" style="white-space: pre;"&gt;
 &lt;/span&gt;
 fwhm
 &lt;span class="Apple-tab-span" style="white-space: pre;"&gt;
 &lt;/span&gt;
 1
 &lt;br/&gt;
 google
 &lt;span class="Apple-tab-span" style="white-space: pre;"&gt;
 &lt;/span&gt;
 healpix apt-get
 &lt;span class="Apple-tab-span" style="white-space: pre;"&gt;
 &lt;/span&gt;
 1
 &lt;br/&gt;
 google
 &lt;span class="Apple-tab-span" style="white-space: pre;"&gt;
 &lt;/span&gt;
 healpix repository ubuntu
 &lt;span class="Apple-tab-span" style="white-space: pre;"&gt;
 &lt;/span&gt;
 1
 &lt;br/&gt;
 google
 &lt;span class="Apple-tab-span" style="white-space: pre;"&gt;
 &lt;/span&gt;
 healpix ubuntu 12.04 install
 &lt;span class="Apple-tab-span" style="white-space: pre;"&gt;
 &lt;/span&gt;
 1
 &lt;br/&gt;
 google
 &lt;span class="Apple-tab-span" style="white-space: pre;"&gt;
 &lt;/span&gt;
 healpy ubuntu
 &lt;span class="Apple-tab-span" style="white-space: pre;"&gt;
 &lt;/span&gt;
 1
 &lt;br/&gt;
 google
 &lt;span class="Apple-tab-span" style="white-space: pre;"&gt;
 &lt;/span&gt;
 install healpix ubuntu
 &lt;span class="Apple-tab-span" style="white-space: pre;"&gt;
 &lt;/span&gt;
 1
 &lt;br/&gt;
 google
 &lt;span class="Apple-tab-span" style="white-space: pre;"&gt;
 &lt;/span&gt;
 ipython cluster task output
 &lt;span class="Apple-tab-span" style="white-space: pre;"&gt;
 &lt;/span&gt;
 1
 &lt;br/&gt;
 google
 &lt;span class="Apple-tab-span" style="white-space: pre;"&gt;
 &lt;/span&gt;
 numpy pink noise
 &lt;span class="Apple-tab-span" style="white-space: pre;"&gt;
 &lt;/span&gt;
 1
 &lt;br/&gt;
 google
 &lt;span class="Apple-tab-span" style="white-space: pre;"&gt;
 &lt;/span&gt;
 pink noise numpy
 &lt;span class="Apple-tab-span" style="white-space: pre;"&gt;
 &lt;/span&gt;
 1
 &lt;br/&gt;
 google
 &lt;span class="Apple-tab-span" style="white-space: pre;"&gt;
 &lt;/span&gt;
 python 1/f noise
 &lt;span class="Apple-tab-span" style="white-space: pre;"&gt;
 &lt;/span&gt;
 1
 &lt;br/&gt;
 google
 &lt;span class="Apple-tab-span" style="white-space: pre;"&gt;
 &lt;/span&gt;
 python apply mixin
 &lt;span class="Apple-tab-span" style="white-space: pre;"&gt;
 &lt;/span&gt;
 1
&lt;/blockquote&gt;
&lt;div&gt;
 &lt;br/&gt;
&lt;/div&gt;&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Andrea Zonca</dc:creator><pubDate>Sun, 04 Aug 2013 17:47:00 -0700</pubDate><guid isPermaLink="false">tag:zonca.github.io,2013-08-04:2013/08/export-google-analytics-data-via-api.html</guid><category>python</category></item><item><title>Processing sources in Planck maps with Hadoop and Python</title><link>http://zonca.github.io/2013/07/processing-planck-sources-with-hadoop.html</link><description>&lt;h2&gt;
 Purpose
&lt;/h2&gt;

&lt;div&gt;
 The purpose of this post is to investigate how to process in parallel sources extracted from full sky maps, in this case the maps release by Planck, using Hadoop instead of more traditional MPI-based HPC custom software.
&lt;/div&gt;

&lt;div&gt;
 Hadoop is the MapReduce implementation most used in the enterprise world and it has been traditionally used to process huge amount of text data (~ TBs) , e.g. web pages or logs, over thousands commodity computers connected over ethernet.
&lt;/div&gt;

&lt;div&gt;
 It allows to distribute the data across the nodes on a distributed file-system (HDFS) and then analyze them ("map" step) locally on each node, the output of the map step is traditionally a set of text (key, value) pairs, that are then sorted by the framework and passed to the "reduce" algorithm, which typically aggregates them and then save them to the distributed file-system.
&lt;/div&gt;

&lt;div&gt;
 Hadoop gives robustness to this process by rerunning failed jobs, distribute the data with redundancy and re-distribute in case of failures, among many other features.
&lt;/div&gt;

&lt;div&gt;
 Most scientist use HPC supercomputers for running large data processing software. Using HPC is necessary for algorithms that require frequent communication across the nodes, implemented via MPI calls over a dedicated high speed network (e.g. infiniband). However, often HPC resources are used for running a large number of jobs that are loosely coupled, i.e. each job runs mostly independently of the others, just a sort of aggregation is performed at the end. In this cases the use of a robust and flexible framework like Hadoop could be beneficial.
&lt;/div&gt;

&lt;div&gt;
 &lt;a name="more"&gt;
 &lt;/a&gt;
&lt;/div&gt;

&lt;h2&gt;
 Problem description
&lt;/h2&gt;

&lt;div&gt;
 The Planck collaboration (btw I'm part of it...) released in May 2013 a set of full sky maps in Temperature at 9 different frequencies and catalogs of point and extended galactic and extragalactic sources:
&lt;/div&gt;

&lt;div&gt;
 &lt;a href="http://irsa.ipac.caltech.edu/Missions/planck.html"&gt;
  http://irsa.ipac.caltech.edu/Missions/planck.html
 &lt;/a&gt;
&lt;/div&gt;

&lt;div&gt;
 Each catalog contains about 1000 sources, and the collaboration released the location and flux of each source.
&lt;/div&gt;

&lt;div&gt;
 The purpose of the analysis is to read each of the sky maps, slice out the section of the map around each source and perform some analysis on that patch of sky, as a simple example, to test the infrastructure, I am just going to compute the mean of the pixels located 10 arcminutes around the center of each source.
&lt;/div&gt;

&lt;div&gt;
 In a production run, we might for example run aperture photometry on each source, or fitting for the source center to check for pointing accuracy.
&lt;/div&gt;

&lt;h2&gt;
 Sources
&lt;/h2&gt;

&lt;p&gt;All files are available on github:
&lt;br/&gt;
&lt;div&gt;
 &lt;a href="https://github.com/zonca/planck-sources-hadoop"&gt;
  https://github.com/zonca/planck-sources-hadoop
 &lt;/a&gt;
&lt;/div&gt;
&lt;h2&gt;
 Hadoop setup
&lt;/h2&gt;
&lt;div&gt;
 I am running on the San Diego Supercomputing data intensive cluster Gordon:
&lt;/div&gt;
&lt;div&gt;
 &lt;a href="http://www.sdsc.edu/us/resources/gordon/"&gt;
  http://www.sdsc.edu/us/resources/gordon/
 &lt;/a&gt;
&lt;/div&gt;
&lt;div&gt;
 SDSC has a simplified Hadoop setup based on shell scripts,
 &lt;a href="http://www.sdsc.edu/us/resources/gordon/gordon_hadoop.html"&gt;
  myHadoop
 &lt;/a&gt;
 , which allows running Hadoop as a regular PBS job.
&lt;/div&gt;
&lt;div&gt;
 The most interesting feature is that the Hadoop distributed file-system HDFS is setup on the low-latency local flash drives, one of the distinctive features of Gordon.
&lt;/div&gt;
&lt;h3&gt;
 Using Python with Hadoop-streaming
&lt;/h3&gt;
&lt;div&gt;
 Hadoop applications run natively in Java, however thanks to Hadoop-streaming, we can use stdin and stdout to communicate with a script implemented in any programming language.
&lt;/div&gt;
&lt;div&gt;
 One of the most common choices for scientific applications is Python.
&lt;/div&gt;
&lt;h3&gt;
 Application design
&lt;/h3&gt;
&lt;div&gt;
 Best way to decrease the coupling between different parallel jobs for this application is, instead of analyzing one source at a time, analyze a patch of sky at a time, and loop through all the sources in that region.
&lt;/div&gt;
&lt;div&gt;
 Therefore the largest amount data, the sky map, is only read once by a process, and all the sources are processed. I pre-process the sky map by splitting it in 10x10 degrees patches, saving a 2 columns array with pixel index and map temperature (
 &lt;a href="https://github.com/zonca/planck-sources-hadoop/blob/master/preprocessing.py"&gt;
  preprocessing.py
 &lt;/a&gt;
 ).
&lt;/div&gt;
&lt;div&gt;
 Of course this will produce jobs whose length might be very different, due to the different effective sky area at poles and at equator, and by random number of source per patch, but that's something we do not worry about, that is exactly what Hadoop takes care of.
&lt;/div&gt;
&lt;h2&gt;
 Implementation
&lt;/h2&gt;
&lt;h3&gt;
 Input data
&lt;/h3&gt;
&lt;div&gt;
 The pre-processed patches of sky are available in binary format on a lustre file-system shared by the processes.
&lt;/div&gt;
&lt;div&gt;
 Therefore the text input files for the hadoop jobs are just the list of filenames of the sky patches, one per row.
&lt;/div&gt;
&lt;h3&gt;
 Mapper
&lt;/h3&gt;
&lt;div&gt;
 &lt;a href="https://github.com/zonca/planck-sources-hadoop/blob/master/mapper.py"&gt;
  mapper.py
 &lt;/a&gt;
&lt;/div&gt;
&lt;div&gt;
 &lt;br/&gt;
&lt;/div&gt;
&lt;div&gt;
 The mapper is fed by Hadoop via stdin with a number of lines extracted from the input files and returns a (key, value) text output for each source and for each statistics we compute on the source.
&lt;/div&gt;
&lt;div&gt;
 In this simple scenario, the only returned key printed to stdout is "SOURCENAME_10arcminmean".
&lt;/div&gt;
&lt;div&gt;
 For example, we can run a serial test by running:
&lt;/div&gt;
&lt;div&gt;
 &lt;br/&gt;
&lt;/div&gt;
&lt;div&gt;
 &lt;div&gt;
  &lt;span style="font-family: Courier New, Courier, monospace;"&gt;
   echo plancktest/submaps/030_045_025 | ./mapper.py
  &lt;/span&gt;
 &lt;/div&gt;
&lt;/div&gt;
&lt;div&gt;
 &lt;span style="font-family: Courier New, Courier, monospace;"&gt;
  &lt;br/&gt;
 &lt;/span&gt;
&lt;/div&gt;
&lt;div&gt;
 &lt;span style="font-family: inherit;"&gt;
  and the returned output is:
 &lt;/span&gt;
&lt;/div&gt;
&lt;div&gt;
 &lt;span style="font-family: inherit;"&gt;
  &lt;br/&gt;
 &lt;/span&gt;
&lt;/div&gt;
&lt;div&gt;
 &lt;div&gt;
  &lt;span style="font-family: Courier New, Courier, monospace;"&gt;
   PCCS1 030 G023.00+40.77_10arcminmean
   &lt;span class="Apple-tab-span" style="white-space: pre;"&gt;
   &lt;/span&gt;
   4.49202e-04
  &lt;/span&gt;
 &lt;/div&gt;
 &lt;div&gt;
  &lt;span style="font-family: Courier New, Courier, monospace;"&gt;
   PCCS1 030 G023.13+42.14_10arcminmean
   &lt;span class="Apple-tab-span" style="white-space: pre;"&gt;
   &lt;/span&gt;
   3.37773e-04
  &lt;/span&gt;
 &lt;/div&gt;
 &lt;div&gt;
  &lt;span style="font-family: Courier New, Courier, monospace;"&gt;
   PCCS1 030 G023.84+45.26_10arcminmean
   &lt;span class="Apple-tab-span" style="white-space: pre;"&gt;
   &lt;/span&gt;
   4.69427e-04
  &lt;/span&gt;
 &lt;/div&gt;
 &lt;div&gt;
  &lt;span style="font-family: Courier New, Courier, monospace;"&gt;
   PCCS1 030 G024.32+48.81_10arcminmean
   &lt;span class="Apple-tab-span" style="white-space: pre;"&gt;
   &lt;/span&gt;
   3.79832e-04
  &lt;/span&gt;
 &lt;/div&gt;
 &lt;div&gt;
  &lt;span style="font-family: Courier New, Courier, monospace;"&gt;
   PCCS1 030 G029.42+43.41_10arcminmean
   &lt;span class="Apple-tab-span" style="white-space: pre;"&gt;
   &lt;/span&gt;
   4.11600e-04
  &lt;/span&gt;
 &lt;/div&gt;
 &lt;div style="font-family: inherit;"&gt;
  &lt;br/&gt;
 &lt;/div&gt;
&lt;/div&gt;
&lt;h3&gt;
 Reducer
&lt;/h3&gt;
&lt;div&gt;
 There is no need for a reducer in this scenario, so Hadoop will just use the default IdentityReducer, which just aggregates all the mappers outputs to a single output file.
&lt;/div&gt;
&lt;h3&gt;
 Hadoop call
&lt;/h3&gt;
&lt;div&gt;
 &lt;a href="https://github.com/zonca/planck-sources-hadoop/blob/master/run.pbs"&gt;
  run.pbs
 &lt;/a&gt;
&lt;/div&gt;
&lt;div&gt;
 &lt;br/&gt;
&lt;/div&gt;
&lt;div&gt;
 The hadoop call is:
&lt;/div&gt;
&lt;div&gt;
 &lt;br/&gt;
&lt;/div&gt;
&lt;div&gt;
 &lt;div&gt;
  &lt;span style="font-family: Courier New, Courier, monospace;"&gt;
   &lt;code&gt;
    $HADOOP_HOME/bin/hadoop --config $HADOOP_CONF_DIR jar $HADOOP_HOME/contrib/streaming/hadoop&lt;em&gt;streaming&lt;/em&gt;.jar -file $FOLDER/mapper.py -mapper $FOLDER/mapper.py -input /user/$USER/Input/* -output /user/$USER/Output
   &lt;/code&gt;
  &lt;/span&gt;
 &lt;/div&gt;
&lt;/div&gt;
&lt;div&gt;
 &lt;br/&gt;
&lt;/div&gt;
&lt;div&gt;
 So we are using the Hadoop-streaming interface and providing just the mapper, the input text files (list of sources) had been already copied to HDFS, the output needs then to be copied from HDFS to the local file-system, see run.pbs.
&lt;/div&gt;
&lt;h2&gt;
 Hadoop run and results
&lt;/h2&gt;
&lt;div&gt;
 For testing purposes we have just used 2 of the 9 maps (30 and 70 GHz), and processed all the total of ~2000 sources running Hadoop on 4 nodes.
&lt;/div&gt;
&lt;div&gt;
 Processing takes about 5 minutes, Hadoop automatically chooses the number of mappers, and in this case only uses 2 mappers, as I think it reserves a couple of nodes to run the Scheduler and auxiliary processes.
&lt;/div&gt;
&lt;div&gt;
 The outputs of the mappers are then joined, sorted and written on a single file, see the output file
&lt;/div&gt;
&lt;div&gt;
 &lt;a href="https://github.com/zonca/planck-sources-hadoop/blob/master/output/SAMPLE_RESULT_part-00000"&gt;
  output/SAMPLE_RESULT_part-00000
 &lt;/a&gt;
 .
&lt;/div&gt;
&lt;div&gt;
 See the full log
 &lt;a href="https://github.com/zonca/planck-sources-hadoop/blob/master/sample_logs.txt"&gt;
  sample_logs.txt
 &lt;/a&gt;
 extracted running:
&lt;/div&gt;
&lt;div&gt;
 &lt;span style="font-family: Courier New, Courier, monospace;"&gt;
  /opt/hadoop/bin/hadoop job -history output
 &lt;/span&gt;
&lt;/div&gt;
&lt;h3&gt;
 &lt;span style="font-family: inherit;"&gt;
  Comparison of the results with the catalog
 &lt;/span&gt;
&lt;/h3&gt;
&lt;div&gt;
 &lt;span style="font-family: inherit;"&gt;
  Just for a rough consistency check, I compared the normalized temperatures computed with Hadoop using just the mean of the pixels in a radius of 10 arcmin to the fluxes computed by the Planck collaboration. I find a general agreement with the expected noise excess.
 &lt;/span&gt;
&lt;/div&gt;
&lt;div&gt;
 &lt;br/&gt;
 &lt;div class="separator" style="clear: both; text-align: left;"&gt;
  &lt;a href="http://zonca.github.io/images/processing-planck-sources-with-hadoop_s1600_download.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"&gt;
   &lt;img border="0" src="http://zonca.github.io/images/processing-planck-sources-with-hadoop_s1600_download.png"/&gt;
  &lt;/a&gt;
 &lt;/div&gt;
 &lt;h2&gt;
  Conclusion
 &lt;/h2&gt;
 &lt;div&gt;
  The advantage of using Hadoop is mainly the scalability, this same setup could be used on AWS or Cloudera using hundreds of nodes. All the complexity of scaling is managed by Hadoop.
 &lt;/div&gt;
 &lt;div&gt;
  The main concern is related to loading the data, in a HPC supercomputer it is easy to load directly from a high-performance shared disk, in a cloud environment instead we might opt for a similar setup loading data from S3, but the best would be to use Hadoop itself and stream the data to the mapper in the input files. This is complicated by the fact that Hadoop-streaming only supports text and not binary, the options would be either find a way to pack the binary data in a text file or use Hadoop-pipes instead of Hadoop-streaming.
 &lt;/div&gt;
 &lt;div&gt;
  &lt;br/&gt;
 &lt;/div&gt;
 &lt;div class="separator" style="clear: both; text-align: center;"&gt;
  &lt;br/&gt;
 &lt;/div&gt;
 &lt;div class="separator" style="clear: both; text-align: center;"&gt;
  &lt;br/&gt;
 &lt;/div&gt;
&lt;/div&gt;&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Andrea Zonca</dc:creator><pubDate>Mon, 15 Jul 2013 08:16:00 -0700</pubDate><guid isPermaLink="false">tag:zonca.github.io,2013-07-15:2013/07/processing-planck-sources-with-hadoop.html</guid><category>hpc</category><category>supercomputing</category><category>python</category><category>Planck</category><category>hadoop</category></item><item><title>How to use the IPython notebook on a small computing cluster</title><link>http://zonca.github.io/2013/06/how-to-use-ipython-notebook-on-small.html</link><description>&lt;p&gt;&lt;a href="http://ipython.org/ipython-doc/dev/interactive/htmlnotebook.html"&gt;The IPython notebook&lt;/a&gt; is a powerful and easy to use interface for using Python and particularly useful when running remotely, because it allows the interface to run locally in your browser, while the computing kernel runs remotely on the cluster.&lt;/p&gt;
&lt;h2&gt;1) Configure IPython notebook:&lt;/h2&gt;
&lt;p&gt;First time you use the notebook you need to follow this configuration steps:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Login to the cluster&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Load the python environment, for example:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;module&lt;/span&gt; &lt;span class="n"&gt;load&lt;/span&gt; &lt;span class="n"&gt;pythonEPD&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Create the profile files:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;ipython profile create # creates the configuration files
vim .ipython/profile_default/ipython_notebook_config.py
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;set a password, see instructions in the file.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Change the port to something specific to you, &lt;strong&gt;please change this to avoid conflict with other users&lt;/strong&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;c.NotebookApp.port = 8900
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Set a certificate to serve the notebook over https:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;c.NotebookApp.certfile = u&amp;#39;/home/zonca/mycert.pem&amp;#39;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;or create a new certificate, see &lt;a href="http://ipython.org/ipython-doc/dev/interactive/htmlnotebook.html"&gt;the documentation&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Set:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;c.NotebookApp.open_browser = False
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;2) Run the notebook for testing on the login node.&lt;/h2&gt;
&lt;p&gt;You can use IPython notebook on the login node if you do not use much memory, e.g. &amp;lt; 300MB.
ssh into the login node, at the terminal run:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;ipython notebook --pylab=inline
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;open the browser on your local machine and connect to (always use https, replace 8900 with your port):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;https://LOGINNODEURL:8900
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Dismiss all the browser complaints about the certificate and go ahead.&lt;/p&gt;
&lt;h2&gt;3) Run the notebook on a computing node&lt;/h2&gt;
&lt;p&gt;You should always use a computing node whenever you need a large amount of resources.&lt;/p&gt;
&lt;p&gt;Create a folder &lt;code&gt;notebooks/&lt;/code&gt; in your home, just copy this script in &lt;code&gt;runipynb.pbs&lt;/code&gt; in your that folder:&lt;/p&gt;
&lt;script src="https://gist.github.com/zonca/5840518.js"&gt;
&lt;/script&gt;

&lt;p&gt;replace &lt;code&gt;LOGINNODEURL&lt;/code&gt; with the url of the login node of your cluster.&lt;/p&gt;
&lt;p&gt;NOTICE: you need to ask the sysadmin to set &lt;code&gt;GatewayPorts yes&lt;/code&gt; in &lt;code&gt;sshd_config&lt;/code&gt; on the login node to allow access externally to the notebook.&lt;/p&gt;
&lt;p&gt;Submit the job to the queue running:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;qsub runipynb.pbs
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Then from your local machine connect to (replace 8900 with your port):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;https://LOGINNODEURL:8900
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Other introductory python resources&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://scipy-lectures.github.io/"&gt;Scientific computing with Python&lt;/a&gt;, large and detailed introduction to Python, Numpy, Matplotlib, Scipy&lt;/li&gt;
&lt;li&gt;My &lt;a href="https://github.com/zonca/PythonHPC"&gt;Python for High performance computing&lt;/a&gt;: slides and few ipython notebook examples, see the README&lt;/li&gt;
&lt;li&gt;My &lt;a href="https://github.com/zonca/healpytut/blob/master/healpytut.pdf?raw=true"&gt;short Python and healpy tutorial&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Andrea Zonca</dc:creator><pubDate>Sat, 22 Jun 2013 11:12:00 -0700</pubDate><guid isPermaLink="false">tag:zonca.github.io,2013-06-22:2013/06/how-to-use-ipython-notebook-on-small.html</guid><category>hpc</category><category>ipython</category></item><item><title>IPython parallell setup on Carver at NERSC</title><link>http://zonca.github.io/2013/04/ipython-parallell-setup-on-carver-at.html</link><description>&lt;p&gt;
 IPython parallel is one of the easiest ways to spawn several Python sessions on a Supercomputing cluster and process jobs in parallel.
 &lt;br/&gt;
 &lt;br/&gt;
 On Carver, the basic setup is running a controller on the login node, and submit engines to the computing nodes via PBS.
 &lt;br/&gt;
 &lt;br/&gt;
 &lt;a name="more"&gt;
 &lt;/a&gt;
 &lt;br/&gt;
 First create your configuration files running:
 &lt;br/&gt;
 &lt;br/&gt;
 &lt;span style="font-family: Courier New, Courier, monospace;"&gt;
  ipython profile create --parallel
 &lt;/span&gt;
 &lt;br/&gt;
 &lt;br/&gt;
 Therefore in the ~/.config/ipython/profile_default/ipcluster_config.py, just need to set:
 &lt;br/&gt;
 &lt;br/&gt;
 &lt;span style="font-family: Courier New, Courier, monospace;"&gt;
  c.IPClusterStart.controller_launcher_class = 'LocalControllerLauncher'
 &lt;/span&gt;
 &lt;br/&gt;
 &lt;span style="font-family: Courier New, Courier, monospace;"&gt;
  c.IPClusterStart.engine_launcher_class = 'PBS'
 &lt;/span&gt;
 &lt;br/&gt;
 &lt;span style="font-family: Courier New, Courier, monospace;"&gt;
  c.PBSLauncher.batch_template_file = u'~/.config/ipython/profile_default/pbs.engine.template'
 &lt;/span&gt;
 &lt;br/&gt;
 &lt;br/&gt;
 You also need to allow connections to the controller from other hosts, setting  in ~/.config/ipython/profile_default/ipcontroller_config.py:
 &lt;br/&gt;
 &lt;br/&gt;
 &lt;span style="font-family: Courier New, Courier, monospace;"&gt;
  c.HubFactory.ip = '*'
 &lt;/span&gt;
 &lt;br/&gt;
&lt;/p&gt;

&lt;div&gt;
 &lt;br/&gt;
&lt;/div&gt;

&lt;p&gt;With the path to the pbs engine template.
&lt;br/&gt;
&lt;br/&gt;
Next a couple of examples of pbs templates, for 2 or 8 processes per node:
&lt;script src="https://gist.github.com/zonca/5334225.js"&gt;
&lt;/script&gt;
&lt;br/&gt;
IPython configuration does not seem to be flexible enough to add a parameter for specifying the processes per node.
&lt;br/&gt;
So I just created a bash script that get as parameters the processes per node and the total number of nodes:
&lt;br/&gt;
&lt;br/&gt;
&lt;span style="font-family: Courier New, Courier, monospace;"&gt;
 ipc 8 2 # 2 nodes with 8ppn, 16 total engines
&lt;/span&gt;
&lt;br/&gt;
&lt;span style="font-family: Courier New, Courier, monospace;"&gt;
 ipc 2 3 # 3 nodes with 2ppn, 6 total engines
&lt;/span&gt;
&lt;br/&gt;
&lt;br/&gt;
&lt;span style="font-family: inherit;"&gt;
 Once the engines are running, jobs can be submitted opening an IPython shell on the login node and run:
&lt;/span&gt;
&lt;br/&gt;
&lt;span style="font-family: inherit;"&gt;
 &lt;br/&gt;
&lt;/span&gt;
&lt;br/&gt;
&lt;span style="font-family: Courier New, Courier, monospace;"&gt;
 from IPython.parallel import Client
&lt;/span&gt;
&lt;br/&gt;
&lt;span style="font-family: Courier New, Courier, monospace;"&gt;
 rc = Client()
&lt;/span&gt;
&lt;br/&gt;
&lt;br/&gt;
&lt;span style="font-family: Courier New, Courier, monospace;"&gt;
 lview = rc.load_balanced_view() # default load-balanced view
&lt;/span&gt;
&lt;br/&gt;
&lt;div&gt;
 &lt;span style="font-family: Courier New, Courier, monospace;"&gt;
  def serial_func(argument):
 &lt;/span&gt;
&lt;/div&gt;
&lt;div&gt;
 &lt;span style="font-family: Courier New, Courier, monospace;"&gt;
  pass
 &lt;/span&gt;
&lt;/div&gt;
&lt;div&gt;
 &lt;span style="font-family: Courier New, Courier, monospace;"&gt;
  parallel_result = lview.map(serial_func, list_of_arguments)
 &lt;/span&gt;
&lt;/div&gt;
&lt;br/&gt;
&lt;div style="font-family: inherit;"&gt;
 &lt;br/&gt;
&lt;/div&gt;
&lt;div&gt;
 &lt;span style="font-family: inherit;"&gt;
  The serial function is sent to the engines and executed for each element of the list of arguments.
 &lt;/span&gt;
&lt;/div&gt;
&lt;div&gt;
 &lt;span style="font-family: inherit;"&gt;
  If the function returns a value, than it is transferred back to the login node.
 &lt;/span&gt;
&lt;/div&gt;
&lt;div&gt;
 &lt;span style="font-family: inherit;"&gt;
  In case the returned values are memory consuming, is also possible to still run the controller on the login node, but execute the interactive IPython session in an interactive job.
 &lt;/span&gt;
&lt;/div&gt;
&lt;div style="font-family: inherit;"&gt;
 &lt;br/&gt;
&lt;/div&gt;
&lt;div style="font-family: inherit;"&gt;
 &lt;br/&gt;
&lt;/div&gt;&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Andrea Zonca</dc:creator><pubDate>Thu, 11 Apr 2013 05:53:00 -0700</pubDate><guid isPermaLink="false">tag:zonca.github.io,2013-04-11:2013/04/ipython-parallell-setup-on-carver-at.html</guid><category>hpc</category><category>supercomputing</category><category>ipython</category><category>python</category></item><item><title>Simple Mixin usage in python</title><link>http://zonca.github.io/2013/04/simple-mixin-usage-in-python.html</link><description>&lt;p&gt;
 One situation where Mixins are useful in Python is when you need to modify  a method of similar classes that you are importing from a package.
 &lt;br/&gt;
&lt;/p&gt;

&lt;div&gt;
 &lt;br/&gt;
&lt;/div&gt;

&lt;div&gt;
 For just a single class, it is easier to just create a derived class, but if the same modification must be applied to several classes, then it is cleaner to implement this modification once in a Mixin and then apply it to all of them.
&lt;/div&gt;

&lt;div&gt;
 &lt;br/&gt;
 &lt;a name="more"&gt;
 &lt;/a&gt;
&lt;/div&gt;

&lt;div&gt;
 Here an example in Django:
&lt;/div&gt;

&lt;div&gt;
 &lt;br/&gt;
&lt;/div&gt;

&lt;div&gt;
 Django has several generic view classes that allow to pull objects from the database and feed them to the html templates.
&lt;/div&gt;

&lt;div&gt;
 &lt;br/&gt;
&lt;/div&gt;

&lt;div&gt;
 One for example shows the detail of a specific object:
&lt;/div&gt;

&lt;div&gt;
 &lt;br/&gt;
&lt;/div&gt;

&lt;div&gt;
 &lt;span style="font-family: Courier New, Courier, monospace;"&gt;
  from django.views.generic.detail import DetailView
 &lt;/span&gt;
&lt;/div&gt;

&lt;div&gt;
 &lt;div&gt;
  &lt;br/&gt;
 &lt;/div&gt;
 &lt;div&gt;
  This class has a get_object method that gets an object from the database given a primary key.
 &lt;/div&gt;
 &lt;div&gt;
  We need to modify this method to allow access to an object only to the user that owns them.
 &lt;/div&gt;
 &lt;div&gt;
  &lt;br/&gt;
 &lt;/div&gt;
 &lt;div&gt;
  We first implement a Mixin, i.e. an independent class that only implements the method we wish to override:
 &lt;/div&gt;
 &lt;div&gt;
  &lt;br/&gt;
 &lt;/div&gt;
 &lt;div&gt;
  &lt;span style="font-family: Courier New, Courier, monospace;"&gt;
   class OwnedObjectMixin(object):
  &lt;/span&gt;
 &lt;/div&gt;
 &lt;div&gt;
  &lt;span style="font-family: Courier New, Courier, monospace;"&gt;
   def get_object(self, *args, **kwargs):
  &lt;/span&gt;
 &lt;/div&gt;
 &lt;div&gt;
  &lt;span style="font-family: Courier New, Courier, monospace;"&gt;
   obj = super(OwnedObjectMixin, self).get_object(*args, **kwargs)
  &lt;/span&gt;
 &lt;/div&gt;
 &lt;div&gt;
  &lt;span style="font-family: Courier New, Courier, monospace;"&gt;
   if not obj.user == self.request.user:
  &lt;/span&gt;
 &lt;/div&gt;
 &lt;div&gt;
  &lt;span style="font-family: Courier New, Courier, monospace;"&gt;
   raise Http404
  &lt;/span&gt;
 &lt;/div&gt;
 &lt;div&gt;
  &lt;span style="font-family: Courier New, Courier, monospace;"&gt;
   return obj
  &lt;/span&gt;
 &lt;/div&gt;
 &lt;div&gt;
  &lt;br/&gt;
 &lt;/div&gt;
&lt;/div&gt;

&lt;div&gt;
 &lt;span style="font-family: inherit;"&gt;
  Then we create a new derived class which inherits both from the Mixin and from the class we want to modify.
 &lt;/span&gt;
&lt;/div&gt;

&lt;div&gt;
 &lt;span style="font-family: inherit;"&gt;
  &lt;br/&gt;
 &lt;/span&gt;
&lt;/div&gt;

&lt;div&gt;
 &lt;div&gt;
  &lt;div&gt;
   &lt;span style="font-family: Courier New, Courier, monospace;"&gt;
    class ProtectedDetailView(OwnedObjectMixin, DetailView):
   &lt;/span&gt;
  &lt;/div&gt;
  &lt;div&gt;
   &lt;span style="font-family: Courier New, Courier, monospace;"&gt;
    pass
   &lt;/span&gt;
  &lt;/div&gt;
 &lt;/div&gt;
&lt;/div&gt;

&lt;div&gt;
 &lt;span style="font-family: Courier New, Courier, monospace;"&gt;
  &lt;br/&gt;
 &lt;/span&gt;
&lt;/div&gt;

&lt;div&gt;
 This overrides the get_object method of DetailView with the get_object method of OwnedObjectMixin, and the call to super calls the get_object method of DetailView, so has the same effect of subclassing DetailView and override the get_object method, but we can be apply the same Mixin to other classes.
&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Andrea Zonca</dc:creator><pubDate>Mon, 08 Apr 2013 01:34:00 -0700</pubDate><guid isPermaLink="false">tag:zonca.github.io,2013-04-08:2013/04/simple-mixin-usage-in-python.html</guid><category>python</category></item><item><title>Noise in spectra and map domain</title><link>http://zonca.github.io/2013/04/noise-in-spectra-and-map-domain.html</link><description>&lt;h3&gt;
 Spectra
&lt;/h3&gt;

&lt;p&gt;NET or $\sigma$ is the standard deviation of the noise, measured in mK/sqrt(Hz), typical values for microwave amplifiers are 0.2-5.
&lt;br/&gt;
This is the natural unit of the amplitude spectra (ASD), therefore the high frequency tail of the ASD should get to the expected value of the NET.
&lt;br/&gt;
NET can also be expressed in mKsqrt(s), which is NOT the same unit.
&lt;br/&gt;
&lt;b&gt;
 mK/sqrt(Hz)
&lt;/b&gt;
refers to an integration bandwidth of 1 Hz that assumes a 6dB/octave rolloff, its integration time is only about 0.5 seconds.
&lt;br/&gt;
&lt;b&gt;
 mK/sqrt(s)
&lt;/b&gt;
instead refers to integration time of 1 second, therefore assumes a top hat bandpass.
&lt;br/&gt;
Therefore there is a factor of sqrt(2) difference between the two conventions, therefore mK/sqrt(Hz) = sqrt(2) * mK sqrt(s)
&lt;br/&gt;
See appendix B of Noise Properties of the Planck-LFI Receivers
&lt;br/&gt;
&lt;a href="http://arxiv.org/abs/1001.4608"&gt;
 http://arxiv.org/abs/1001.4608
&lt;/a&gt;
&lt;br/&gt;
&lt;h3&gt;
 Maps
&lt;/h3&gt;
To estimate the map domain noise instead we need to integrate the sigma over the time per pixel; in this case it is easier to convert the noise to sigma/sample, therefore we need to multiply by the square root of the sampling frequency:
&lt;br/&gt;
&lt;br/&gt;
sigma_per_sample = NET * sqrt(sampling_freq)
&lt;br/&gt;
&lt;br/&gt;
Then the variance per pixel is sigma_per_sample**2/number_of_hits
&lt;br/&gt;
&lt;h3&gt;
 Angular power spectra
&lt;/h3&gt;
&lt;div&gt;
 $C_\ell$ of the variance map is just the variance map multiplied by the pixel area divided by the integration time.
 &lt;br/&gt;
 &lt;br/&gt;
 $$C_\ell = \Omega_{\rm pix} \langle \frac{\sigma^2}{\tau} \rangle = \Omega_{\rm pix} \langle \frac{\sigma^2 f_{\rm samp}}{hits} \rangle$$
&lt;/div&gt;&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Andrea Zonca</dc:creator><pubDate>Mon, 08 Apr 2013 01:32:00 -0700</pubDate><guid isPermaLink="false">tag:zonca.github.io,2013-04-08:2013/04/noise-in-spectra-and-map-domain.html</guid><category>map</category><category>power spectra</category><category>noise</category></item><item><title>Basic fork/pull git workflow</title><link>http://zonca.github.io/2013/04/basic-forkpull-git-workflow.html</link><description>&lt;div dir="ltr"&gt;
 Typical simple workflow for a (github) repository with few users.
&lt;/div&gt;

&lt;div dir="ltr"&gt;
 &lt;b&gt;
  &lt;br/&gt;
 &lt;/b&gt;
&lt;/div&gt;

&lt;div dir="ltr"&gt;
 &lt;b&gt;
  Permissions configuration:
 &lt;/b&gt;
&lt;/div&gt;

&lt;div dir="ltr"&gt;
 Main developers have write access to the repository, occasional contributor are supposed to fork and create pull requests.
&lt;/div&gt;

&lt;div dir="ltr"&gt;
&lt;/div&gt;

&lt;p&gt;&lt;a name="more"&gt;
&lt;/a&gt;
&lt;br/&gt;
&lt;div dir="ltr"&gt;
 &lt;b&gt;
  Main developer:
 &lt;/b&gt;
 Small bug fix just go directly in master:
&lt;/div&gt;
&lt;div dir="ltr"&gt;
 &lt;span style="font-family: Courier New, Courier, monospace;"&gt;
  &lt;br/&gt;
 &lt;/span&gt;
&lt;/div&gt;
&lt;div dir="ltr"&gt;
 &lt;span style="font-family: Courier New, Courier, monospace;"&gt;
  git  checkout master
  &lt;br/&gt;
  # update from repository, better use rebase in case there are unpushed commits
  &lt;br/&gt;
  git pull --rebase
  &lt;br/&gt;
  git commit -m "commit message"
  &lt;br/&gt;
  git push
 &lt;/span&gt;
&lt;/div&gt;
&lt;div dir="ltr"&gt;
 &lt;br/&gt;
&lt;/div&gt;
&lt;div dir="ltr"&gt;
 More complex feature, better use a branch:
&lt;/div&gt;
&lt;div dir="ltr"&gt;
 &lt;span style="font-family: Courier New, Courier, monospace;"&gt;
  &lt;br/&gt;
 &lt;/span&gt;
&lt;/div&gt;
&lt;div dir="ltr"&gt;
 &lt;span style="font-family: Courier New, Courier, monospace;"&gt;
  git checkout -b featurebranch
  &lt;br/&gt;
  git commit -am "commit message"
  &lt;br/&gt;
  # work and make several commits
  &lt;br/&gt;
  # backup and share to github
  &lt;br/&gt;
  git push origin featurebranch
 &lt;/span&gt;
&lt;/div&gt;
&lt;div dir="ltr"&gt;
 &lt;span style="font-family: Courier New, Courier, monospace;"&gt;
  &lt;br/&gt;
 &lt;/span&gt;
&lt;/div&gt;
&lt;div dir="ltr"&gt;
 &lt;span style="font-family: inherit;"&gt;
  When ready to merge (cannot push cleanly anymore after any rebasing):
 &lt;/span&gt;
 &lt;span style="font-family: Courier New, Courier, monospace;"&gt;
  &lt;br/&gt;
 &lt;/span&gt;
 &lt;br/&gt;
 &lt;span style="font-family: inherit;"&gt;
  &lt;br/&gt;
 &lt;/span&gt;
&lt;/div&gt;
&lt;div dir="ltr"&gt;
 &lt;span style="font-family: Courier New, Courier, monospace;"&gt;
  # reorder, squash some similar commits, better commit msg
 &lt;/span&gt;
 &lt;br/&gt;
 &lt;span style="font-family: Courier New, Courier, monospace;"&gt;
  git rebase -i HEAD~10
 &lt;/span&gt;
 &lt;br/&gt;
 &lt;span style="font-family: Courier New, Courier, monospace;"&gt;
  # before merging move commits all together to the end of history
 &lt;/span&gt;
 &lt;br/&gt;
 &lt;span style="font-family: Courier New, Courier, monospace;"&gt;
  git rebase master
 &lt;/span&gt;
 &lt;br/&gt;
 &lt;span style="font-family: Courier New, Courier, monospace;"&gt;
  git checkout master
 &lt;/span&gt;
 &lt;br/&gt;
 &lt;span style="font-family: Courier New, Courier, monospace;"&gt;
  git merge featurebranch
 &lt;/span&gt;
 &lt;br/&gt;
 &lt;span style="font-family: Courier New, Courier, monospace;"&gt;
  git push
 &lt;/span&gt;
 &lt;br/&gt;
 &lt;span style="font-family: Courier New, Courier, monospace;"&gt;
  # branch is fully merged, no need to keep it
 &lt;/span&gt;
 &lt;br/&gt;
 &lt;span style="font-family: Courier New, Courier, monospace;"&gt;
  git branch -d featurebranch
 &lt;/span&gt;
 &lt;br/&gt;
 &lt;span style="font-family: Courier New, Courier, monospace;"&gt;
  git push origin --delete featurebranch
 &lt;/span&gt;
&lt;/div&gt;
&lt;div dir="ltr"&gt;
 &lt;br/&gt;
&lt;/div&gt;
&lt;div dir="ltr"&gt;
 Optional, if the feature requires discussing within the team, better create a pull request.
 &lt;br/&gt;
 After cleanup and rebase, instead of merging to master:
 &lt;br/&gt;
 &lt;span style="font-family: Courier New, Courier, monospace;"&gt;
  &lt;br/&gt;
 &lt;/span&gt;
&lt;/div&gt;
&lt;div dir="ltr"&gt;
 &lt;span style="font-family: Courier New, Courier, monospace;"&gt;
  # create new branch
  &lt;br/&gt;
  git checkout -b readyfeaturebranch
  &lt;br/&gt;
  git push origin readyfeaurebranch
 &lt;/span&gt;
&lt;/div&gt;
&lt;div dir="ltr"&gt;
 Connect to github and create a pull request from the new branch to master (now github has a shortcut for creating a pull request from the last branch pushed).
&lt;/div&gt;
&lt;div dir="ltr"&gt;
 &lt;br/&gt;
&lt;/div&gt;
&lt;div dir="ltr"&gt;
 During the discussion on the pull request, any commit to the readyfeaturebranch is added to the pull request.
 &lt;br/&gt;
 When ready either automatically merge on github, or do it manually as previously.
&lt;/div&gt;
&lt;div dir="ltr"&gt;
 &lt;br/&gt;
&lt;/div&gt;
&lt;div dir="ltr"&gt;
 &lt;b&gt;
  For occasional developers:
 &lt;/b&gt;
 Just fork the repo on github to their account, work on a branch there, and then create a pull request on the github web interface from the branch to master on the main repository.
&lt;/div&gt;&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Andrea Zonca</dc:creator><pubDate>Sat, 06 Apr 2013 07:52:00 -0700</pubDate><guid isPermaLink="false">tag:zonca.github.io,2013-04-06:2013/04/basic-forkpull-git-workflow.html</guid><category>git</category><category>github</category></item><item><title>Interactive 3D plot of a sky map</title><link>http://zonca.github.io/2013/03/interactive-3d-plot-of-sky-map.html</link><description>&lt;p&gt;&lt;a href="http://code.enthought.com/projects/mayavi/"&gt;
 Mayavi
&lt;/a&gt;
is a Python package from Enthought for 3D visualization, here a simple example of creating a 3D interactive map starting from a HEALPix pixelization sky map:
&lt;br/&gt;
&lt;div&gt;
 &lt;br/&gt;
 &lt;div&gt;
  &lt;div class="separator" style="clear: both; text-align: center;"&gt;
   &lt;a href="http://zonca.github.io/images/interactive-3d-plot-of-sky-map_s1600_snapshot.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"&gt;
    &lt;img border="0" height="271" src="http://zonca.github.io/images/interactive-3d-plot-of-sky-map_s400_snapshot.png" width="400"/&gt;
   &lt;/a&gt;
  &lt;/div&gt;
  &lt;div class="separator" style="clear: both; text-align: center;"&gt;
   &lt;br/&gt;
  &lt;/div&gt;
  &lt;br/&gt;
  &lt;a name="more"&gt;
  &lt;/a&gt;
  &lt;br/&gt;
  Here the code:
  &lt;br/&gt;
  &lt;script src="https://gist.github.com/zonca/5146356.js"&gt;
  &lt;/script&gt;
  &lt;br/&gt;
  &lt;br/&gt;
  The output is a beautiful 3D interactive map, Mayavi allows to pan, zoom and rotate.
  &lt;br/&gt;
  UPDATE 13 Mar: actually there was a bug (found by Marius Millea) in the script, there is no problem in the projection!
  &lt;br/&gt;
  &lt;div class="separator" style="clear: both; text-align: center;"&gt;
   &lt;br/&gt;
  &lt;/div&gt;
  Mayavi can be installed in Ubuntu installing
  &lt;span style="font-family: Courier New, Courier, monospace;"&gt;
   python-vtk
  &lt;/span&gt;
  and then
  &lt;span style="font-family: Courier New, Courier, monospace;"&gt;
   sudo pip install mayavi.
  &lt;/span&gt;
 &lt;/div&gt;
&lt;/div&gt;&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Andrea Zonca</dc:creator><pubDate>Tue, 12 Mar 2013 19:49:00 -0700</pubDate><guid isPermaLink="false">tag:zonca.github.io,2013-03-12:2013/03/interactive-3d-plot-of-sky-map.html</guid><category>mayavi</category><category>python</category><category>astrophysics</category></item><item><title>How to cite HDF5 in bibtex</title><link>http://zonca.github.io/2013/02/how-to-cite-hdf5-in-bibtex.html</link><description>&lt;p&gt;
 here the bibtex entry:
 &lt;br/&gt;
 &lt;br/&gt;
 &lt;script src="https://gist.github.com/zonca/5043796.js"&gt;
 &lt;/script&gt;
 &lt;br/&gt;
 reference:
 &lt;br/&gt;
 &lt;a href="http://www.hdfgroup.org/HDF5-FAQ.html#gcite"&gt;
  http://www.hdfgroup.org/HDF5-FAQ.html#gcite
 &lt;/a&gt;
&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Andrea Zonca</dc:creator><pubDate>Wed, 27 Feb 2013 00:42:00 -0800</pubDate><guid isPermaLink="false">tag:zonca.github.io,2013-02-27:2013/02/how-to-cite-hdf5-in-bibtex.html</guid></item><item><title>Compile healpix C++ to javascript</title><link>http://zonca.github.io/2013/01/tag:blogger.html</link><description>&lt;p&gt;
 Compile C++ -&amp;gt; LLVM with clang
 &lt;br/&gt;
 &lt;br/&gt;
 Convert LLVM -&amp;gt; Javascript:
 &lt;br/&gt;
 &lt;a href="https://github.com/kripken/emscripten/wiki/Tutorial"&gt;
  https://github.com/kripken/emscripten/wiki/Tutorial
 &lt;/a&gt;
&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Andrea Zonca</dc:creator><pubDate>Mon, 28 Jan 2013 21:06:00 -0800</pubDate><guid isPermaLink="false">tag:zonca.github.io,2013-01-28:2013/01/tag:blogger.html</guid><category>javascript</category><category>hackideas</category><category>healpix</category></item><item><title>Elliptic beams, FWHM and ellipticity</title><link>http://zonca.github.io/2013/01/elliptic-beams-fwhm-and-ellipticity.html</link><description>&lt;p&gt;&lt;span style="background-color: white; color: #222222; font-family: arial, sans-serif; font-size: 13px;"&gt;
 The relationship between the Full Width Half Max, FWHM (min, max, and average) and the
&lt;/span&gt;
&lt;br/&gt;
&lt;span style="background-color: white; color: #222222; font-family: arial, sans-serif; font-size: 13px;"&gt;
 ellipticity is:
&lt;/span&gt;
&lt;br/&gt;
&lt;br style="background-color: white; color: #222222; font-family: arial, sans-serif; font-size: 13px;"/&gt;
&lt;span style="font-family: Courier New, Courier, monospace;"&gt;
 &lt;span style="background-color: white; color: #222222; font-size: 13px;"&gt;
  FWHM = sqrt(FWHM_min * FWHM_max)
 &lt;/span&gt;
 &lt;br style="background-color: white; color: #222222; font-size: 13px;"/&gt;
 &lt;span style="background-color: white; color: #222222; font-size: 13px;"&gt;
  e = FWHM_max/FWHM_min
 &lt;/span&gt;
&lt;/span&gt;
&lt;br/&gt;
&lt;span style="background-color: white; color: #222222; font-family: arial, sans-serif; font-size: 13px;"&gt;
 &lt;br/&gt;
&lt;/span&gt;&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Andrea Zonca</dc:creator><pubDate>Fri, 18 Jan 2013 00:58:00 -0800</pubDate><guid isPermaLink="false">tag:zonca.github.io,2013-01-18:2013/01/elliptic-beams-fwhm-and-ellipticity.html</guid><category>astrophysics</category></item><item><title>Ubuntu PPA for HEALPix and healpy</title><link>http://zonca.github.io/2012/12/ubuntu-ppa-for-healpix-and-healpy.html</link><description>&lt;p&gt;&lt;br/&gt;
&lt;b&gt;
 HEALPix C, C++
&lt;/b&gt;
version 3.00 and
&lt;b&gt;
 healpy
&lt;/b&gt;
version 1.4.1 are now available in a PPA repository for Ubuntu 12.04 Precise and Ubuntu 12.10 Quantal.
&lt;br/&gt;
&lt;br/&gt;
First remove your previous version of
&lt;span style="font-family: Courier New, Courier, monospace;"&gt;
 healpy
&lt;/span&gt;
, just find the location of the package:
&lt;br/&gt;
&lt;br/&gt;
&lt;span style="font-family: Courier New, Courier, monospace;"&gt;
 &amp;gt; python -c "import healpy; print healpy.&lt;strong&gt;file&lt;/strong&gt;"
&lt;/span&gt;
&lt;br/&gt;
&lt;br/&gt;
and remove it:
&lt;br/&gt;
&lt;br/&gt;
&lt;span style="font-family: Courier New, Courier, monospace;"&gt;
 &amp;gt; sudo rm -r /some-base-path/site-packages/healpy*
&lt;/span&gt;
&lt;br/&gt;
&lt;div style="font-family: 'Courier New', Courier, monospace;"&gt;
 &lt;span style="font-family: Courier New, Courier, monospace;"&gt;
  &lt;br/&gt;
 &lt;/span&gt;
&lt;/div&gt;
&lt;span style="font-family: inherit;"&gt;
 Then add the apt repository and install the packages:
&lt;/span&gt;
&lt;br/&gt;
&lt;div style="font-family: 'Courier New', Courier, monospace;"&gt;
 &lt;span style="font-family: Courier New, Courier, monospace;"&gt;
  &lt;br/&gt;
 &lt;/span&gt;
&lt;/div&gt;
&lt;span style="font-family: Courier New, Courier, monospace;"&gt;
 &amp;gt; sudo add-apt-repository ppa:zonca/healpix
&lt;/span&gt;
&lt;br/&gt;
&lt;span style="font-family: Courier New, Courier, monospace;"&gt;
 &amp;gt; sudo apt-get update
&lt;/span&gt;
&lt;br/&gt;
&lt;span style="font-family: Courier New, Courier, monospace;"&gt;
 &amp;gt; sudo apt-get install healpix-cxx libhealpix-cxx-dev
&lt;/span&gt;
&lt;span style="font-family: 'Courier New', Courier, monospace;"&gt;
&lt;/span&gt;
&lt;span style="font-family: 'Courier New', Courier, monospace;"&gt;
 libchealpix0
&lt;/span&gt;
&lt;span style="font-family: 'Courier New', Courier, monospace;"&gt;
 libchealpix-dev python-healpy
&lt;/span&gt;
&lt;br/&gt;
&lt;br/&gt;
&lt;div&gt;
 &lt;div&gt;
  &lt;span style="font-family: Courier New, Courier, monospace;"&gt;
   &amp;gt; which anafast_cxx
  &lt;/span&gt;
 &lt;/div&gt;
 &lt;div&gt;
  &lt;span style="font-family: Courier New, Courier, monospace;"&gt;
   /usr/bin/anafast_cxx
  &lt;/span&gt;
 &lt;/div&gt;
&lt;/div&gt;
&lt;div&gt;
 &lt;span style="font-family: Courier New, Courier, monospace;"&gt;
 &lt;/span&gt;
 &lt;br/&gt;
 &lt;div&gt;
  &lt;span style="font-family: Courier New, Courier, monospace;"&gt;
   &amp;gt; python -c "import healpy; print healpy.&lt;strong&gt;version&lt;/strong&gt;"
  &lt;/span&gt;
 &lt;/div&gt;
 &lt;span style="font-family: Courier New, Courier, monospace;"&gt;
 &lt;/span&gt;
 &lt;br/&gt;
 &lt;div&gt;
  &lt;span style="font-family: Courier New, Courier, monospace;"&gt;
   1.4.1
  &lt;/span&gt;
 &lt;/div&gt;
&lt;/div&gt;&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Andrea Zonca</dc:creator><pubDate>Mon, 17 Dec 2012 10:37:00 -0800</pubDate><guid isPermaLink="false">tag:zonca.github.io,2012-12-17:2012/12/ubuntu-ppa-for-healpix-and-healpy.html</guid><category>healpix</category><category>ubuntu</category></item><item><title>Butterworth filter with Python</title><link>http://zonca.github.io/2012/10/butterworth-filter-with-python.html</link><description>&lt;p&gt;
 Using IPython notebook of course:
 &lt;br/&gt;
 &lt;br/&gt;
 &lt;a href="http://nbviewer.ipython.org/3843014/"&gt;
  http://nbviewer.ipython.org/3843014/
 &lt;/a&gt;
&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Andrea Zonca</dc:creator><pubDate>Sat, 06 Oct 2012 00:00:00 -0700</pubDate><guid isPermaLink="false">tag:zonca.github.io,2012-10-06:2012/10/butterworth-filter-with-python.html</guid><category>ipython</category><category>scipy</category><category>python</category></item><item><title>IPython.parallel for Planck data analysis at NERSC</title><link>http://zonca.github.io/2012/09/ipythonparallel-for-planck-data.html</link><description>&lt;p&gt;&lt;a href="http://www.esa.int/planck"&gt;
 Planck
&lt;/a&gt;
is a Space mission for high precision measurements of the
&lt;a href="http://en.wikipedia.org/wiki/Cosmic_microwave_background_radiation"&gt;
 Cosmic Microwave Background
&lt;/a&gt;
(CMB), data are received as timestreams of output voltages from the 2 instruments on-board, the Low and High Frequency Instruments [LFI / HFI].
&lt;br/&gt;
&lt;br/&gt;
The key phase in data reduction is map-making, where data are binned to a map of the microwave emission of our galaxy, the CMB, and extragalactic sources. This phase is intrinsically parallel and requires simultaneous access to all the data, so requires a fully parallel MPI-based software.
&lt;br/&gt;
&lt;br/&gt;
However, preparing the data for map-making requires several tasks that are serial, but are data and I/O intensive, therefore need to be parallelized.
&lt;br/&gt;
&lt;br/&gt;
&lt;a name="more"&gt;
&lt;/a&gt;
&lt;br/&gt;
IPython.parallel offers the easiest solution for managing a large amount of trivially parallel jobs.
&lt;br/&gt;
&lt;br/&gt;
The first task is pointing reconstruction, where we interpolate and apply several rotations and corrections to low-sampled satellite quaternions stored on disk and then write the output dense detector pointing to disk.
&lt;br/&gt;
The disk quota of pointing files is about 2.5TB split in about 3000 files, those files can be processed independently, therefore we implement a function that processes 1 file, to be used interactively for debugging and testing.
&lt;br/&gt;
Then launch an IPython cluster, typically between 20 and 300 engines on Carver (NERSC), and use the exact same function to process all the ~3000 files in parallel.
&lt;br/&gt;
The IPython
&lt;a href="http://ipython.org/ipython-doc/dev/api/generated/IPython.parallel.client.view.html?highlight=apply_async#IPython.parallel.client.view.LoadBalancedView"&gt;
 BalancedView
&lt;/a&gt;
controller automatically balances the queue therefore we get maximum efficiency, and it is possible to leave the cluster running and submit other instances of the job to be added to its queue.
&lt;br/&gt;
&lt;br/&gt;
Second task is calibration and dipole removal, which processes about 1.2 TB of data, but it needs to read the dense pointing from disk, so it is very I/O intensive. Also in this case we can submit the ~3000 jobs to an IPython.parallel cluster.
&lt;br/&gt;
&lt;br/&gt;
In a next post I'll describe in detail my setup and how I organize my code to make it easy to swap back and forth between debugging code interactively and  running production runs in parallel.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Andrea Zonca</dc:creator><pubDate>Thu, 27 Sep 2012 06:24:00 -0700</pubDate><guid isPermaLink="false">tag:zonca.github.io,2012-09-27:2012/09/ipythonparallel-for-planck-data.html</guid><category>ipython</category><category>python</category><category>Planck</category></item><item><title>homepage on about.me</title><link>http://zonca.github.io/2012/09/homepage-on-aboutme.html</link><description>&lt;p&gt;
 moved my homepage to about.me:
 &lt;br/&gt;
 &lt;br/&gt;
 &lt;a href="http://about.me/andreazonca"&gt;
  http://about.me/andreazonca
 &lt;/a&gt;
 &lt;br/&gt;
 &lt;br/&gt;
 it is quite nice, and essential, as most of it is just links to other websites, i.e. arXiv for publications, Linkedin for CV, github for code.
 &lt;br/&gt;
 So I'm going to use andreazonca.com as blog, hosted on blogger.
&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Andrea Zonca</dc:creator><pubDate>Wed, 26 Sep 2012 22:19:00 -0700</pubDate><guid isPermaLink="false">tag:zonca.github.io,2012-09-26:2012/09/homepage-on-aboutme.html</guid></item><item><title>doctests and unittests happiness 2</title><link>http://zonca.github.io/2012/08/doctests-and-unittests-happiness-2.html</link><description>&lt;blockquote&gt;
 nosetests -v --with-doctest
 &lt;br/&gt;
 Doctest: healpy.pixelfunc.ang2pix ... ok
 &lt;br/&gt;
 Doctest: healpy.pixelfunc.get_all_neighbours ... ok
 &lt;br/&gt;
 Doctest: healpy.pixelfunc.get_interp_val ... ok
 &lt;br/&gt;
 Doctest: healpy.pixelfunc.get_map_size ... ok
 &lt;br/&gt;
 Doctest: healpy.pixelfunc.get_min_valid_nside ... ok
 &lt;br/&gt;
 Doctest: healpy.pixelfunc.get_neighbours ... ok
&lt;/blockquote&gt;

&lt;p&gt;&lt;br/&gt;
&lt;a name="more"&gt;
&lt;/a&gt;
&lt;br/&gt;
&lt;blockquote&gt;
 Doctest: healpy.pixelfunc.isnpixok ... ok
 &lt;br/&gt;
 Doctest: healpy.pixelfunc.isnsideok ... ok
 &lt;br/&gt;
 Doctest: healpy.pixelfunc.ma ... ok
 &lt;br/&gt;
 Doctest: healpy.pixelfunc.maptype ... ok
 &lt;br/&gt;
 Doctest: healpy.pixelfunc.mask_bad ... ok
 &lt;br/&gt;
 Doctest: healpy.pixelfunc.mask_good ... ok
 &lt;br/&gt;
 Doctest: healpy.pixelfunc.max_pixrad ... ok
 &lt;br/&gt;
 Doctest: healpy.pixelfunc.nest2ring ... ok
 &lt;br/&gt;
 Doctest: healpy.pixelfunc.npix2nside ... ok
 &lt;br/&gt;
 Doctest: healpy.pixelfunc.nside2npix ... ok
 &lt;br/&gt;
 Doctest: healpy.pixelfunc.nside2pixarea ... ok
 &lt;br/&gt;
 Doctest: healpy.pixelfunc.nside2resol ... ok
 &lt;br/&gt;
 Doctest: healpy.pixelfunc.pix2ang ... ok
 &lt;br/&gt;
 Doctest: healpy.pixelfunc.pix2vec ... ok
 &lt;br/&gt;
 Doctest: healpy.pixelfunc.reorder ... ok
 &lt;br/&gt;
 Doctest: healpy.pixelfunc.ring2nest ... ok
 &lt;br/&gt;
 Doctest: healpy.pixelfunc.ud_grade ... ok
 &lt;br/&gt;
 Doctest: healpy.pixelfunc.vec2pix ... ok
 &lt;br/&gt;
 Doctest: healpy.rotator.Rotator ... ok
 &lt;br/&gt;
 test_write_map_C (test_fitsfunc.TestFitsFunc) ... ok
 &lt;br/&gt;
 test_write_map_IDL (test_fitsfunc.TestFitsFunc) ... ok
 &lt;br/&gt;
 test_write_alm (test_fitsfunc.TestReadWriteAlm) ... ok
 &lt;br/&gt;
 test_write_alm_256_128 (test_fitsfunc.TestReadWriteAlm) ... ok
 &lt;br/&gt;
 test_ang2pix_nest (test_pixelfunc.TestPixelFunc) ... ok
 &lt;br/&gt;
 test_ang2pix_ring (test_pixelfunc.TestPixelFunc) ... ok
 &lt;br/&gt;
 test_nside2npix (test_pixelfunc.TestPixelFunc) ... ok
 &lt;br/&gt;
 test_nside2pixarea (test_pixelfunc.TestPixelFunc) ... ok
 &lt;br/&gt;
 test_nside2resol (test_pixelfunc.TestPixelFunc) ... ok
 &lt;br/&gt;
 test_inclusive (test_query_disc.TestQueryDisc) ... ok
 &lt;br/&gt;
 test_not_inclusive (test_query_disc.TestQueryDisc) ... ok
 &lt;br/&gt;
 test_anafast (test_sphtfunc.TestSphtFunc) ... ok
 &lt;br/&gt;
 test_anafast_iqu (test_sphtfunc.TestSphtFunc) ... ok
 &lt;br/&gt;
 test_anafast_xspectra (test_sphtfunc.TestSphtFunc) ... ok
 &lt;br/&gt;
 test_synfast (test_sphtfunc.TestSphtFunc) ... ok
 &lt;br/&gt;
 test_cartview_nocrash (test_visufunc.TestNoCrash) ... ok
 &lt;br/&gt;
 test_gnomview_nocrash (test_visufunc.TestNoCrash) ... ok
 &lt;br/&gt;
 test_mollview_nocrash (test_visufunc.TestNoCrash) ... ok
 &lt;br/&gt;
 &lt;br/&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;br/&gt;
 Ran 43 tests in 19.077s
 &lt;br/&gt;
 &lt;br/&gt;
 OK
&lt;/blockquote&gt;&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Andrea Zonca</dc:creator><pubDate>Thu, 16 Aug 2012 14:07:00 -0700</pubDate><guid isPermaLink="false">tag:zonca.github.io,2012-08-16:2012/08/doctests-and-unittests-happiness-2.html</guid><category>python</category><category>Quote</category></item><item><title>compile python module with mpi support</title><link>http://zonca.github.io/2012/07/compile-python-module-with-mpi-support.html</link><description>&lt;p&gt;
 CC=mpicc LDSHARED="mpicc -shared" python setup.py build_ext -i
&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Andrea Zonca</dc:creator><pubDate>Fri, 06 Jul 2012 16:08:00 -0700</pubDate><guid isPermaLink="false">tag:zonca.github.io,2012-07-06:2012/07/compile-python-module-with-mpi-support.html</guid></item><item><title>some python resources</title><link>http://zonca.github.io/2011/11/some-python-resources.html</link><description>&lt;p&gt;
 python tutorial:
 &lt;br/&gt;
 &lt;a href="http://docs.python.org/tutorial/"&gt;
  http://docs.python.org/tutorial/
  &lt;br/&gt;
 &lt;/a&gt;
 numpy tutorial [arrays]:
 &lt;br/&gt;
 &lt;a href="http://www.scipy.org/Tentative_NumPy_Tutorial"&gt;
  http://www.scipy.org/Tentative_NumPy_Tutorial
  &lt;br/&gt;
 &lt;/a&gt;
 plotting tutorial:
 &lt;br/&gt;
 &lt;a href="http://matplotlib.sourceforge.net/users/pyplot_tutorial.html"&gt;
  http://matplotlib.sourceforge.net/users/pyplot_tutorial.html
 &lt;/a&gt;
 &lt;br/&gt;
 &lt;br/&gt;
 free online books:
 &lt;br/&gt;
 &lt;a href="http://diveintopython.org/toc/index.html"&gt;
  http://diveintopython.org/toc/index.html
  &lt;br/&gt;
 &lt;/a&gt;
 &lt;a href="http://www.ibiblio.org/swaroopch/byteofpython/read/"&gt;
  http://www.ibiblio.org/swaroopch/byteofpython/read/
 &lt;/a&gt;
 &lt;br/&gt;
 &lt;br/&gt;
 install enthought python:
 &lt;br/&gt;
 &lt;a href="http://www.enthought.com/products/edudownload.php"&gt;
  http://www.enthought.com/products/edudownload.php
 &lt;/a&gt;
 &lt;br/&gt;
 &lt;br/&gt;
 video tut:
 &lt;br/&gt;
 http://www.youtube.com/watch?v=YW8jtSOTRAU&amp;amp;feature=channel
&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Andrea Zonca</dc:creator><pubDate>Tue, 01 Nov 2011 23:02:00 -0700</pubDate><guid isPermaLink="false">tag:zonca.github.io,2011-11-01:2011/11/some-python-resources.html</guid><category>python</category></item><item><title>cfitsio wrapper in python</title><link>http://zonca.github.io/2011/06/cfitsio-wrapper-in-python.html</link><description>&lt;p&gt;
 After several issues with pyfits, and tired of it being so overengineered, I've wrote my own fits I/O package in python, wrapping the C library cfitsio with ctypes.
 &lt;br/&gt;
 &lt;br/&gt;
 Pretty easy, first version completely developed in 1 day.
 &lt;br/&gt;
 &lt;br/&gt;
 &lt;a href="https://github.com/zonca/pycfitsio"&gt;
  https://github.com/zonca/pycfitsio
 &lt;/a&gt;
&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Andrea Zonca</dc:creator><pubDate>Tue, 21 Jun 2011 04:43:00 -0700</pubDate><guid isPermaLink="false">tag:zonca.github.io,2011-06-21:2011/06/cfitsio-wrapper-in-python.html</guid><category>python</category><category>numpy</category></item><item><title>unit testing happiness</title><link>http://zonca.github.io/2011/06/unit-testing-happiness.html</link><description>&lt;pre&gt;nosetests -v&lt;br/&gt;test_all_cols (pycfitsio.test.TestPyCfitsIoRead) ... ok&lt;br/&gt;test_colnames (pycfitsio.test.TestPyCfitsIoRead) ... ok&lt;br/&gt;test_move (pycfitsio.test.TestPyCfitsIoRead) ... ok&lt;br/&gt;test_open_file (pycfitsio.test.TestPyCfitsIoRead) ... ok&lt;br/&gt;test_read_col (pycfitsio.test.TestPyCfitsIoRead) ... ok&lt;br/&gt;test_read_hdus (pycfitsio.test.TestPyCfitsIoRead) ... ok&lt;br/&gt;test_create (pycfitsio.test.TestPyCfitsIoWrite) ... ok&lt;br/&gt;test_write (pycfitsio.test.TestPyCfitsIoWrite) ... ok&lt;br/&gt;&lt;br/&gt;----------------------------------------------------------------------&lt;br/&gt;Ran 8 tests in 0.016s&lt;br/&gt;&lt;br/&gt;OK&lt;/pre&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Andrea Zonca</dc:creator><pubDate>Tue, 21 Jun 2011 04:39:00 -0700</pubDate><guid isPermaLink="false">tag:zonca.github.io,2011-06-21:2011/06/unit-testing-happiness.html</guid><category>python</category></item><item><title>Pink noise (1/f noise) simulations in numpy</title><link>http://zonca.github.io/2011/05/pink-noise-1f-noise-simulations-in-numpy.html</link><description>&lt;p&gt;&lt;a href="https://gist.github.com/979729"&gt;
 https://gist.github.com/979729
&lt;/a&gt;
&lt;br/&gt;
&lt;br/&gt;
&lt;a href="http://zonca.github.io/images/pink-noise-1f-noise-simulations-in-numpy_05_oneoverf1.png"&gt;
 &lt;img alt="" class="alignnone size-medium wp-image-128" height="225" src="http://zonca.github.io/images/pink-noise-1f-noise-simulations-in-numpy_05_oneoverf1.png" title="oneoverf" width="300"/&gt;
&lt;/a&gt;&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Andrea Zonca</dc:creator><pubDate>Wed, 18 May 2011 23:49:00 -0700</pubDate><guid isPermaLink="false">tag:zonca.github.io,2011-05-18:2011/05/pink-noise-1f-noise-simulations-in-numpy.html</guid><category>python</category><category>physics</category></item><item><title>Vim regular expressions</title><link>http://zonca.github.io/2011/04/vim-regular-expressions.html</link><description>&lt;p&gt;
 very good reference of the usage of regular expressions in VIM:
 &lt;br/&gt;
 &lt;br/&gt;
 &lt;a href="http://www.softpanorama.org/Editors/Vimorama/vim_regular_expressions.shtml"&gt;
  http://www.softpanorama.org/Editors/Vimorama/vim_regular_expressions.shtml
 &lt;/a&gt;
&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Andrea Zonca</dc:creator><pubDate>Fri, 29 Apr 2011 02:14:00 -0700</pubDate><guid isPermaLink="false">tag:zonca.github.io,2011-04-29:2011/04/vim-regular-expressions.html</guid><category>linux</category></item><item><title>set python logging level</title><link>http://zonca.github.io/2011/04/set-python-logging-level.html</link><description>&lt;p&gt;
 often using logging.basicConfig is useless because if the logging module is already configured upfront by one of the imported libraries this is ignored.
 &lt;br/&gt;
 &lt;br/&gt;
 The solution is to set the level directly in the root logger:
 &lt;br/&gt;
 &lt;code&gt;
  ﻿﻿logging.root.level = logging.DEBUG
 &lt;/code&gt;
&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Andrea Zonca</dc:creator><pubDate>Wed, 13 Apr 2011 01:02:00 -0700</pubDate><guid isPermaLink="false">tag:zonca.github.io,2011-04-13:2011/04/set-python-logging-level.html</guid><category>python</category></item><item><title>pyfits memory leak in new_table</title><link>http://zonca.github.io/2011/03/pyfits-memory-leak-in-newtable.html</link><description>&lt;p&gt;
 I found a memory leakage issue in pyfits.new_table, data were NOT deleted when the table was deleted, I prepared a test on github, using
 &lt;a href="http://mg.pov.lt/objgraph/" title="objgraph"&gt;
  objgraph
 &lt;/a&gt;
 , which shows that data are still in memory:
 &lt;br/&gt;
 &lt;a name="more"&gt;
 &lt;/a&gt;
 &lt;a href="https://gist.github.com/884298"&gt;
  https://gist.github.com/884298
 &lt;/a&gt;
 &lt;br/&gt;
 &lt;br/&gt;
 the issue was solved by Erik Bray of STSCI on March 28th, 2011 , see bug report:
 &lt;br/&gt;
 &lt;a href="http://trac6.assembla.com/pyfits/ticket/49"&gt;
  http://trac6.assembla.com/pyfits/ticket/49
  &lt;br/&gt;
 &lt;/a&gt;
 and changeset:
 &lt;br/&gt;
 &lt;a href="http://trac6.assembla.com/pyfits/changeset/844"&gt;
  http://trac6.assembla.com/pyfits/changeset/844
 &lt;/a&gt;
&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Andrea Zonca</dc:creator><pubDate>Mon, 28 Mar 2011 17:22:00 -0700</pubDate><guid isPermaLink="false">tag:zonca.github.io,2011-03-28:2011/03/pyfits-memory-leak-in-newtable.html</guid><category>python</category><category>astrophysics</category></item><item><title>ipython and PyTrilinos</title><link>http://zonca.github.io/2011/02/ipython-and-pytrilinos.html</link><description>&lt;ol&gt;
 &lt;br/&gt;
 &lt;li&gt;
  start ipcontroller
 &lt;/li&gt;
 &lt;br/&gt;
 &lt;li&gt;
  start ipengines:
  &lt;br/&gt;
  &lt;code&gt;
   mpiexec -n 4 ipengine --mpi=pytrilinos
  &lt;/code&gt;
 &lt;/li&gt;
 &lt;br/&gt;
 &lt;li&gt;
  start ipython 0.11:
  &lt;br/&gt;
  &lt;code&gt;
   import PyTrilinos
   &lt;br/&gt;
   from IPython.kernel import client
   &lt;br/&gt;
   mec = client.MultiEngineClient()
   &lt;br/&gt;
   %load_ext parallelmagic
   &lt;br/&gt;
   mec.activate()
   &lt;br/&gt;
   px import PyTrilinos
   &lt;br/&gt;
   px comm=PyTrilinos.Epetra.PyComm()
   &lt;br/&gt;
   px print(comm.NumProc())
  &lt;/code&gt;
 &lt;/li&gt;
 &lt;br/&gt;
&lt;/ol&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Andrea Zonca</dc:creator><pubDate>Wed, 16 Feb 2011 19:10:00 -0800</pubDate><guid isPermaLink="false">tag:zonca.github.io,2011-02-16:2011/02/ipython-and-pytrilinos.html</guid><category>parallel programming</category><category>python</category></item><item><title>git make local branch tracking origin</title><link>http://zonca.github.io/2011/02/git-make-local-branch-tracking-origin.html</link><description>&lt;p&gt;&lt;code&gt;
 git branch --set-upstream master origin/master
&lt;/code&gt;
&lt;br/&gt;
&lt;br/&gt;
you obtain the same result as initial cloning&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Andrea Zonca</dc:creator><pubDate>Wed, 02 Feb 2011 02:58:00 -0800</pubDate><guid isPermaLink="false">tag:zonca.github.io,2011-02-02:2011/02/git-make-local-branch-tracking-origin.html</guid><category>git</category></item><item><title>memory map npy files</title><link>http://zonca.github.io/2011/01/memory-map-npy-files.html</link><description>&lt;p&gt;
 Mem-map the stored array, and then access the second row directly from disk:
 &lt;br/&gt;
 &lt;br/&gt;
 &lt;code&gt;
  X = np.load('/tmp/123.npy', mmap_mode='r')
 &lt;/code&gt;
&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Andrea Zonca</dc:creator><pubDate>Fri, 07 Jan 2011 21:04:00 -0800</pubDate><guid isPermaLink="false">tag:zonca.github.io,2011-01-07:2011/01/memory-map-npy-files.html</guid><category>python</category><category>numpy</category></item><item><title>force local install of python module</title><link>http://zonca.github.io/2010/12/force-local-install-of-python-module.html</link><description>&lt;p&gt;&lt;code&gt;
 python setup.py install --prefix FOLDER
 &lt;br/&gt;
&lt;/code&gt;
&lt;br/&gt;
&lt;br/&gt;
creates lib/python2.6/site-packages, to force a local install you should use:
&lt;br/&gt;
&lt;br/&gt;
&lt;code&gt;
 python setup.py install --install-lib FOLDER
&lt;/code&gt;&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Andrea Zonca</dc:creator><pubDate>Fri, 03 Dec 2010 22:18:00 -0800</pubDate><guid isPermaLink="false">tag:zonca.github.io,2010-12-03:2010/12/force-local-install-of-python-module.html</guid><category>python</category></item><item><title>gnome alt f2 popup launcher</title><link>http://zonca.github.io/2010/08/gnome-alt-f2-popup-launcher.html</link><description>&lt;p&gt;
 ﻿
 &lt;br/&gt;
 &lt;code&gt;
  gnome-panel-control --run-dialog
 &lt;/code&gt;
&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Andrea Zonca</dc:creator><pubDate>Tue, 31 Aug 2010 18:14:00 -0700</pubDate><guid isPermaLink="false">tag:zonca.github.io,2010-08-31:2010/08/gnome-alt-f2-popup-launcher.html</guid><category>linux</category><category>ubuntu</category></item><item><title>switch to interactive backend with ipython -pylab</title><link>http://zonca.github.io/2010/08/switch-to-interactive-backend-with.html</link><description>&lt;p&gt;
 objective:
 &lt;br/&gt;
&lt;/p&gt;

&lt;ol&gt;
 &lt;br/&gt;
 &lt;li&gt;
  when running ipython without pylab or executing scripts you want to use an image matplotlib backend like Agg
 &lt;/li&gt;
 &lt;br/&gt;
 &lt;li&gt;
  just when calling ipython -pylab you want to use an interactive backend like GTKAgg or TKAgg
 &lt;/li&gt;
 &lt;br/&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;br/&gt;
&lt;a name="more"&gt;
&lt;/a&gt;
&lt;br/&gt;
&lt;br/&gt;
you need first to setup as default backend on .matplotlib/matplotlibrc
&lt;strong&gt;
 Agg
&lt;/strong&gt;
:
&lt;br/&gt;
&lt;code&gt;
 backend      : Agg
&lt;/code&gt;
&lt;br/&gt;
then setup you ipython to switch to interactive, in ipython file Shell.py, in the class MatplotlibShellBase, at about line 516, add:
&lt;br/&gt;
&lt;code&gt;
 matplotlib.use('GTKAgg')
&lt;/code&gt;
&lt;br/&gt;
after the first import of matplotlib&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Andrea Zonca</dc:creator><pubDate>Sat, 21 Aug 2010 00:33:00 -0700</pubDate><guid isPermaLink="false">tag:zonca.github.io,2010-08-21:2010/08/switch-to-interactive-backend-with.html</guid><category>python</category></item><item><title>numpy dtypes and fits keywords</title><link>http://zonca.github.io/2010/08/numpy-dtypes-and-fits-keywords.html</link><description>&lt;p&gt;&lt;code&gt;
 bool: 'L',
 &lt;br/&gt;
 uint8: 'B',
 &lt;br/&gt;
 int16: 'I',
 &lt;br/&gt;
 int32: 'J',
 &lt;br/&gt;
 int64: 'K',
 &lt;br/&gt;
 float32: 'E',
 &lt;br/&gt;
 float64: 'D',
 &lt;br/&gt;
 complex64: 'C',
 &lt;br/&gt;
 complex128: 'M'
&lt;/code&gt;&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Andrea Zonca</dc:creator><pubDate>Wed, 04 Aug 2010 21:57:00 -0700</pubDate><guid isPermaLink="false">tag:zonca.github.io,2010-08-04:2010/08/numpy-dtypes-and-fits-keywords.html</guid><category>python</category><category>numpy</category></item><item><title>count hits with numpy</title><link>http://zonca.github.io/2010/07/count-hits-with-numpy.html</link><description>&lt;p&gt;
 I have an array where I record hits
 &lt;br/&gt;
 &lt;code&gt;
  a=np.zeros(5)
 &lt;/code&gt;
 &lt;br/&gt;
 and an array with the indices of the hits, for example I have 2 hits on index 2
 &lt;br/&gt;
 &lt;code&gt;
  hits=np.array([2,2])
 &lt;/code&gt;
 &lt;br/&gt;
 so I want to increase index 2 of a by 2
 &lt;br/&gt;
 &lt;a name="more"&gt;
 &lt;/a&gt;
 &lt;br/&gt;
 I tried:
 &lt;br/&gt;
 &lt;code&gt;
  a[hits]+=1
 &lt;/code&gt;
 &lt;br/&gt;
 but it gives array([ 0.,  0.,  1.,  0.,  0.])
 &lt;br/&gt;
 does someone have a suggestion?
 &lt;br/&gt;
 &lt;code&gt;
  bins=np.bincount(hits)
  &lt;br/&gt;
  a[:len(bins)] += bins
  &lt;br/&gt;
  a
  &lt;br/&gt;
  array([ 0.,  0.,  2.,  0.,  0.])
 &lt;/code&gt;
&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Andrea Zonca</dc:creator><pubDate>Fri, 23 Jul 2010 15:18:00 -0700</pubDate><guid isPermaLink="false">tag:zonca.github.io,2010-07-23:2010/07/count-hits-with-numpy.html</guid><category>python</category><category>numpy</category></item><item><title>change column name in a fits with pyfits</title><link>http://zonca.github.io/2010/06/change-column-name-in-fits-with-pyfits.html</link><description>&lt;p&gt;
 no way to change it manipulating the dtype of the data array.
 &lt;br/&gt;
 &lt;code&gt;
  a=pyfits.open('filename.fits')
  &lt;br/&gt;
  a[1].header.update('TTYPE1','newname')
 &lt;/code&gt;
 &lt;br/&gt;
 you need to change the header, using the update method of the right TTYPE and then write again the fits file using a.writeto.
&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Andrea Zonca</dc:creator><pubDate>Wed, 30 Jun 2010 22:06:00 -0700</pubDate><guid isPermaLink="false">tag:zonca.github.io,2010-06-30:2010/06/change-column-name-in-fits-with-pyfits.html</guid><category>python</category></item><item><title>healpix coordinates</title><link>http://zonca.github.io/2010/06/healpix-coordinates.html</link><description>&lt;p&gt;
 Healpix considers
 &lt;strong&gt;
  latitude
 &lt;/strong&gt;
 theta from 0 on north pole to pi south pole,
 &lt;br/&gt;
 so the conversion is:
 &lt;br/&gt;
 &lt;code&gt;
  theta = pi/2 - latitude
 &lt;/code&gt;
 &lt;br/&gt;
 &lt;strong&gt;
  longitude
 &lt;/strong&gt;
 and phi instead are consistently from 0 to 2*pi with
 &lt;br/&gt;
&lt;/p&gt;

&lt;ul&gt;
 &lt;br/&gt;
 &lt;li&gt;
  zero on vernal equinox (for
  &lt;a href="http://en.wikipedia.org/wiki/Ecliptic_coordinate_system"&gt;
   ecliptic
  &lt;/a&gt;
  ).
 &lt;/li&gt;
 &lt;br/&gt;
 &lt;li&gt;
  zero in the direction from Sun to galactic center (for
  &lt;a href="http://en.wikipedia.org/wiki/Galactic_coordinate_system"&gt;
   galactic
  &lt;/a&gt;
  )
 &lt;/li&gt;
 &lt;br/&gt;
&lt;/ul&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Andrea Zonca</dc:creator><pubDate>Wed, 23 Jun 2010 01:01:00 -0700</pubDate><guid isPermaLink="false">tag:zonca.github.io,2010-06-23:2010/06/healpix-coordinates.html</guid><category>astrophysics</category><category>physics</category></item><item><title>parallel computing the python way</title><link>http://zonca.github.io/2010/06/parallel-computing-python-way.html</link><description>&lt;p&gt;
 forget MPI:
 &lt;br/&gt;
 &lt;a href="http://showmedo.com/videotutorials/series?name=N49qyIFOh"&gt;
  http://showmedo.com/videotutorials/series?name=N49qyIFOh
 &lt;/a&gt;
&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Andrea Zonca</dc:creator><pubDate>Mon, 21 Jun 2010 07:27:00 -0700</pubDate><guid isPermaLink="false">tag:zonca.github.io,2010-06-21:2010/06/parallel-computing-python-way.html</guid><category>parallel programming</category><category>python</category></item><item><title>quaternions for python</title><link>http://zonca.github.io/2010/06/quaternions-for-python.html</link><description>&lt;p&gt;
 the situation is pretty problematic, I hope someday
 &lt;strong&gt;
  scipy
 &lt;/strong&gt;
 will add a python package for rotating and interpolating quaternions, up to now:
 &lt;br/&gt;
&lt;/p&gt;

&lt;ul&gt;
 &lt;br/&gt;
 &lt;li&gt;
  &lt;a href="http://cgkit.sourceforge.net/doc2/quat.html"&gt;
   http://cgkit.sourceforge.net/doc2/quat.html
  &lt;/a&gt;
  : slow, bad interaction with numpy, I could not find a simple way to turn a list of N quaternions to a 4xN array without a loop
 &lt;/li&gt;
 &lt;br/&gt;
 &lt;li&gt;
  &lt;a href="http://cxc.harvard.edu/mta/ASPECT/tool_doc/pydocs/Quaternion.html"&gt;
   http://cxc.harvard.edu/mta/ASPECT/tool_doc/pydocs/Quaternion.html
  &lt;/a&gt;
  : more lightweight, does not implement quaternion interpolation
 &lt;/li&gt;
 &lt;br/&gt;
&lt;/ul&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Andrea Zonca</dc:creator><pubDate>Mon, 21 Jun 2010 07:21:00 -0700</pubDate><guid isPermaLink="false">tag:zonca.github.io,2010-06-21:2010/06/quaternions-for-python.html</guid><category>python</category><category>physics</category></item><item><title>change permission recursively to folders only</title><link>http://zonca.github.io/2010/03/change-permission-recursively-to.html</link><description>&lt;p&gt;&lt;code&gt;
 find . -type d -exec chmod 777 {} \;
&lt;/code&gt;&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Andrea Zonca</dc:creator><pubDate>Tue, 23 Mar 2010 17:58:00 -0700</pubDate><guid isPermaLink="false">tag:zonca.github.io,2010-03-23:2010/03/change-permission-recursively-to.html</guid><category>linux</category></item><item><title>aptitude search 'and'</title><link>http://zonca.github.io/2010/03/aptitude-search.html</link><description>&lt;p&gt;
 this is really something
 &lt;strong&gt;
  really annoying
 &lt;/strong&gt;
 about aptitude, if you run:
 &lt;br/&gt;
 &lt;code&gt;
  aptitude search linux headers
 &lt;/code&gt;
 &lt;br/&gt;
 it will make an 'or' search...to perform a 'and' search, which I need 99.9% of the time, you need quotation marks:
 &lt;br/&gt;
 &lt;code&gt;
  aptitude search 'linux headers'
 &lt;/code&gt;
&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Andrea Zonca</dc:creator><pubDate>Tue, 16 Mar 2010 22:50:00 -0700</pubDate><guid isPermaLink="false">tag:zonca.github.io,2010-03-16:2010/03/aptitude-search.html</guid><category>linux</category><category>ubuntu</category></item><item><title>using numpy dtype with loadtxt</title><link>http://zonca.github.io/2010/03/using-numpy-dtype-with-loadtxt.html</link><description>&lt;p&gt;
 Let's say you want to read a text file like this:
 &lt;br/&gt;
 &lt;br/&gt;
 &lt;br/&gt;
&lt;/p&gt;

&lt;blockquote&gt;
 #filename start end
 &lt;br/&gt;
 fdsafda.fits 23143214 23143214
 &lt;br/&gt;
 safdsafafds.fits 21423 23423432
&lt;/blockquote&gt;

&lt;p&gt;&lt;br/&gt;
&lt;br/&gt;
&lt;br/&gt;
&lt;a name="more"&gt;
&lt;/a&gt;
&lt;br/&gt;
you can use dtype to create a custom array, which is very flexible as you can work by row or columns with strings and floats in the same array:
&lt;br/&gt;
&lt;code&gt;
 dt=np.dtype({'names':['filename','start','end'],'formats':['S100',np.float,np.float]})
 &lt;br/&gt;
&lt;/code&gt;
[I tried also using np.str instead of S100 without success, anyone knows why?]
&lt;br/&gt;
then give this as input to loadtxt to load the file and create the array.
&lt;br/&gt;
&lt;code&gt;
 a = np.loadtxt(open('yourfile.txt'),dtype=dt)
&lt;/code&gt;
&lt;br/&gt;
so each element is:
&lt;br/&gt;
&lt;code&gt;
 ('dsafsadfsadf.fits', 1.6287776249537126e+18, 1.6290301584937428e+18)
 &lt;br/&gt;
&lt;/code&gt;
&lt;br/&gt;
but you can get the array of start or end times using:
&lt;br/&gt;
&lt;code&gt;
 a['start']
&lt;/code&gt;&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Andrea Zonca</dc:creator><pubDate>Wed, 03 Mar 2010 22:49:00 -0800</pubDate><guid isPermaLink="false">tag:zonca.github.io,2010-03-03:2010/03/using-numpy-dtype-with-loadtxt.html</guid><category>linux</category><category>python</category><category>numpy</category></item><item><title>Stop ipcluster from a script</title><link>http://zonca.github.io/2010/02/stop-ipcluster-from-script.html</link><description>&lt;p&gt;
 Ipcluster is easy to start but not trivial to stop from a script, after having finished the processing, here's the solution:
 &lt;br/&gt;
 &lt;code&gt;
  from IPython.kernel import client
  &lt;br/&gt;
  mec = client.MultiEngineClient()
  &lt;br/&gt;
  mec.kill(controller=True)
 &lt;/code&gt;
&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Andrea Zonca</dc:creator><pubDate>Fri, 19 Feb 2010 02:23:00 -0800</pubDate><guid isPermaLink="false">tag:zonca.github.io,2010-02-19:2010/02/stop-ipcluster-from-script.html</guid><category>parallel programming</category><category>python</category></item><item><title>Correlation</title><link>http://zonca.github.io/2010/01/correlation.html</link><description>&lt;p&gt;&lt;strong&gt;
 Expectation value
&lt;/strong&gt;
or first moment of a random variable is the probability weighted sum of the possible values (weighted mean).
&lt;br/&gt;
Expectation value of a 6-dice is 1+2+3+4+5+6 / 6 = 3.5
&lt;br/&gt;
&lt;br/&gt;
&lt;strong&gt;
 Covariance
&lt;/strong&gt;
of 2 random variables is:
&lt;br/&gt;
&lt;code&gt;
 COV(X,Y)=E[(X-E(X))(Y-E(Y))]=E(X&lt;em&gt;Y) - E(X)E(Y)
&lt;/code&gt;
&lt;br/&gt;
i.e. the difference between the expected value of their product and the product of their expected values.
&lt;br/&gt;
So if the variables change together, they will have a high covariance, if they are independent, their covariance is zero.
&lt;br/&gt;
&lt;br/&gt;
&lt;strong&gt;
 Variance
&lt;/strong&gt;
is the covariance on the same variable, :
&lt;br/&gt;
&lt;code&gt;
 COV(X,X)=VAR(X)=E(X&lt;strong&gt;2) - E(X)&lt;/strong&gt;2
&lt;/code&gt;
&lt;br/&gt;
&lt;br/&gt;
&lt;strong&gt;
 Standard deviation
&lt;/strong&gt;
is the square root of Variance
&lt;br/&gt;
&lt;br/&gt;
&lt;strong&gt;
 Correlation
&lt;/strong&gt;
is:
&lt;br/&gt;
&lt;code&gt;
 COR(X,Y)=COV(X,Y)/STDEV(X)&lt;/em&gt;STDEV(Y)
&lt;/code&gt;
&lt;br/&gt;
&lt;br/&gt;
&lt;br/&gt;
&lt;a href="http://mathworld.wolfram.com/Covariance.html"&gt;
 http://mathworld.wolfram.com/Covariance.html
&lt;/a&gt;&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Andrea Zonca</dc:creator><pubDate>Thu, 28 Jan 2010 00:45:00 -0800</pubDate><guid isPermaLink="false">tag:zonca.github.io,2010-01-28:2010/01/correlation.html</guid><category>physics</category></item><item><title>execute bash script remotely with ssh</title><link>http://zonca.github.io/2010/01/execute-bash-script-remotely-with-ssh.html</link><description>&lt;p&gt;
 a bash script launched remotely via ssh does not load the environment, if this is an issue it is necessary to specify --login when calling bash:
 &lt;br/&gt;
 &lt;br/&gt;
 &lt;code&gt;
  ssh user@remoteserver.com 'bash --login life_om/cronodproc' | mail your@email.com -s cronodproc
 &lt;/code&gt;
&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Andrea Zonca</dc:creator><pubDate>Thu, 07 Jan 2010 14:37:00 -0800</pubDate><guid isPermaLink="false">tag:zonca.github.io,2010-01-07:2010/01/execute-bash-script-remotely-with-ssh.html</guid><category>linux</category><category>bash</category></item><item><title>lock pin hold a package using apt on ubuntu</title><link>http://zonca.github.io/2010/01/lock-pin-hold-package-using-apt-on.html</link><description>&lt;p&gt;
 set hold:
 &lt;br/&gt;
 &lt;code&gt;
  echo packagename hold | dpkg --set-selections
 &lt;/code&gt;
 &lt;br/&gt;
 &lt;br/&gt;
 check, should be
 &lt;strong&gt;
  hi
 &lt;/strong&gt;
 :
 &lt;br/&gt;
 &lt;code&gt;
  dpkg -l packagename
 &lt;/code&gt;
 &lt;br/&gt;
 &lt;br/&gt;
 unset hold:
 &lt;br/&gt;
 &lt;code&gt;
  echo packagename install | dpkg --set-selections
 &lt;/code&gt;
&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Andrea Zonca</dc:creator><pubDate>Thu, 07 Jan 2010 13:49:00 -0800</pubDate><guid isPermaLink="false">tag:zonca.github.io,2010-01-07:2010/01/lock-pin-hold-package-using-apt-on.html</guid><category>linux</category><category>ubuntu</category></item><item><title>load arrays from a text file with numpy</title><link>http://zonca.github.io/2010/01/load-arrays-from-text-file-with-numpy.html</link><description>&lt;p&gt;
 space separated text file with 5 arrays in columns:
 &lt;br/&gt;
 &lt;br/&gt;
 [sourcecode language="python"]
 &lt;br/&gt;
 ods,rings,gains,offsets,rparams = np.loadtxt(filename,unpack=True)
 &lt;br/&gt;
 [/sourcecode]
 &lt;br/&gt;
 &lt;br/&gt;
 quite impressive...
&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Andrea Zonca</dc:creator><pubDate>Tue, 05 Jan 2010 16:32:00 -0800</pubDate><guid isPermaLink="false">tag:zonca.github.io,2010-01-05:2010/01/load-arrays-from-text-file-with-numpy.html</guid><category>python</category><category>numpy</category></item><item><title>Latest Maxima and WxMaxima for Ubuntu Karmic</title><link>http://zonca.github.io/2009/12/latest-maxima-and-wxmaxima-for-ubuntu.html</link><description>&lt;p&gt;&lt;a href="http://zeus.nyf.hu/~blahota/maxima/karmic/" title="maxima for ubuntu"&gt;
 http://zeus.nyf.hu/~blahota/maxima/karmic/
&lt;/a&gt;
&lt;br/&gt;
&lt;br/&gt;
on maxima mailing lists they suggested to install the sbcl built, so I first installed sbcl from the Ubuntu repositories and then maxima and wxmaxima f
&lt;br/&gt;
rom this url.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Andrea Zonca</dc:creator><pubDate>Tue, 15 Dec 2009 11:20:00 -0800</pubDate><guid isPermaLink="false">tag:zonca.github.io,2009-12-15:2009/12/latest-maxima-and-wxmaxima-for-ubuntu.html</guid><category>linux</category><category>maxima</category><category>ubuntu</category></item><item><title>number of files in a folder and subfolders</title><link>http://zonca.github.io/2009/12/number-of-files-in-folder-and-subfolders.html</link><description>&lt;p&gt;
 folders are not counted
 &lt;br/&gt;
 &lt;code&gt;
  find . -type f | wc -l
 &lt;/code&gt;
&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Andrea Zonca</dc:creator><pubDate>Thu, 10 Dec 2009 18:16:00 -0800</pubDate><guid isPermaLink="false">tag:zonca.github.io,2009-12-10:2009/12/number-of-files-in-folder-and-subfolders.html</guid><category>linux</category><category>bash</category></item><item><title>forcefully unmount a disk partition</title><link>http://zonca.github.io/2008/09/forcefully-unmount-disk-partition.html</link><description>&lt;p&gt;
 check which processes are accessing a partition:
 &lt;br/&gt;
 &lt;br/&gt;
 [sourcecode language="python"]lsof | grep '/opt'[/sourcecode]
 &lt;br/&gt;
 &lt;br/&gt;
 kill all the processes accessing the partition (check what you're killing, you could loose data):
 &lt;br/&gt;
 &lt;br/&gt;
 [sourcecode language="python"]fuser -km /mnt[/sourcecode]
 &lt;br/&gt;
 &lt;br/&gt;
 try to unmount now:
 &lt;br/&gt;
 [sourcecode language="python"]umount /opt[/sourcecode]
&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Andrea Zonca</dc:creator><pubDate>Wed, 17 Sep 2008 15:14:00 -0700</pubDate><guid isPermaLink="false">tag:zonca.github.io,2008-09-17:2008/09/forcefully-unmount-disk-partition.html</guid><category>linux</category></item><item><title>netcat: quickly send binaries through network</title><link>http://zonca.github.io/2008/04/netcat-quickly-send-binaries-through.html</link><description>&lt;p&gt;
 just start nc in server mode on localhost:
 &lt;br/&gt;
 &lt;br/&gt;
 [sourcecode language='python'] nc -l -p 3333 [/sourcecode]
 &lt;br/&gt;
 &lt;br/&gt;
 send a string to localhost on port 3333:
 &lt;br/&gt;
 &lt;br/&gt;
 [sourcecode language='python'] echo "hello world" | nc localhost 3333 [/sourcecode]
 &lt;br/&gt;
 &lt;br/&gt;
 you'll see on server side appearing the string you sent.
 &lt;br/&gt;
 &lt;br/&gt;
 very useful for sending binaries, see
 &lt;a href="http://www.g-loaded.eu/2006/11/06/netcat-a-couple-of-useful-examples/"&gt;
  examples
 &lt;/a&gt;
 .
&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Andrea Zonca</dc:creator><pubDate>Tue, 29 Apr 2008 12:25:00 -0700</pubDate><guid isPermaLink="false">tag:zonca.github.io,2008-04-29:2008/04/netcat-quickly-send-binaries-through.html</guid><category>linux</category></item><item><title>Decibels, dB and dBm, in terms of Power and Amplitude</title><link>http://zonca.github.io/2008/03/decibels-db-and-dbm-in-terms-of-power.html</link><description>&lt;p&gt;
 It's not difficult, just always having some doubts...
 &lt;br/&gt;
&lt;/p&gt;

&lt;h4&gt;
 Power
&lt;/h4&gt;

&lt;p&gt;&lt;br/&gt;
$latex L_{dB} = 10 log_{10} \left( \dfrac{P_1}{P_0} \right) $
&lt;br/&gt;
&lt;br/&gt;
10 dB increase for a factor 10 increase in the ratio
&lt;br/&gt;
&lt;br/&gt;
3 dB = doubling
&lt;br/&gt;
&lt;br/&gt;
40 dB = 10000 times
&lt;br/&gt;
&lt;h4&gt;
 Amplitude
&lt;/h4&gt;
&lt;br/&gt;
$latex L_{dB} = 10 log_{10} \left( \dfrac{A_1^2}{A_0^2} \right) = 20 log_{10} \left( \dfrac{A_1}{A_0} \right)  $
&lt;br/&gt;
&lt;h4&gt;
 dBm
&lt;/h4&gt;
&lt;br/&gt;
dBm is an absolute value obtained by a ratio with 1 mW:
&lt;br/&gt;
&lt;br/&gt;
$latex L_{dBm} = 10 log_{10} \left( \dfrac{P_1}{1 mW} \right) $
&lt;br/&gt;
&lt;ul&gt;
 &lt;br/&gt;
 &lt;li&gt;
  0 dBm = 1 mW
 &lt;/li&gt;
 &lt;br/&gt;
 &lt;li&gt;
  3 dBm ≈ 2 mW
 &lt;/li&gt;
 &lt;br/&gt;
&lt;/ul&gt;&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Andrea Zonca</dc:creator><pubDate>Sat, 29 Mar 2008 02:13:00 -0700</pubDate><guid isPermaLink="false">tag:zonca.github.io,2008-03-29:2008/03/decibels-db-and-dbm-in-terms-of-power.html</guid><category>general physics</category><category>physics</category></item><item><title>Relation between Power density and temperature in an antenna</title><link>http://zonca.github.io/2008/03/relation-between-power-density-and.html</link><description>&lt;p&gt;
 Considering an antenna placed inside a blackbody enclosure at temperature T, the power received per unit bandwidth is:
 &lt;br/&gt;
 $latex \omega = kT$
 &lt;br/&gt;
 &lt;br/&gt;
 where k is Boltzmann constant.
 &lt;br/&gt;
 &lt;br/&gt;
 This relationship derives from considering a constant brightness $latex B$ in all directions, therefore Rayleigh Jeans law tells:
 &lt;br/&gt;
 &lt;br/&gt;
 $latex B = \dfrac{2kT}{\lambda^2}$
 &lt;br/&gt;
 &lt;br/&gt;
 Power per unit bandwidth is obtained by integrating brightness over antenna beam
 &lt;br/&gt;
 &lt;br/&gt;
 $latex \omega = \frac{1}{2} A_e \int \int B \left( \theta , \phi \right) P_n \left( \theta , \phi \right) d \Omega  $
 &lt;br/&gt;
 &lt;br/&gt;
 therefore
 &lt;br/&gt;
 &lt;br/&gt;
 $latex \omega = \dfrac{kT}{\lambda^2}A_e\Omega_A $
 &lt;br/&gt;
 &lt;br/&gt;
 where:
 &lt;br/&gt;
&lt;/p&gt;

&lt;ul&gt;
 &lt;br/&gt;
 &lt;li&gt;
  $latex A_e$ is antenna effective aperture
 &lt;/li&gt;
 &lt;br/&gt;
 &lt;li&gt;
  $latex \Omega_A$ is antenna beam area
 &lt;/li&gt;
 &lt;br/&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br/&gt;
$latex \lambda^2 = A_e\Omega_A $ another post should talk about this
&lt;br/&gt;
&lt;br/&gt;
finally:
&lt;br/&gt;
&lt;br/&gt;
$latex \omega = kT $
&lt;br/&gt;
&lt;br/&gt;
which is the same noise power of a resistor.
&lt;br/&gt;
&lt;br/&gt;
source : Kraus Radio Astronomy pag 107&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Andrea Zonca</dc:creator><pubDate>Fri, 28 Mar 2008 18:29:00 -0700</pubDate><guid isPermaLink="false">tag:zonca.github.io,2008-03-28:2008/03/relation-between-power-density-and.html</guid><category>astrophysics</category><category>physics</category></item><item><title>Producing PDF from XML files</title><link>http://zonca.github.io/2008/03/producing-pdf-from-xml-files.html</link><description>&lt;p&gt;
 I need to produce formatted pdf from XML data input file.
 &lt;br/&gt;
 The more standard way looks like to use
 &lt;a href="http://www.w3schools.com/xsl" title="w3schools tutorial"&gt;
  XSL stylesheets.
 &lt;/a&gt;
 &lt;br/&gt;
 Associating a XSL sheet to an XML file permits most browsers to render them directly as HMTL, this can be used for web publishing XML sheets.
 &lt;br/&gt;
 &lt;br/&gt;
 The quick and dirty way to produce PDF could be printing them from Firefox, but an interesting option is to use
 &lt;a href="http://http://cyberelk.net/tim/software/xmlto/" title="xmlto homepage"&gt;
  xmlto
 &lt;/a&gt;
 , a script for running a XSL transformation and render an XML in PDF or other formats. It would be interesting to test this script and understand if it needs just docbook XML input or any XML.
&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Andrea Zonca</dc:creator><pubDate>Fri, 28 Mar 2008 16:27:00 -0700</pubDate><guid isPermaLink="false">tag:zonca.github.io,2008-03-28:2008/03/producing-pdf-from-xml-files.html</guid><category>linux</category></item><item><title>vim costumization</title><link>http://zonca.github.io/2006/10/vim-costumization.html</link><description>&lt;p&gt;
 it is about perl but it suggests very useful tricks for programming with vim
 &lt;br/&gt;
 http://mamchenkov.net/wordpress/2004/05/10/vim-for-perl-developers/
&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Andrea Zonca</dc:creator><pubDate>Tue, 17 Oct 2006 10:49:00 -0700</pubDate><guid isPermaLink="false">tag:zonca.github.io,2006-10-17:2006/10/vim-costumization.html</guid><category>linux</category></item><item><title>using gnu find</title><link>http://zonca.github.io/2006/10/using-gnu-find.html</link><description>&lt;p&gt;
 list all the directories excluding ".":
 &lt;br/&gt;
&lt;/p&gt;

&lt;blockquote&gt;
 find . -maxdepth 1 -type d -not -name ".*"
&lt;/blockquote&gt;

&lt;p&gt;&lt;br/&gt;
find some string in all files matching a pattern in the subfolders (with grep -r you cannot specify the type of file)
&lt;br/&gt;
&lt;blockquote&gt;
 find . -name '*.py' -exec grep -i pdb '{}' \;
&lt;/blockquote&gt;&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Andrea Zonca</dc:creator><pubDate>Tue, 03 Oct 2006 14:00:00 -0700</pubDate><guid isPermaLink="false">tag:zonca.github.io,2006-10-03:2006/10/using-gnu-find.html</guid><category>linux</category><category>bash</category></item><item><title>beginners bash guide</title><link>http://zonca.github.io/2006/10/beginners-bash-guide.html</link><description>&lt;p&gt;
 great guide with many examples:
 &lt;br/&gt;
 &lt;br/&gt;
 http://tille.xalasys.com/training/bash/
&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Andrea Zonca</dc:creator><pubDate>Tue, 03 Oct 2006 13:56:00 -0700</pubDate><guid isPermaLink="false">tag:zonca.github.io,2006-10-03:2006/10/beginners-bash-guide.html</guid><category>linux</category><category>bash</category></item><item><title>tar quickref</title><link>http://zonca.github.io/2006/09/tar-quickref.html</link><description>&lt;p&gt;
 compress: tar cvzf foo.tgz *.cc *.h
 &lt;br/&gt;
 check inside: tar tzf foo.tgz | grep file.txt
 &lt;br/&gt;
 extract: tar xvzf foo.tgz
 &lt;br/&gt;
 extract 1 file only: tar xvzf foo.tgz path/to/file.txt
&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Andrea Zonca</dc:creator><pubDate>Mon, 25 Sep 2006 13:19:00 -0700</pubDate><guid isPermaLink="false">tag:zonca.github.io,2006-09-25:2006/09/tar-quickref.html</guid><category>linux</category><category>bash</category></item><item><title>software carpentry</title><link>http://zonca.github.io/2006/09/software-carpentry.html</link><description>&lt;p&gt;
 basic software for scientists and engineers:
 &lt;br/&gt;
 http://www.swc.scipy.org/
 &lt;br/&gt;
&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Andrea Zonca</dc:creator><pubDate>Mon, 25 Sep 2006 12:51:00 -0700</pubDate><guid isPermaLink="false">tag:zonca.github.io,2006-09-25:2006/09/software-carpentry.html</guid><category>linux</category><category>physics</category></item><item><title>Software libero per il trattamento di dati scientifici</title><link>http://zonca.github.io/2006/09/software-libero-per-il-trattamento-di.html</link><description>&lt;p&gt;
 nella ricerca del miglior ambiente per analisi di dati scientifici da leggere questi articoli:
 &lt;br/&gt;
 &lt;br/&gt;
 http://www.pluto.it/files/journal/pj0501/swlibero-scie1.html
 &lt;br/&gt;
 &lt;br/&gt;
 http://www.pluto.it/files/journal/pj0504/swlibero-scie2.html
 &lt;br/&gt;
 &lt;br/&gt;
 http://www.pluto.it/files/journal/pj0505/swlibero-scie3.html
&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Andrea Zonca</dc:creator><pubDate>Fri, 22 Sep 2006 13:35:00 -0700</pubDate><guid isPermaLink="false">tag:zonca.github.io,2006-09-22:2006/09/software-libero-per-il-trattamento-di.html</guid><category>italian</category><category>linux</category><category>physics</category></item><item><title>command line processing</title><link>http://zonca.github.io/2006/09/command-line-processing.html</link><description>&lt;p&gt;
 Very useful summary of many linux command line processing tools (great perl onliners)
 &lt;br/&gt;
 &lt;br/&gt;
 http://grad.physics.sunysb.edu/~leckey/personal/forget/
&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Andrea Zonca</dc:creator><pubDate>Fri, 22 Sep 2006 13:34:00 -0700</pubDate><guid isPermaLink="false">tag:zonca.github.io,2006-09-22:2006/09/command-line-processing.html</guid><category>linux</category><category>bash</category></item><item><title>awk made easy</title><link>http://zonca.github.io/2006/09/awk-made-easy.html</link><description>&lt;p&gt;&lt;strong&gt;
 awk '/REGEX/ {print NR "\t" $9 "\t" $4"_"$5 ;}' file.txt
&lt;/strong&gt;
&lt;br/&gt;
supports extended REGEX like perl (       e.g. [:blank:]  Space or tab characters )
&lt;br/&gt;
NR is line number
&lt;br/&gt;
NF                Number of fields
&lt;br/&gt;
$n is the column to be printed, $0 is the whole row
&lt;br/&gt;
&lt;br/&gt;
if it only necessary to print columns of a file it is easier to use cut:
&lt;br/&gt;
&lt;br/&gt;
name -a | cut -d" " -f1,3,11,12
&lt;br/&gt;
&lt;br/&gt;
-d: or -d" " is the delimiter
&lt;br/&gt;
-f1,3 are the fields to be displayed
&lt;br/&gt;
other options: -s doesnt show lines without delimiters, --complement is selfesplicative
&lt;br/&gt;
condition on a specific field:
&lt;br/&gt;
$&amp;lt;field&amp;gt; ~ /&amp;lt;string&amp;gt;/   Search for string in specified field.
&lt;br/&gt;
&lt;br/&gt;
you can use awk also in pipes:
&lt;br/&gt;
ll | awk 'NR!=1 {s+=$5} END {print "Average: " s/(NR-1)}'
&lt;br/&gt;
END to process al file and then print results
&lt;br/&gt;
&lt;br/&gt;
tutorial on using awk from the command line:
&lt;br/&gt;
&lt;a href="http://www.vectorsite.net/tsawk_3.html#m1" target="_blank" title="awk tutorial"&gt;
 http://www.vectorsite.net/tsawk_3.html#m1
&lt;/a&gt;&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Andrea Zonca</dc:creator><pubDate>Fri, 22 Sep 2006 13:20:00 -0700</pubDate><guid isPermaLink="false">tag:zonca.github.io,2006-09-22:2006/09/awk-made-easy.html</guid><category>linux</category><category>bash</category></item><item><title>pillole di astrofisica</title><link>http://zonca.github.io/2006/09/pillole-di-astrofisica.html</link><description>&lt;p&gt;
 curiosita' ben spiegate da annibale d'ercole, interessante l'idea di avere un livello base e un livello avanzato
 &lt;br/&gt;
 &lt;a href="http://www.bo.astro.it/sait/spigolature/spigostart.html" target="_blank" title="spigolature astronomiche"&gt;
  http://www.bo.astro.it/sait/spigolature/spigostart.html
 &lt;/a&gt;
&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Andrea Zonca</dc:creator><pubDate>Wed, 20 Sep 2006 13:39:00 -0700</pubDate><guid isPermaLink="false">tag:zonca.github.io,2006-09-20:2006/09/pillole-di-astrofisica.html</guid><category>italian</category><category>astrophysics</category><category>physics</category></item></channel></rss>