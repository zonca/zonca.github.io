<!DOCTYPE html>
<html>
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
        <title>Jupyterhub deployment on multiple nodes with Docker Swarm | Andrea Zonca's blog</title>
        <meta name="author" content="Andrea Zonca">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width">

        <link rel="alternate" type="application/atom+xml" title="Andrea Zonca's blog blog atom feed"
              href="/feeds/all.atom.xml" />

        <link rel="stylesheet" href="/theme/css/normalize.min.css">
        <link rel="stylesheet" href="/theme/css/fonts/proximanova.css">
        <link rel="stylesheet" href="/theme/css/fonts/ss-social.css">
        <link rel="stylesheet" href="/theme/css/fonts/ss-standard.css">
        <link rel="stylesheet" href="/theme/css/pygments.css">
        <!-- <link rel="stylesheet" href="/theme/css/ipython.css"> -->
        <link rel="stylesheet" href="/theme/css/main.css">
<script src="//ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/7.3/highlight.min.js"></script>
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/7.3/styles/github.min.css">
<script src="/theme/js/github-files/src/main/javascript/github-files.js"> </script>

    </head>
    <body>
        <div class="header">
            <div class="navbar">
              <a href="/" class="ss-icon" title="home">home</a>
              <a href="/tags.html" class="ss-icon" title="tags">tag</a>
              <a href="http://about.me/andreazonca" class="ss-icon" title="about">user</a>
              <a href="http://twitter.com/andreazonca" class="ss-icon ss-social" title="on twitter">twitter</a>
              <a href="http://github.com/zonca" class="ss-icon ss-social" title="on github">octocat</a>
              <a href="https://plus.google.com/+AndreaZonca?rel=author" class="ss-icon ss-social" title="on googleplus">GooglePlus</a>
              <a href="/feeds/all.atom.xml" class="ss-icon ss-social" title="rss feed">rss</a>
              <a href="mailto:andrea.zonca|on the google mail service" class="ss-icon" title="email me">mail</a>
            </div>
        </div>

        <div id="content">
<div class="post">
    <article>
        <header>
            <h1>Jupyterhub deployment on multiple nodes with Docker Swarm</h1>
        </header>
        <div class='post-content'>
            <p>This post is part of a series on deploying Jupyterhub on OpenStack tailored at workshops, in the previous posts I showed:</p>
<ul>
<li><a href="http://zonca.github.io/2016/04/jupyterhub-sdsc-cloud.html">How to deploy a Jupyterhub on a single server with Docker and Python/R/Julia support</a></li>
<li><a href="http://zonca.github.io/2016/04/jupyterhub-image-sdsc-cloud.html">How to deploy the previous server from a pre-built image and customize it</a></li>
</ul>
<p>The limitation of a single server setup is that it cannot scale beyond the resources available on that server, especially memory. Therefore for a workshop that requires to load large amount of data or with lots of students it is recommended to use a multi-server setup.</p>
<p>Fortunately Docker already provides that flexibility thanks to <a href="https://docs.docker.com/swarm/overview/">Docker Swarm</a>. Docker Swarm allows to have a Docker interface that behaves like a normal single server instance but instead launches containers on a pool of servers. Therefore there are mininal changes on the Jupyterhub server.</p>
<p>Jupyterhub will interface with the Docker Swarm service running locally, Docker Swarm will take care of launching containers across the other nodes. Each container will launch a Jupyter Notebook server for a single user, then Jupyterhub will proxy the container port to the users. Users won't connect directly to the nodes in the Docker Swarm pool. </p>
<h2>Setup the Jupyterhub server</h2>
<p>Let's start from the public image already available, see just the first section "Create a Virtual Machine in OpenStack with the pre-built image" in <a href="http://zonca.github.io/2016/04/jupyterhub-image-sdsc-cloud.html">http://zonca.github.io/2016/04/jupyterhub-image-sdsc-cloud.html</a> for instructions on how to get the Jupyterhub single server running.</p>
<h3>Setup Docker Swarm</h3>
<p>First of all we need to have Docker accessible remotely so we need to configure it to listen on a TCP port, edit <code>/etc/init/docker.conf</code> and replace <code>DOCKER_OPTS=</code> in the <code>start</code> section with:</p>
<div class="highlight"><pre><span></span>DOCKER_OPTS=&quot;-H tcp://0.0.0.0:2375 -H unix:///var/run/docker.sock&quot;
</pre></div>


<p>Port 2375 is not open on the OpenStack configuration, so this is not a security issue.</p>
<p>Then we need to run 2 swarm services in Docker containers, first a distributed key-store listening on port 8500 that is needed for Swarm to store information about all the available nodes, Consul:</p>
<div class="highlight"><pre><span></span>docker run --restart=always  -d -p 8500:8500 --name=consul progrium/consul -server -bootstrap
</pre></div>


<p>the manager which provides the interface to Docker Swarm:</p>
<div class="highlight"><pre><span></span>HUB_LOCAL_IP=$(ip route get 8.8.8.8 | awk &#39;NR==1 {print $NF}&#39;)
docker run --restart=always  -d -p 4000:4000 swarm manage -H :4000 --replication --advertise $HUB_LOCAL_IP:4000 consul://$HUB_LOCAL_IP:8500
</pre></div>


<p>This sets <code>HUB_LOCAL_IP</code> to the internal ip of the instance, then starts the Manager container.</p>
<p>We are running both with automatic restarting, so that they are launched again in case of failure or after reboot.</p>
<p>You can check if the containers are running with:</p>
<div class="highlight"><pre><span></span>docker ps -a
</pre></div>


<p>and then you can check if connection works with Docker Swarm on port 4000:</p>
<div class="highlight"><pre><span></span>docker -H :4000 ps -a
</pre></div>


<p>Check the Docker documentation for a more robust setup with multiple Consul services and a backup Manager.</p>
<h3>Setup Jupyterhub</h3>
<p>Following the work by Jess Hamrick for the <a href="https://github.com/compmodels/jupyterhub">compmodels Jupyterhub deployment</a>, we can get the <code>jupyterhub_config.py</code> from <a href="https://gist.github.com/zonca/83d222df8d0b9eaebd02b83faa676753">https://gist.github.com/zonca/83d222df8d0b9eaebd02b83faa676753</a> and copy them into the home of the ubuntu user.</p>
<h3>Share users home via NFS</h3>
<p>We have now a distributed system and we need a central location to store the home folders of the users, so that even if they happen to get containers on different server, they can still access their files.</p>
<p>Install NFS with the package manager:</p>
<div class="highlight"><pre><span></span>sudo apt-get install nfs-kernel-server
</pre></div>


<p>edit <code>/etc/exports</code>, add:</p>
<div class="highlight"><pre><span></span>/home    *(rw,sync,no_root_squash)
</pre></div>


<p>Ports are not open in the NFS configuration.</p>
<h2>Setup networking</h2>
<p>Before preparing a node, create a new security group under Compute -&gt; Access &amp; Security and name it <code>swarm_group</code>.</p>
<p>We need to be able to have open traffic between the <code>swarmsecgroup</code> and the group of the Jupyterhub instance, <code>jupyterhubsecgroup</code> in my previous tutorial. So in the new <code>swarmsecgroup</code>, add this rule: </p>
<ul>
<li>Add Rule</li>
<li>Rule: ALL TCP</li>
<li>Direction: Ingress</li>
<li>Remote: Security Group</li>
<li>Security Group: <code>jupyterhubsecgroup</code></li>
</ul>
<p>Add another rule replacing Ingress with Egress.
Now open the <code>jupyterhubsecgroup</code> group and add the same 2 rules, just make sure to choose as target "Security Group" <code>swarmsecgroup</code>.</p>
<p>On the <code>swarmsecgroup</code> also add a Rule for SSH traffic from any source choosing CIDR and 0.0.0.0/0, you can disable this after having executed the configuration.</p>
<h2>Setup the Docker Swarm nodes</h2>
<h3>Launch a plain Ubuntu instance</h3>
<p>Launch a new instance, all it <code>swarmnode</code>, choose the size depending on your requirements, and then choose "Boot from image" and get Ubuntu 14.04 LTS (16.04 should work as well, but I haven't yet tested it). Remember to choose a Key Pair under Access &amp; Security and assign the Security Group <code>swarmsecgroup</code>.</p>
<p>Temporarily add a floating IP to this instance in order to SSH into it, see my first tutorial for more details.</p>
<h3>Setup Docker Swarm</h3>
<p>First install Docker engine:</p>
<div class="highlight"><pre><span></span>sudo apt update
sudo apt install apt-transport-https ca-certificates
sudo apt-key adv --keyserver hkp://p80.pool.sks-keyservers.net:80 --recv-keys 58118E89F3A912897C070ADBF76221572C52609D
echo &quot;deb https://apt.dockerproject.org/repo ubuntu-trusty main&quot; | sudo tee /etc/apt/sources.list.d/docker.list 
sudo apt update
sudo apt install -y docker-engine
sudo usermod -aG docker ubuntu
</pre></div>


<p>Then make the same edit we did on the hub, edit <code>/etc/init/docker.conf</code> and replace <code>DOCKER_OPTS=</code> in the <code>start</code> section with:</p>
<div class="highlight"><pre><span></span>DOCKER_OPTS=&quot;-H tcp://0.0.0.0:2375 -H unix:///var/run/docker.sock&quot;
</pre></div>


<p>Restart Docker with:</p>
<div class="highlight"><pre><span></span>sudo service docker restart
</pre></div>


<p>Then run the container that interfaces with Swarm:</p>
<div class="highlight"><pre><span></span>HUB_LOCAL_IP=10.XX.XX.XX
NODE_LOCAL_IP=$(ip route get 8.8.8.8 | awk &#39;NR==1 {print $NF}&#39;)
docker run --restart=always -d swarm join --advertise=$NODE_LOCAL_IP:2375 consul://$HUB_LOCAL_IP:8500
</pre></div>


<p>Copy the address of the Jupyterhub server in the <code>HUB_LOCAL_IP</code> variable.</p>
<h3>Setup mounting the home filesystem</h3>
<div class="highlight"><pre><span></span>sudo apt-get install autofs
</pre></div>


<p>add in <code>/etc/auto.master</code>:</p>
<div class="highlight"><pre><span></span>/home         /etc/auto.home
</pre></div>


<p>create <code>/etc/auto.home</code>:</p>
<div class="highlight"><pre><span></span><span class="nt">echo</span> <span class="s2">&quot;* $HUB_LOCAL_IP:/home/&amp;amp;&quot;</span> <span class="o">|</span> <span class="nt">sudo</span> <span class="nt">tee</span> <span class="o">/</span><span class="nt">etc</span><span class="o">/</span><span class="nt">auto</span><span class="p">.</span><span class="nc">home</span>
</pre></div>


<p>using the internal IP of the hub.</p>
<div class="highlight"><pre><span></span>sudo service autofs restart
</pre></div>


<p>verify by doing:</p>
<div class="highlight"><pre><span></span>ls /home/ubuntu
</pre></div>


<p>or </p>
<div class="highlight"><pre><span></span>ls /home/training01
</pre></div>


<p>you should see the same files that were on the Jupyterhub server.</p>
<h3>Create users</h3>
<p>As we are using system users and mounting the home filesystem it is important that users have the same UID on all nodes, so we are going to run on the node the same script we ran on the Jupyterhub server:</p>
<div class="highlight"><pre><span></span> bash create_users.sh
</pre></div>


<h3>Test Jupyterhub</h3>
<p>Login on the Jupyterhub instance with 2 or more different users, then check on the console of the Hub that the containers were launched on the <code>swarmnode</code> instance:</p>
<div class="highlight"><pre><span></span> docker -H :4000 ps -a
</pre></div>


<h2>Create more nodes</h2>
<p>Now that we created a fully functioning node we can clone it to create more to accomodate more users.</p>
<h3>Create a snapshot of the node</h3>
<p>First we need to delete all Docker containers, ssh into the <code>swarmnode</code> and execute:</p>
<div class="highlight"><pre><span></span> docker rm -f $(docker ps -a -q)
</pre></div>


<p>Docker has a unique identifying key, we need to remove that so that it will be regenerated by the clones.</p>
<div class="highlight"><pre><span></span>sudo service docker stop
sudo rm /etc/docker/key.json
</pre></div>


<p>Then from Compute-&gt;Instances choose "Create Snapshot", call it <code>swarmnodeimage</code>.</p>
<h3>Launch other nodes</h3>
<p>Click on Launch instance-&gt;"Boot from Snapshot"-&gt;<code>swarmnodeimage</code>, choose the <code>swarmnodesecgroup</code> Security Group. Choose any number of instances you need.</p>
<p>Each node will need to launch the Swarm container with its own local ip, not the same as our first node. Therefore we need to use the "Post Creation"-&gt;"Direct Input" and add this script: </p>
<table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre>1
2
3
4</pre></div></td><td class="code"><div class="highlight"><pre><span></span><span class="ch">#!/bin/bash</span>
<span class="nv">HUB_LOCAL_IP</span><span class="o">=</span><span class="m">10</span>.XX.XX.XX
<span class="nv">NODE_LOCAL_IP</span><span class="o">=</span><span class="k">$(</span>ip route get <span class="m">8</span>.8.8.8 <span class="p">|</span> awk <span class="s1">&#39;NR==1 {print $NF}&#39;</span><span class="k">)</span>
docker run --restart<span class="o">=</span>always -d swarm join --advertise<span class="o">=</span><span class="nv">$NODE_LOCAL_IP</span>:2375 consul://<span class="nv">$HUB_LOCAL_IP</span>:8500
</pre></div>
</td></tr></table>

<p><code>HUB_LOCAL_IP</code> is the internal network IP address of the Jupyterhub instance and <code>NODE_LOCAL_IP</code> will be filled with the IP of the OpenStack image just created.</p>
<p>See for example Jupyterhub with 3 remote Swarm nodes running containers for 4 training users:</p>
<div class="highlight"><pre><span></span>$ docker -H :4000 ps -a
CONTAINER ID        IMAGE                                     COMMAND                  CREATED              STATUS              PORTS                         NAMES
60189f208df2        zonca/jupyterhub-datascience-systemuser   <span class="s2">&quot;tini -- sh /srv/sing&quot;</span>   <span class="m">11</span> seconds ago       Up <span class="m">7</span> seconds        <span class="m">10</span>.128.1.28:32769-&gt;8888/tcp   swarmnodes-1/jupyter-training04
1d7b05caedb1        zonca/jupyterhub-datascience-systemuser   <span class="s2">&quot;tini -- sh /srv/sing&quot;</span>   <span class="m">36</span> seconds ago       Up <span class="m">32</span> seconds       <span class="m">10</span>.128.1.27:32768-&gt;8888/tcp   swarmnodes-2/jupyter-training03
733c5ff0a5ed        zonca/jupyterhub-datascience-systemuser   <span class="s2">&quot;tini -- sh /srv/sing&quot;</span>   <span class="m">58</span> seconds ago       Up <span class="m">54</span> seconds       <span class="m">10</span>.128.1.29:32768-&gt;8888/tcp   swarmnodes-3/jupyter-training02
282abce201dd        zonca/jupyterhub-datascience-systemuser   <span class="s2">&quot;tini -- sh /srv/sing&quot;</span>   About a minute ago   Up About a minute   <span class="m">10</span>.128.1.28:32768-&gt;8888/tcp   swarmnodes-1/jupyter-training01
29b2d394fab9        swarm                                     <span class="s2">&quot;/swarm join --advert&quot;</span>   <span class="m">13</span> minutes ago       Up <span class="m">13</span> minutes       <span class="m">2375</span>/tcp                      swarmnodes-2/romantic_easley
8fd3d32fe849        swarm                                     <span class="s2">&quot;/swarm join --advert&quot;</span>   <span class="m">13</span> minutes ago       Up <span class="m">13</span> minutes       <span class="m">2375</span>/tcp                      swarmnodes-3/clever_mestorf
1ae073f7b78b        swarm                                     <span class="s2">&quot;/swarm join --advert&quot;</span>   <span class="m">13</span> minutes ago       Up <span class="m">13</span> minutes       <span class="m">2375</span>/tcp                      swarmnodes-1/jovial_goldwasser
</pre></div>


<h2>Where to go from here</h2>
<p>At this level the deployment is quite complicated, so it is probably worth automating it with an <code>ansible</code> playbook, that will be the subject of the next blog post, I think the result will be a simplified version of <a href="https://github.com/compmodels/jupyterhub-deploy">Jess Hamrick's compmodels deployment</a>. Still, I recommend starting with a manual setup to understand how the different pieces work.</p>
<h2>Troubleshooting</h2>
<p>If <code>docker -H :4000 ps -a</code> gives the error:</p>
<div class="highlight"><pre><span></span>Error response from daemon: No elected primary cluster manager
</pre></div>


<p>it means the Consul container is broken, remove it and create it again.</p>
<h2>Acknowledgments</h2>
<p>Thanks to Jess Hamrick for sharing the setup of her <a href="https://github.com/compmodels">compmodel class on Github</a>, the Jupyter team for releasing such great tools and Kevin Coakley and the rest of the <a href="http://www.sdsc.edu/services/it/cloud.html">SDSC Cloud</a> team for OpenStack support and resources.</p>
        </div>
    </article>

    <div class="metadata">
        <ul>
            <li>
                <strong>Published:</strong>
                <i><time datetime="2016-05-24T12:00:00-07:00">Tue 24 May 2016</time></i>
            </li>
            <li>
                <strong>In:</strong>
                <a href="http://zonca.github.io/category/misc.html">misc</a>
            </li>
            <li>
                <strong>Tags:</strong>
<a href="http://zonca.github.io/tag/ipython.html">ipython</a> 
<a href="http://zonca.github.io/tag/jupyterhub.html">jupyterhub</a> 
<a href="http://zonca.github.io/tag/sdsc.html">sdsc</a> 
            </li>
            <li>
                <strong>Written by:</strong>
                <a href="http://zonca.github.io/author/andrea-zonca.html">Andrea Zonca</a>
            </li>
        </ul>
    </div>
</div>
        </div>

        <footer>
<section class='footer'>
    <div class="post social container row-fluid">
        <div class="span3">
            <div class="fb-like" data-send="false" data-layout="button_count" data-show-faces="false"></div>
        </div>
        <div class="span3">
            <a href="https://twitter.com/share" class="twitter-share-button" data-via="andreazonca">Tweet</a>
        </div>

        <div class="span3">
            <div class="g-plusone" data-size="medium" data-annotation="bubble"></div>
        </div>

        <div class="span3 hn">
            <span id="hnews"></span>
        </div>
    </div>


<script src="/theme/js/social.js"></script>
        </footer>


        <script type="text/javascript">
        var _gaq = _gaq || [];
        _gaq.push(['_setAccount', 'UA-15291408-1']);
        _gaq.push(['_trackPageview']);

        (function() {
        var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
        ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
        var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
        })();
        </script>
<script>
$(document).ready(function () {
    $("pre").each(function(i, e){
        console.log(e);
        if (e.textContent.match("^github")) {
            var parts = e.textContent.split(":")[1].split("/");
            $.getGithubFileByFilePath(parts[0], parts[1], parts.slice(2).join("/"), function(contents)
            {
                e.textContent = contents;
                hljs.highlightBlock(e);
            } );
        };
    });
 });
</script>
    </body>
</html>